[{"categories":null,"contents":"e-commerce guy, camera junkie, father of two\nI\u0026rsquo;m 43 years old and hail from India\u0026rsquo;s capital city, New Delhi. I studied in NGFS Saket with Sanskrit, Physics, Chemistry, Math and Engineering Drawing being my electives.\nI moved out of New Delhi when I was 17 and went to the university, where I graduated as Bachelor of Technology in Chemical Engineering. Even though I graduated as a Chemical Engineer, I decided to pursue a career in software. While in university, I developed a keen interest in C programming and UNIX/Linux operating systems.\nI moved to Mumbai (Bombay, as it was known earlier) in the year 2001 to work with NCST where I worked until the year 2004. I did a lot of learning and a bit of teaching while there and developed an interested in Database Design and Distributed Systems.\nIn 2004, I moved to Bangalore to join Yahoo, where I really started my career in \u0026ldquo;the industry\u0026rdquo;. It was in Bangalore that I found myself interested in cameras and photography. I bought my first digital camera \u0026ndash; a Sony Ericsson w800i. I shot a lot of photos with that little thing before I upgraded to a DSLR.\nI bought my first DSLR, a Nikon D80, in 2007 to photograph the Formula 1 Malaysian GP at Sepang. This was also the first time that I set foot on foreign soil. That same year, I also travelled to the USA. Afterall, \u0026ldquo;have camera, must travel\u0026rdquo;.\nIn 2008, I married Nazia with whom I have two children — Yusuf and Yusra. They\u0026rsquo;re both Bangalore-born. Family life came with its own challenges and rewards, with the rewards greatly outweighing the challenges. The best part of it is how it helped me grow as a person and improve my EQ.\nIn 2010, I joined Zynga\u0026rsquo;s India Studio and worked there until 2015, making it my longest career stint thus far, thanks to the great work culture and multi-functional team dynamics. This is also where I realised my dream of acquiring the \u0026ldquo;Architect\u0026rdquo; title. Yes, that was a big goal of mine. Zynga started my slow transformation from \u0026ldquo;Individual Contributor\u0026rdquo; to a leader.\nIn 2017, after a two-year stint at Flipkart, I took up a job in Indonesia with Tokopedia as VP of Engineering. It\u0026rsquo;s been a thrilling ride with Tokopedia so far and a good life in Indonesia.\n","permalink":"https://tahirhashmi.com/about/tahir/","tags":null,"title":"Tahir's Bio"},{"categories":null,"contents":"20+ years of experience in software development, leadership and innovation at 100M+ user companies valued over $10B. Domain experience in e-commerce, cloud services, gaming and social media.\nSkills Growth engineering, organisation management, agile teams \u0026amp; processes, user experience, technology budget, negotiation and partnerships. Platforms, SaaS, cloud security, multi-cloud, micro-services, distributed databases, performance and scalability.\nExperience Chief Technology Officer \u0026amp; Head of Cloud Solutions, Dekara (Cloud Solutions \u0026amp; Services) 2022-Present Dekara (dekara.io) is a Cloud Solutions \u0026amp; Services startup with a mission to enable reliable, scalable and secure technology products and services that empower business in a sustainable way. Dekara is an initiative by key technology leaders at Tokopedia.\nAs CTO and Head of Cloud Solutions, I provide product development and technology oversight for our upcoming solutions. I also engage with potential customers to help them realise the benefits on offer.\nSVP of Engineering, Tokopedia (e-Commerce – Jakarta) 2017-Present I joined Tokopedia as VP of Engineering, where I started with setting company-wide standards for talent evaluation, engineering competency levels, and SDLC. I led purchase-funnel tribes, including cart, checkout, logistics and promotional discounts, and core technology teams including User Accounts, Communications, Media and Performance Engineering.\nI also established the Platform Engineering tribe for greater focus on scalability, reliability and efficiency of our technology. This tribe comprises modernized, cloud-native versions of erstwhile SRE, Sysadmin, DBA, etc. teams. I laid down the foundational principles that led to the improvement of Tokopedia’s reliability and performance improvements in high traffic sale events. This led to me receiving the title of Technical Fellow in 2019.\nAs Technical Fellow I continued to be responsible for Tokopedia\u0026rsquo;s technology development strategy. I also acted as the Head of Architects\u0026rsquo; Office, comprising 350+ ICs at leadership levels. I was also the executive sponsor for Tokopedia’s cloud projects and made build-vs-buy decisions with a x100M USD budget.\nAs Senior VP, I currently lead the technology teams for Digital e-commerce, B2B, B2B2C \u0026amp; Customer Experience business units with 120+ engineers.\nI\u0026rsquo;m also a regular speaker at industry events such as those by Forrester, Google, Intel, Microsoft, Tokopedia, etc.\nArchitect, Flipkart India - a Walmart co. (e-Commerce – Bangalore) 2015-2017 I joined Flipkart as Technical Architect in FSE (Fulfilment Services Ecosystem) group. I was tasked with the ownership of architecture and SRE of the LPE (Logistics Promise Engine) and related services. I helped scale LPE by 10x to handle up to 150,000 QPS with sub-50ms p95 latency. I also developed a customised database engine for tracking logistics capacity. The engine was capable of delivering 20,000 QPS per CPU cure.\nI joined Flipkart Cloud team as architect on the PaaS group, to help improve the application development experience for Flipkart services. I designed Flipkart\u0026rsquo;s standardised application development model over Flipkart Cloud, including a secure build environment, a rolling immutable deployment system, a managed application execution environment and self-healing elastic application cluster logic.\nI spoke at Flipkart \\N Technology Conference in 2016 and also mentored Flipkart \\N speakers.\nArchitect, Zynga Inc. (Social Gaming – Bangalore, San Francisco) 2010-2015 I started as a Principal Engineer on Vampire Wars game and later became Architect and CTO of “Mafia Wars” — a $40M+ GAR social game.\nDeveloped an award winning multi-player game engine for Zynga’s first MOBA, “Mafia Wars: The Arena” yielding $1M+ incremental GAR. For this, I won the \u0026ldquo;CTO Award\u0026rdquo; for making business impact through technology.\nI led initiatives like Technology cost reduction, Anti-abuse system development, Core game system optimisation and rewrites.\nI chose to write \u0026ldquo;Mafia Wars: The Arena\u0026rdquo; in Go, and eventually became a Go evangelist and still prefer to write in Go.\nTech Lead, Lulu Inc. (Self Publishing \u0026amp; Book Reviews – Bangalore, Raleigh) 2009-2010 Developed wsloader, an open source high performance Python adapter for exposing Python modules as JSON-HTTP APIs. Developed thriftbench, an open source load-testing tool for Thrift RPCs Rebuilt and scaled weRead\u0026rsquo;s \u0026ldquo;Ratings, Reviews and Tags\u0026rdquo; (RRT) web services. Mentored on MySQL performance optimisation, scalability and development processes. Senior Tech Lead, WisdomTap Ltd. (5-person startup – Bangalore) 2008 Invented and implemented WisdomRank™ product ranking algorithm. Built Web-service APIs for B2B, and widgets for B2C integration. In-charge of website development and operations. Tech Lead, Yahoo Inc., Bangalore (Internet Portal – Bangalore, Sunnyvale) 2004-2008 I joined Yahoo in 2004, back when it was actually bigger than Google. My first project was to rebuild a Reviews and Ratings (RnR) platform to replace a legacy backend. I led the design and development for the RnR platform along with a few other developers.\nI used XP and TDD principles for improving development velocity and reliability on my project. I designed the API following intent based principles, resulting in a platform that was flexible enough to fit multiple verticals\u0026rsquo; requirements.\nSeeing the rapid uptake of the platform, I chose to provide it as SaaS (in 2005, when the term was not even coined). This allowed the platform to be deployed everywhere in Yahoo — from USA to Europe, all the way to Hong Kong, Taiwan and even Yahoo Japan.\nMost memorably, the Ratings \u0026amp; Reviews Platform powered the audience votes for \u0026ldquo;The Apprentice\u0026rdquo; TV Show in 2005.\nI also contributed to Hadoop\u0026rsquo;s MapReduce engine and Yahoo Video Search.\nSoftware Engineer, C-DAC (née NCST) (Ministry of IT R\u0026amp;D Centre – Mumbai) 2001-2004 Application development in ASP, ASP.NET and Microsoft SQL Server 2000. Lecturer for “OOP in C++”, “Web Programming”, “Data Structures \u0026amp; Algorithms” and “Advanced Application Development” courses. Developed Xqueeze, an open source, compact XML encoding with SAX API support. Publications Cloud Strategy: A Decision-based Approach to Successful Cloud Migration by Hohpe, Danieli, Hashmi, Landreau\nEducation \u0026amp; Certification Bachelor of Technology, 2001 — AMU, Aligarh Entrepreneurship 1-4 Certification, 2020 — Wharton Online ","permalink":"https://tahirhashmi.com/about/career/","tags":null,"title":"Career"},{"categories":["Reviews","Photography"],"contents":"Fujifilm X100V Review: An Unfulfilled Dream A few months ago, I reviewed the Fujifilm X-E4 and mentioned that I got that camera because\nFor years, I\u0026rsquo;ve been searching for something that\u0026rsquo;s compact, elegant and competent \u0026ndash; a \u0026ldquo;Vanity Camera\u0026rdquo;, as I call it. After much deliberation, I settled on getting the world\u0026rsquo;s prettiest camera, a Fujifilm x100v. Except, as of this writing, I couldn\u0026rsquo;t find a new body in three different countries that I looked.\nWell, it turned out that my brother managed to find an X100V and was gracious enough to swap it for my X-E4 kit. This is my very personal, highly subjective review/opinion piece on this camera.\nFujifilm X100V Handling The X100V is lovely to look at and begs picking up to shoot. The camera powers on quickly, with the built-in lens seemingly popping into shooting position with a slight crackle, although the movement is entirely internal, unlike the XF 27mm f/2.8 R WR. The camera is surprisingly fast to operate, especially once you get the hang of the sparse controls. I never found myself having to wait for the camera, and I enjoyed being able to use the dials to prep the camera for an upcoming situation even before switching it on.\nThe optical part of the hybrid viewfinder is well implemented. The EVF part, while not the best in business, is fairly adequate for its purpose. The shutter speed dial has a clutch mechanism that allows it to be slightly lifted to adjust the ISO, which is visible through a window in the speed dial. It\u0026rsquo;s a very nice control scheme implementation, I must say. There\u0026rsquo;s a dial up front that can be made to fine tune shutter speed, aperture or ISO, around the values set on the dials. The EVF/OVF toggle switch can be long-pulled to toggle the 3-stop ND filter on and off.\nMost of the other operations are similar to the X-E4, which I\u0026rsquo;ve reviewed previously.\nThe only downside in this department is the grip. The camera is farily thick and a bit too heavy to hold single handed. The lack of a decent grip made my right hand hurt within half an hour of carrying the camera.\nDubai Streets Lens Since the day I became aware of DSLRs and detachable lenses, I\u0026rsquo;ve been looking for my all-purpose prime focal length. When I started almost twenty years ago, 50mm was really popular with film photographers but being digital newbies, we hadn\u0026rsquo;t got the crop-factor memo. I shot with the 50mm \u0026ndash; 75mm, effectively \u0026ndash; and realised that was too long. So I shot for several years with a 35mm f/2D Nikkor and a 35mm f/1.8G DX Nikkor without realising that it was effectively like 50mm on film cameras, all the while feeling like I needed more width.\n24mm on APS-C was my dream prime lens and I hated Nikon for not making a compact 24mm DX, while every other system had one. All this to say that I was very excited to try the 23mm on X100V \u0026ndash; my long sought after focal length, now with improved optics!\nThe 23mm f/2 lens built into the X100V is highly regarded by reviewers. As it turns out, that doesn\u0026rsquo;t mean it\u0026rsquo;s a good lens though. Before I elaborate, let me go on a tangent here and recall a bit of history.\n15 years ago, I was working in a startup that used natural language processing to quantify opinions about products under certain applicable situations. We used to crawl photography discussion forums and use our technology to arrive at scores like, \u0026ldquo;Nikon D90 scores 9.1/10 for Landscape Photography\u0026rdquo;.\nAccording to our algorithms, the Nikon 18-200mm VR DX lens was the best lens in the world. That was a somewhat understandable ranking, all things considered, as it would be a good lens to recommend in general. However, to our puzzlement, it also ranked higher for specific situations like low light, even when there were much better and faster f/2.8 or f/1.8 lenses available.\nWe had to dig through the raw text that we had crawled to figure this one out. It turned out that people\u0026rsquo;s opinions are highly relative to the context and expectations. The 18-200 VR lens was not a stellar lens in absolute terms, but it beat expectations from a lens of its kind in pretty much all situations. The more specialised lenses, on the other hand, got a lot more of the hair-splitting criticism.\nThe 23mm f/2 in the X100V is the 18-200mm of compacts. It\u0026rsquo;s not really a stellar lens. It lacks micro-contrast and flare resistance. It has unsharp corners. It doesn\u0026rsquo;t have a spatial rendering (aka 3D look). But, it\u0026rsquo;s not a bad lens either. If it were a bigger, removable lens, it might be getting some respect. However, being a slim profile fixed lens, it beats expectations and that\u0026rsquo;s what earns it rave reviews.\nAs for my long sought-after FoV, 23mm (35mm on full-frame) didn\u0026rsquo;t inspire me a lot. I had much more fun shooting the XF 18mm f/2 R on X-E4 than this lens on X100V. I\u0026rsquo;m not certain that it\u0026rsquo;s entirely down to the FoV, though, because I do enjoy the 40mm f/2 on Z6/7ii (26mm on APS-C). But yeah, I could go wider out to 28mm.\nMasjid an-Nabawi: Dome Ceiling Image Quality I\u0026rsquo;ve commented quite a bit on the Fujifilm image quality in my X-E4 review. The X100V is pretty much the same in that department, which is to say it\u0026rsquo;s pretty disappointing.\nWhat I have learned with reasonable certainty now is that Fujifilm systematically under-exposes by 1 stop compared to, say, Nikon. On top of that, Fujifilm\u0026rsquo;s matrix meter is strongly biased towards highlights. Another way to say it is that the matrix meter is basically a highlight priority meter with a bit more consideration for midtones.\nThe result is that the raw files I obtained needed to be invariably pushed by 1 to 3 EV to achieve a level of brightness that I\u0026rsquo;m familiar with. I suspect this is a consequence of Fujifilm\u0026rsquo;s positioning with film simulations as its raison d\u0026rsquo;être. The camera would go to great lengths to protect highlights because blown highlights were not a film thing, though crushed shadows were.\nThis underexposure is combined with under-saturation as well, to produce raw images that are frankly sad to edit in post-processing. Why then does Fujifilm get such high praise for its image quality? Here\u0026rsquo;s my hypothesis:\nWhich of these two words comes to your mind first when you think of photographs \u0026ndash; \u0026ldquo;memories\u0026rdquo;, or \u0026ldquo;expression\u0026rdquo;?\nIf you chose \u0026ldquo;memories\u0026rdquo;, you\u0026rsquo;d like the slightly dreamy, slightly faded, slightly dull look of Fujifilm. If you chose \u0026ldquo;expression\u0026rdquo;, you\u0026rsquo;re probably going to be frustrated with the lack of post-processing pliability of Fujifilm files.\nAwani, Levant Restaurant Conclusion I really wanted to be able to like the X100V. In the concept, it was everything that I wanted. I mostly enjoyed shooting with it in the field. It\u0026rsquo;s the actual photographs from the camera that left a bitter taste in my mouth. Fujifilm cameras, I realise, are not for me.\n","permalink":"https://tahirhashmi.com/posts/x100v-review/","tags":null,"title":"Fujifilm X100V Review: An Unfulfilled Dream"},{"categories":["Technology","Programming"],"contents":"In part \u0026ldquo;Why\u0026rdquo; of this series that appeared previously, I talked about the need for a new computing model that simplifies modern cloud-native distributed application development. In this part, I\u0026rsquo;ll go into some details of what this new computing model should be and what it should provide.\nWho Is the MetaComputer™ For? You would have figured by now that we\u0026rsquo;re talking about a computing model for programmers who develop cloud-native distributed applications. But what does that mean?\nThe MetaComputer™ would be useful for programmers who develop connected applications, i.e. applications that are accessible over the network rather than being invoked as standalone executables or libraries. The key properties of these applications are:\nthey expose their functionality through network endpoints (REST, gRPC, GraphQL, Messaging\u0026hellip; ) they need to respond whenever a network client invokes them they may be running continuously in a wait loop they may be terminated when clients disconnect and respawn when clients connect they handle persistent state that outlives the application process execution. i.e. even if the application is terminated, the data it operates on remains available for future use they run on elastic compute resources, i.e. they may be deployed to as few or as many computers1 or VMs as needed to satisfy the number of clients invoking them but not more they are parallelised, i.e. multiple instances of these applications may be running at the same time, accessing the same persistent data they usually work with other connected applications as a group to provide end user functionality (e.g. microservices) they are heavily instrumented to provide computation and business execution data for subsequent analysis. i.e. they support the build-measure-learn development cycle. What Does the MetaComputer™ Provide? Now that we understand what kind of applications we need to be able to build on the MetaComputer™, we can start asking for functionality that the model should handle on its own, rather than requiring the programmer to be explicit about.\nThe MetaComputer™ must provide the following:\nService Programming Constructs Native support for services2 as a programming construct transparent network daemons, wire format conversion, stubs and skeletons, service discovery user-to-service / service-to-service authentication and authorization Well defined resiliency model for interacting services Programming constructs for service integration through sychronous request/response asynchronous request/callback bidirectional streaming events and actors Persistence aware programming and native support for durable datatypes Language level support for key architectural patterns (e.g. CQRS, Event Sourcing, Actors, Workflows, etc.) Built-in extensible observability stack with logs, monitors, profilers and pre-defined/user-defined metrics MetaComputing Environment The primary concern of a developer is to get the code they have written to execute in a computing environment. On top of that, they need to be able to get feedback from the environment about the code\u0026rsquo;s performance and tools to troubleshoot unexpected program behaviour.\nDevelopers aren\u0026rsquo;t the only people that interact with a computing environment. A significant improvement to basic computing models can come in the area of state management and data driven intelligence. Following is the minimal list of built-in features that a MetaComputer™ must support\nMetaCompiler™ Compiles MetaComputer™ programs into distributed service topologies Provisions networks and infrastructure for the compiled service ecosystem Built-in support for managing versions and environments SuperState™ Logically consolidated persistent data storage CDC and data aggregation Out-of-the-box analytics over persistent data types First class programming support for experimentation branches and experiment analysis Managed (serverless) runtime environment with auto-scaling MetaComputer™ Schematic Next Generation Ecosystem Contemporary software distribution is predominantly through source code integration (library dependencies) or as SaaS. There was a time when distributing commercial software through precompiled binary libraries was quite common but it\u0026rsquo;s no longer acceptable at large, compared to open source software distribution. Unfortunately, open source doesn\u0026rsquo;t work very well for commercial vendors which has led to monetisation either through support services \u0026ndash; which only works for monetising through enterprise customers \u0026ndash; or through SaaS . SaaS, however, doesn\u0026rsquo;t work very well for customers due to unnecessary network dependencies, loss of control over data fencing and single-vendor dependency.\nA cloud-native programming environment brings about the opportunity to change software integration and distribution models. It can lead to a new distribution model that brings the best of both worlds (source integration and SaaS). This new model can support:\nSecure code sharing and distribution, resilient to supply-chain attacks Install tracking and monetization for authors of proprietary as well as open source code Open standards for enabling competition and differentiation among providers of metacomputing environment implementations Broadly uniform pricing structure across metacomputing providers with different rate cards Ability to switch across metacomputing providers by simply redeploying (eliminate cloud/SaaS vendor lock-in) What the MetaComputer™ Isn\u0026rsquo;t At this point, one might think that we\u0026rsquo;re just looking for an enhancement over some existing technology. However, that is not the case. I ended the previous article talking about why Apache Mesos or Kubernetes are not the computing model we are looking for. Here I elaborate a bit more on the conceptual differences.\nNot a Supercomputer Apache Mesos originated from the cluster computing / supercomputing / HPC background. The main concern in that environment is to allow running singular applications that place extreme demands on computing power. Think climate simulations involving particle physics etc.\nA supercomputing model then is all about abstracting a collection of many many interconnected computers as one humongous computer with the sum total of all the computers\u0026rsquo; CPU power, memory and disk space.\nThat\u0026rsquo;s not the kind of abstraction that a typical service developer really wants. Most service backends are applications with modest resource requirements for a single business process but they run many instances of the same business process in parallel.\nNot an Orchestrator or IaaS Kubernetes, Docker Swarm and similar tools are a much more appropriate interface for managing the kind of applications we\u0026rsquo;re interested in developing.\nThey take the underlying cluster of available computers and rather than summing them up into one giant conglomeration, they divvy up the individual computers\u0026rsquo; resources into smaller logical chunks. Then they take the applications along with their resource requirement specifications and deploy those applications on to these chunks.\nThey go a step further and recognise that some applications can be run as \u0026ldquo;services\u0026rdquo; \u0026ndash; a set of identical instances of the application all running in parallel under a load balancer that evenly distributes instances of business tasks across these instances.\nWhere the orchestrators fall short is in the amount of operational concerns that they require developers to become aware of. That\u0026rsquo;s why I previously mentioned that suggesting these orchestrators as a computing model is like suggesting LLVM to someone looking for a modern general purpose programming language. The level of abstraction is too low.\nNot PaaS / FaaS Platform as a Service (PaaS) is essentially IaaS with some of the common applications and operational support made available by the PaaS provider through their APIs.\nAn IaaS provider would give you access to computing resources, leaving it up to you to run your application and also any supporting components of that application. A PaaS provider would, at the very minimum, include applications required to for smooth operation of your main applications.\nThere\u0026rsquo;s no limit to how many supporting applications a PaaS provider may bundle. Usually, PaaS providers include popular databases that they manage entirely on their own and only require you to access them via APIs or special client libraries in your code. Stretching the PaaS concept all the way takes you to Serverless computing, wherein you do not need to deal with any server provisioning, management or operations.\nPaaS, especially Serverless PaaS, offloads a lot of infrastructure provisioning and management but unfortunately it doesn\u0026rsquo;t move the conceptual needle much further beyond the IaaS model. As a developer you still need to know about all the components and you need to wire them together in code. The more abstracted the PaaS is (e.g. FaaS), the more tedious it becomes to code applications that don\u0026rsquo;t strictly fit its model.\nNot \u0026ldquo;Low Code\u0026rdquo; As we\u0026rsquo;ve gone through the \u0026ldquo;nots\u0026rdquo; above, we\u0026rsquo;ve moved closer to the ideal MetaComputer™ model. However, we now must take a wide diversion into Low Code Land.\nLow Code is a class of application development environments that comprise a lot of pre-developed functionality that you simply import and customise for your needs. Any functionality that is not covered by the customisation needs to be written in the regular (High Code?) way.\nIf programming were like sculpting, Low Code would be like assembling lego blocks. What we\u0026rsquo;re looking for is the 3D Printer with swappable materials.\nWhat Is the MetaComputer™? We have come the long way round to finally define what a MetaComputer™ is.\nA MetaComputer™ is a computing model with an associated programming language and runtime for distributed applications. The MetaComputer™ models elastic network services and abstracts their integration, change management and infrastructure.\nThe layman definition may be\nA MetaComputer™ is a magical technology that makes modern software development 10x more effective, scalable and reliable in a cost-efficient way.\nOr maybe\nA computing foundation that present day programmers can develop their skills on to last through to the end of their careers, while the MetaComputer™ abstracts technology changes into ever improving implementations of the MetaComputing Environment.\nPhew! That\u0026rsquo;s a lot to process in one go! By this point pretty much everyone I\u0026rsquo;ve talked to about this shurgs their shoulders and wonders if it\u0026rsquo;s even possible to implement such a system.\nWell, that would take us to the upcoming Part \u0026ldquo;How\u0026rdquo; of this series. That\u0026rsquo;s going to take me a while to write. Meanwhile, if you believe you have some idea of how to make this happen, join the MetaComputer™ organisation on Github.\nMy definition of a \u0026ldquo;computer\u0026rdquo; is an independently bootable chassis with one or more CPUs, RAM, network interface(s) and some non-volatile storage. This also includes VMs, unless the exclusion is pertinent.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Service is an elastically scalable network API with specific calling conventions, attached SL[I|O|A]s and access permissions\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://tahirhashmi.com/posts/2022-12-09-metacomputer-what/","tags":null,"title":"The MetaComputer™ (Part \"What\" of 3)"},{"categories":["Technology","Programming"],"contents":"It\u0026rsquo;s been a quite a while since I truly enjoyed programming at work. Don\u0026rsquo;t get me wrong. I like wrangling with code to make interesting stuff happen. The problem is that for a long time now, making interesting stuff happen with code hasn\u0026rsquo;t been the end game. Since the last ten years or so, it\u0026rsquo;s become incredibly more complex to get finished code to start working in the real world (aka production). Some say it\u0026rsquo;s because we OD\u0026rsquo;d on microservices. That probably true but there\u0026rsquo;s more to it than that alone.\nOf Skyscrapers and Music Back in the 1970s the popular opinion among software engineering experts was that software development is like building skyscrapers and bridges. You needed to know exactly what you want to build before you start writing punching code. That led to the (now vilified) waterfall model of software development.\nI believe the waterfall model made sense at that time. The programming was done on bespoke machines that were constructed for corporate customers and were usually housed in the same building as the corporate offices. The customers were all co-located, so they could give immediate feedback if something was broken.\nThe challenge was that the cost of producing broken software was pretty high for the programmer. Here\u0026rsquo;s a quote from someone on Quora about programming on old hardware:\nWhen I started to learn programming as a Physics student in the early 80s, I was sitting in front of a card typewriter and typed my [Fortran] program onto punched cards - one line per card with a maximum of 80 characters. To compile and run my program on a mainframe computer involved taking the stack of cards (never drop them or you will have to manually sort them!) to a card reader and “feed” them into the pre-processing unit.\nThe binary version of my program including some cards with some machine instructions was then put into a compile queue and once it was successfully compiled, it was placed into an execution queue. Based on one’s priority, it could take a few minutes or even half a day until your program was executed.\nThe output of this entire process was then printed onto “endless” paper tape and placed into an output bin for me to pick up. If I had a syntax or other error in my program, I would get a corresponding error message in my output and I had to start this process from the beginning after correcting the error.\nOuch.\nThings were much better for people who were programming in the 80s through to mid/late 2000s. The computers became interactive, thanks to the attached CRT displays and keyboards. Hardware was a lot more standardised, thanks to the IBM PC. Even if it wasn\u0026rsquo;t, a lot of hardware peculiarities were abstracted by higher level programming languages like C. CPUs used to be single-core. They weren\u0026rsquo;t much too powerful but rapidly gaining strength, as was the available RAM. Floppies and hard disks started appearing for data storage. Also, the Internet was still in its infancy.\nProgrammers back then spent most of their time sussing out the most efficient algorithms that would fit in the minuscule quantities of RAM (remember the 640 kB meme?) and run fast enough on the measly compute power of the CPUs of that era.\nThis was also the age of shrink-wrapped software where there was a definite end point for software changes before it went to the Floppy/CD masters.\nIn a way, this was the best time to be a programmer. The development environment was way less painful and the production environment (customers) were far removed from them, with support tickets being the only channel on which they could be\u0026hellip; bugged.\nThis was the time when software engineers decided that they were not construction workers but creative artistes like writers or music composers. They went on to invent concepts like agile development (which later got instutionalised beyond recognition) and eXtreme Programming.\nAh, these were the good old days that I can vouch for. Also, here\u0026rsquo;s an obligatory quote from Quora:\n[Programming in the 80s and 90s] was much less chaotic than it is today. In those days, I programmed in FORTRAN and C using fairly simple toolchains.\nThings haven\u0026rsquo;t been the same since. They\u0026rsquo;ve become worse, trust me.\nWelcome to the Hospitality Business By the start of 2010s, two massive developments changed the scene dramatically for the 90s\u0026rsquo; creative programmers.\nMulti-core CPUs with Non-Uniform Memory Architecture The Internet as the dominant software delivery channel See, back in the 90s and early 2000s, through all the major advancements undergoing mass consumption hardware, the conceptual computer architecture remained constant \u0026ndash; single-threaded x86 CPUs, with single-tier memory model and block storage devices.\nIf you doubt the impact that this architecture had on programming, just look at the sheer number of apps for PC/Mac that just don\u0026rsquo;t scale with multi-core CPUs even to this day \u0026ndash; more than 15 years on since the first dual-core Pentium was launched.\nAs for the Internet, well, it\u0026rsquo;s been a massive game-changer in software development. The bar for defensive programming has been raised multiple orders of magnitude \u0026ndash; the simplistic 1980s\u0026rsquo; memory abstraction as used in C/C++ is no longer even acceptable.\nEven more significantly, the software distribution model has changed dramatically. The most effective software monetisation model now is SaaS \u0026ndash; Software as a Service \u0026ndash; where you don\u0026rsquo;t ship neatly packed versions of code for others to run on their computers. You run your own code on your own computers and you\u0026rsquo;re responsible for ensuring that it\u0026rsquo;s running flawlessly 24x7x365\u0026hellip; x whatever the age of your company is.\nWelcome to the hospitality business where your customer is a paying visitor to your premises that happen to be located in a dangerous neighbourhood.\nThe customer rarely gives you feedback and doesn\u0026rsquo;t like opening support tickets, but always does judge your service \u0026ndash; the speed, the taste, the decor, the safety of your service. And if they don\u0026rsquo;t like your business they\u0026rsquo;d quietly walk away to the business next door that\u0026rsquo;s serving the same cuisine in the same price range but with different decor and cutlery.\nState of the Art One consequence of the programming-as-hospitality age is the rise of the \u0026ldquo;full cycle programmer\u0026rdquo;. The programmer doesn\u0026rsquo;t just write code and disconnect. The programmer needs to attend to how the program is running in production and address issues live. You can\u0026rsquo;t shut the restaurant down if a customer throws up between the tables. You isolate the fault and operate around it until it is remedied.\nThe full cycle programmer also needs to understand the context in which the program is executing. It\u0026rsquo;s not just the quirks of the immediate computer (server or container), it\u0026rsquo;s the entire topology of distributed services that the application is comprised of.\nAdd to this the need to modify the program while still maintaining \u0026ldquo;99.9[9\u0026hellip;]% uptime\u0026rdquo; and things get rather out of hand really fast. Like mentioned in this quora quote:\nWe have Docker, Kubernetes, AWS, Azure, and a ton of other tools that we must navigate. We have dozens of new languages trying to pull us away from the tried-and-true, for example: Clojure, D, Dart, Elixir, Elm, F#, Go, Haskell, Julia, Kotlin, Lua, OCaml, Rust, Scala, TypeScript, etc.\nAnd I’ve just barely scratched the surface! It’s all rather overwhelming.\nDocker, Kubernetes, tons of other tools and the dozens of new languages are all attempts to cope with the increased demands on security, parallelism and continuous operation.\nHowever, they\u0026rsquo;re not the solution. They\u0026rsquo;re merely stepping stones towards the solution, which begs the question \u0026ldquo;why do we have this complexity?\u0026rdquo;. The root cause is that we do not have a conceptual model for cloud computing and continuously evolving online services. Developing online software services without that model is like writing a modern desktop application with a programming language that pre-dates C. It\u0026rsquo;s just too complex.\nThat\u0026rsquo;s also the reason why I don\u0026rsquo;t enjoy programming as much now. I need to spend way too much effort in wiring together all the components of my distributed topology in a resilient, secure and scalable way. In some ways I can relate to that programmer who was punching Fortran programs onto cards \u0026ndash; way too much effort spent on working the machine, as opposed to having the machine do your work.\nIntroducing the MetaComputer™ In 1992, the NCSA showcased a \u0026ldquo;Metacomputer\u0026rdquo;, which they defined as follows:\nThe Metacomputer is a network of heterogeneous, computational resources linked by software in such a way that they can be used as easily as a personal computer.\nNice.\nThis was their schematic for the Metacomputer:\nNCSA LAN Metacomputer If you look at it carefully, it\u0026rsquo;s not that hard to extrapolate this schematic towards current cloud environments with neat sections for compute, storage and a web console to access all of these resources. Interestingly, the authors of the NCSA Metacomputer paper only talked at a surface level about the programming model.\nOne recent effort I know of that tries to offer a unified metacomputer presentation is Apache Mesos. Mesos aspires to be like Linux, except that it runs on a large cluster of machines and offers a unified view into them along with some resource management APIs.\nUnfortunately, that\u0026rsquo;s not the kind of abstration a lot of people really need. The places where Mesos is most applicable is with large scale batch jobs or self-terminating tasks rather than always-on interactive services.\nThe other contender is the greek k-word. I mean kubernetes. Kubernetes promises to solve a lot of problems associated with programming for large scale distributed systems. Unfortunately, it was designed by gods of intellect with lots of PhDs and places a very high bar on the intellect of people eligible to write programs for it.\nIf you don\u0026rsquo;t believe me, there\u0026rsquo;s a Downfall (Hitler) parody for it. There\u0026rsquo;s also a meme about it: Explaining Kubernetes Oh actually, there\u0026rsquo;s a Twitter account full of memes about kubernetes!.\nTo be fair, kubernetes is a great piece of engineering that solves many problems related to operating distributed applications in a reliable manner. However, for most developers, it\u0026rsquo;s too large scale low level. Recommending kubernetes as a computing model is like recommending LLVM when someone asks for a general purpose programming language. It\u0026rsquo;s not the right level of abstraction.\nAnyway, the point is, the average billion-dollar tech company of today demands a lot from its programming staff and the programmers are still working with an out-dated computing model that were meant for single-threaded non-networked computers.\nThis is why we need a new computing model with new conceptual building blocks and new programming models to build applications with them. Not the IaC, or CDK \u0026ndash; they\u0026rsquo;re great for automating infrastructure provisioning and access but that\u0026rsquo;s not really the concern of a programmer, even though it\u0026rsquo;s their responsibility.\nSo what is this computing model for, indeed? I\u0026rsquo;ll answer this question in The MetaComputer™ (Part \u0026ldquo;What\u0026rdquo; of 3) of this series, coming up next.\nMeanwhile, I\u0026rsquo;d love to invite you to join the MetaComputer™ organisation on GitHub to discuss further. The logo of the organisation was generated by DALL·E-2, by the way.\nPS: I didn\u0026rsquo;t include the evolution of front-ends (browsers, mobile apps) because there\u0026rsquo;s not a lot of disconnected apps (i.e. having no \u0026ldquo;backend\u0026rdquo;) so it\u0026rsquo;s inconsequential to the discussion here. I also didn\u0026rsquo;t include the evolution of data storage because SQL and the relational model have held strong as a conceptual model for the last five decades, despite some buzz about NoSQL.\n","permalink":"https://tahirhashmi.com/posts/2022-11-27-metacomputer-why/","tags":null,"title":"The MetaComputer™ (Part \"Why\" of 3)"},{"categories":["Photography","Reviews"],"contents":"When I\u0026rsquo;m on a business trip and I go for breakfast at the hotel\u0026rsquo;s restaurant, I want to take a few photos of the ambience and the food. However, my Nikon D750 feels a bit like an overkill for this job, as does my Nikon D500\u0026hellip; or Z6 or even Z50.\nWhat I want for this occasion is something svelte. Like a Nikon Coolpix A or a Nikon J5. Except the former is as usable as a butter knife for carving steak and the latter is often outdone by an iPhone.\nFor years, I\u0026rsquo;ve been searching for something that\u0026rsquo;s compact, elegant and competent \u0026ndash; a \u0026ldquo;Vanity Camera\u0026rdquo;, as I call it. After much deliberation, I settled on getting the world\u0026rsquo;s prettiest camera, a Fujifilm x100v. Except, as of this writing, I couldn\u0026rsquo;t find a new body in three different countries that I looked.\nSo, I settled for its interchangeable lens sibling, the Fujifilm X-E4. The X-E4 is marginally more compact compared to x100v. It shares the same imaging components as the x100v and only loses out on the sculpted handgrip and OVF.\nFujifilm X-E4 with XF 27mm f/2.8 R WR First Use Unboxing the Fujifilm X-E4 was a delightful experience and the camera looks breathtakingly beautiful out of the box. I got the kit that includes the XF 27mm f/2.8 R WR lens. Sadly, the kit doesn\u0026rsquo;t come with the lens\u0026rsquo;s teeny tiny absolute must-have hood.\nBeing a Nikon shooter, I looked at the lens with envy \u0026mdash; so compact, and shiny deep black with an aperture ring around it and its specifications printed boldly in front.\nFujifilm X-E4 with XF 27mm f/2.8 R WR As I mounted the lens, turned on the camera and tried taking a few pictures, my heart momentarily sank to the floor. As I half-pressed the shutter, the camera made rattling sounds and (I think) I could feel it vibrating in my hand. Took me a moment to realise that the camera wasn\u0026rsquo;t broken.\nThe rattling was the price of the lens\u0026rsquo;s compactness. The lens uses a noisy motor, reminiscent of the AF-D screw drive motor in old Nikon bodies. Its front element moves in and out while focusing. The last lens I owned that didn\u0026rsquo;t have internal focusing was the Nikon 50mm f/1.8D that I let go of 10 years ago. Most significantly, even the moving front element of that 50mm lens moved inside a cavity akin to a lens hood and didn\u0026rsquo;t pop out of its outer barrel.\nSo, while shooting against a window, I can\u0026rsquo;t ever touch the lens against the glass as I have been used to doing with every other lens that I ever used over the last 15 years.\nLens construction aside, the camera clearly felt slow compared to the x100v that I had tried in a store, but more of this in the next section.\nHandling and Usability The camera is pretty light and compact, but the lack of a grip makes holding it confidently a bit of a challenge. I got the OEM thumb-rest which, along with a wrist strap, improves the camera\u0026rsquo;s handling a lot. OEM and third-party front grips are available too but I didn\u0026rsquo;t get them because I want to keep things compact and I don\u0026rsquo;t plan on using large or heavy lenses with this body.\nTurning on the camera, it starts up pretty quickly \u0026ndash; something I can\u0026rsquo;t say for the Nikon Z6. The LCD monitor looks crisp and clear and I don\u0026rsquo;t have any complaints against the tiny little EVF either. I do wish the diopter adjustment wheel were a bit more stiff and/or recessed.\nA lot of existing Fujifilm users and popular reviewers have panned the minimalistic controls of the X-E4. However, I have no problems with the controls. The joystick is delightfully precise for its size and quite usable. I wish the Z6 had this exact joystick, instead of the mushy rubbery thing that it does have. The other buttons, especially the shutter button, feel very nice to operate. The lack of IBIS is a limiting factor, though.\nI am not using any of the 6 possible customisation groups. I\u0026rsquo;ve set the Fn button to toggle Face/Eye detection. The front dial cycles between ISO and Shutter Speed adjustments. I\u0026rsquo;ve also set swipe gestures \u0026mdash; swipe left, right, up or down for self-timer, horizon, film simulations and white balance respectively.\nI primarily use the camera with AF-C in tracking mode and matrix metering. Usually, the tracking works really well, except in some situations where the bigger AF box disintegrates into a group of smaller boxes that try to crawl along some edge. Out-of-the-box, the camera is sluggish in its AF but there\u0026rsquo;s a funny setting called \u0026ldquo;Boost Mode\u0026rdquo; that fixes this. Why Fujifilm decided to cripple camera performance by default is beyond my comprehension, especially considering that the battery life on this camera is quite good.\nFujifilm X-E4 with XF 18mm f/2 R The matrix meter works well in many situations but the camera doesn\u0026rsquo;t have highlight weighted metering, which means that I often have to use the EV compensation dial (mostly to under-expose) where I want to preserve highlight detail.\nThe EV dial also becomes indispensable while using face/eye detection mode, which is this camera\u0026rsquo;s downfall. It seems like the camera simply doesn\u0026rsquo;t have enough smarts to handle face/eye priority mode.\nOne of the challenges is the camera\u0026rsquo;s inability to detect faces sideways. It also doesn\u0026rsquo;t like faces with full beards (or certain styles of beard, maybe). When the camera detects eyes, it sometimes becomes indecisive and keeps switching rapidly between left and right eye. Incidentally, there\u0026rsquo;s a setting to prioritise one eye, but the choice is between left or right eye. This makes no sense to me \u0026ndash; the priority should be the eye closer to the camera.\nThis indecisiveness reaches catastrophic levels in a group setting. Usually the camera prefers to focus on a distant face even if there\u0026rsquo;s a more prominent face closer to the camera. When multiple faces are detected, the camera sometimes sticks with one (distant) face and sometimes rapidly jumps between faces.\nThe camera disables all metering choices when face detection is enabled, choosing to ensure good exposure for the face or eye. This is fine in some situations but in other situations \u0026ndash; especially with eye detection \u0026ndash; the exposure goes dramatically off.\nWhile shooting in continuous mode, there\u0026rsquo;s no guarantee that consecutive shots in a burst will have a consistent exposure, white-balance or even focus. This is the reason why my Fn button is set to toggle Face/Eye detection off.\nImage Quality and Samples Singapore Changi Airport The X-E4 is not as much a raw shooter\u0026rsquo;s camera as it is for JPEG shooter\u0026rsquo;s. I say this for several reasons.\nOne, the raw files don\u0026rsquo;t have much exposure latitude either in highlights or in shadows, even at base ISO. There is quite a bit of noise at high ISOs as well. At the back of the camera, and in the JPEG files, even ISO 12800 looks decent. But raw processing doesn\u0026rsquo;t really bring out anything significant.\nThen, the dynamic range management on this camera is unusual. With bayer sensors, you get the best dynamic range at base ISO. With this camera at its base ISO of 160, the matrix meter will happily blow out the sky every time. If you ask the camera to optimise for dynamic range, the camera chooses to shoot at ISO 320. The most aggressive dynamic range prioritisation makes the camera shoot at ISO 640, even in daylight.\nBangalore Sky Coming to colours, the camera includes the film simulation information into the raw file, which their recommended raw converter (Capture One) faithfully applies. However, the camera also has a feature called \u0026ldquo;Color Chrome Effect\u0026rdquo; that beautifully darkens saturated colours (independently controlled for blue and non-blue). This feature can not be emulated in raw and requires painstaking adjustment using saturation masks, which I think only Photoshop and The Gimp support.\nBangalore Sunset Lastly, you can not transfer raw files to your smartphone. Only JPEGs can be transferred.\nI\u0026rsquo;ve mostly got reasonably good JPEG results with Automatic DR Optimisation and Color Chrome Effect turned on for all colours.\nTalking of film simulations, the Standard (Provia) simulation provides the most versatile results. The Cinema (Eterna) film is great for lo-fi look or softening the contrast. Other colour film simulations are more nostalgic than aesthetic. I haven\u0026rsquo;t tried much of monochrome shooting.\nLenses I use this camera with the XF 18mm f/2 R lens and the XF 27mm f/2.8 R WR kit lens.\nThese are the smallest lenses in Fujifilm\u0026rsquo;s lens line-up. As mentioned before, the compact size comes by compromising on conveniences like internal zoom, quiet ultrasonic motors, and stabilisation. Fujifilm Fujinon XF 18mm f/2 R In terms of rendering, both lenses have pleasing bokeh and are quite sharp. They do have gobs of chromatic abberation and distortion given their compact nature, but that\u0026rsquo;s taken care of through digital correction. Both lenses also exhibit corner softness, which I understand to be because of field curvature but I don\u0026rsquo;t know much about it.\nThe 27mm flares very badly, though, and it does so in a way that even its lens hood can\u0026rsquo;t really mitigate that. It also doesn\u0026rsquo;t focus quite close and that\u0026rsquo;s an annoyance while taking food pics \u0026ndash; one of my important use cases.\nXF 27mm SooC JPG sample with flaring along the right edge Alternatives Fixed Prime Lens Compacts The Ricoh GR III/IIIx, Fujifilm x100v, Leica Q2 and Nikon Coolpix A are some of the alternatives in that they\u0026rsquo;re equipped with APS-C or larger sensors and lenses in the 28-40mm (35mm equivalent) range. Of these, I have only tried the x100v and Coolpix A. This camera comprehensively beats the Nikon. Its performance is supposed to be at par with x100v but the latter trumps it in handling and lens quality. The Ricoh GR III is supposedly even more frustrating and quirky.\nNikon Z50 / Z fc / Z30 Compared to the APS-C Nikon mirrorless cameras, the X-E4 is smaller than all except the viewfinder-less Z30. In terms of operation, though, the Nikon Z cameras beat it hands-down. They are significantly more smart (read consistent and accurate) in face/eye detection mode, have comparable AF speed (except non-face tracking AF), and offer a highlight weighted metering option that works even with face detection.\nWith Nikon\u0026rsquo;s impeccable exposure metering and the great dynamic range latitude in its raw files, I\u0026rsquo;ve never ever wanted a dedicated exposure compensation dial. For the Z fc, I wish Nikon had replaced the exposure compensation dial with something more important, like AF-mode or something.\nTalking of Z fc, though, the silver finish on that camera looks disgusting compared to the fine finish of the X-E4.\nSony A7C Perhaps the camera that most closely resembles the X-E4 in form and function is the Sony A7C. Although it doesn\u0026rsquo;t look as pretty at first glance, it\u0026rsquo;s actually better built and finished than the X-E4. It has all the legendary AF smarts from the House of Sony that have put the Sony Alpha mirrorless cameras at the top of the market-share. It\u0026rsquo;s also ergonomically more comfortable due to its front grip and chunky body.\nThe only challenge with that camera is that Sony doesn\u0026rsquo;t really know how to render pleasing colours out of the box. However, that\u0026rsquo;s hardly an insurmountable challenge compared to the shortcomings of the X-E4.\nConclusion The Fujifilm X-E4 is a small little camera with a highly competent core imaging gadgetry at its disposal. It requires the photographer to be continuously mindful of the shooting parameters, whose operation it makes quite convenient with the well appointed buttons and dials. At the end of the process, it delivers some of the best looking photos when viewed at the back of the camera, triggering a sense of satisfaction that\u0026rsquo;s hard to achieve. Reviewers often label this experience as \u0026ldquo;engaging\u0026rdquo;.\nBut the good stuff ends there. On the computer, those same photos that looked gorgeous on the LCD screen seem to lose a lot of their flair.\nAs for the engagement, I want the camera to get out of my way and achieve the results I want without fussing. I need to be able to set the camera and hand it over to anyone, including my 9 year old, and trust that they would be able to shoot the same quality of photos as I can. No engagement necessary.\nThis is something I am able to achieve with the Nikon Z system. The files from Nikon Z cameras, unlike many others, look better on the computer than in camera. And that\u0026rsquo;s just the beginning given their immense pliability.\nSo the Fujifilm X-E4 is a nice camera for those who want a basic, old-school shooting experience along with nostalgic film simulations.\nBut for me, the Fujifilm X-E4 is as frustrating as it is good looking. Look but don\u0026rsquo;t touch\u0026hellip; or buy.\nSample Images on Flickr\n","permalink":"https://tahirhashmi.com/posts/fujifilm-x-e4/","tags":null,"title":"Fujifilm X-E4: An Engaging Experience I'd Rather Not Have"},{"categories":["Photography","Programming"],"contents":"Following is a collection of real exiftool commands that I’ve used, along with explanations of what each does. exiftool is a command-line utility that provides very powerful EXIF reading, writing and searching capabilities.\nI’m writing this down because I often spend a lot of time reading through exiftool documentation to find out how to get something done, just to forget it within hours. All of these examples work on a Unix shell environment like ZSH on MacOS or the various Linux shells.\nExtracting EXIF Information What does EXIF data look like? Let’s try the following on an image file, DSC09535.JPG:\n$ exiftool -all DSC09535.JPG ExifTool Version Number : 11.59 File Name : DSC09535.JPG Directory : . File Size : 11 MB File Modification Date/Time : 2020:02:02 14:38:34+07:00 File Access Date/Time : 2020:07:26 09:11:20+07:00 File Inode Change Date/Time : 2020:05:05 11:41:44+07:00 File Permissions : rwxrwxrwx File Type : JPEG File Type Extension : jpg MIME Type : image/jpeg Exif Byte Order : Little-endian (Intel, II) Image Description : Make : SONY Camera Model Name : DSC-RX10M3 ... 165 lines snipped ... Focal Length : 220.0 mm (35 mm equivalent: 600.0 mm) Hyperfocal Distance : 1098.31 m Light Value : 5.9 This simplest exiftool invocation that dumps all EXIF data for the file. The default output shows tag descriptions and their readably formatted values. In order to see the actual tag names, use the -s option.\n$ exiftool -s -all DSC09535.JPG ExifToolVersion : 11.59 FileName : DSC09535.JPG Directory : . FileSize : 11 MB FileModifyDate : 2020:02:02 14:38:34+07:00 FileAccessDate : 2020:07:26 09:11:20+07:00 FileInodeChangeDate : 2020:05:05 11:41:44+07:00 FilePermissions : rwxrwxrwx FileType : JPEG FileTypeExtension : jpg MIMEType : image/jpeg ExifByteOrder : Little-endian (Intel, II) ImageDescription : Make : SONY Model : DSC-RX10M3 ... 165 lines snipped ... FocalLength35efl : 220.0 mm (35 mm equivalent: 600.0 mm) HyperfocalDistance : 1098.31 m LightValue : 5.9 Now it becomes easier to query for specific values in a whole collection of files. For example, here’s how to list the 35mm equivalent focal length for all files in the current directory:\n$ exiftool -FocalLength35efl . ======== ./DSC09535.JPG Focal Length : 220.0 mm (35 mm equivalent: 600.0 mm) ======== ./DSC09568.JPG Focal Length : 73.2 mm (35 mm equivalent: 200.0 mm) ======== ./DSC09540.JPG Focal Length : 220.0 mm (35 mm equivalent: 600.0 mm) ======== ./DSC09556.JPG Focal Length : 13.0 mm (35 mm equivalent: 35.0 mm) ======== ./DSC09550.JPG Focal Length : 220.0 mm (35 mm equivalent: 600.0 mm) ======== ./DSC09574_DxO.jpg Focal Length : 110.1 mm (35 mm equivalent: 300.0 mm) ======== ./7DX_0300_DxO.jpg Focal Length : 200.0 mm (35 mm equivalent: 300.0 mm) ======== ./DSC09574.JPG Focal Length : 110.1 mm (35 mm equivalent: 300.0 mm) ======== ./DSC09570.JPG Focal Length : 220.0 mm (35 mm equivalent: 600.0 mm) ======== ./DSC09564.JPG Focal Length : 110.1 mm (35 mm equivalent: 300.0 mm) ======== ./DSC09558.JPG Focal Length : 146.8 mm (35 mm equivalent: 400.0 mm) ======== ./7DX_0300.JPG Focal Length : 200.0 mm (35 mm equivalent: 300.0 mm) ======== ./DSC09539.JPG Focal Length : 220.0 mm (35 mm equivalent: 600.0 mm) 1 directories scanned 13 image files read I often like to perform some quantitative analysis on my photo metadata, so it is helpful to start with data that can be imported into a spreadsheet. Luckily, exiftool can export the above data as CSV too. Example:\n$ exiftool -csv -Model -Aperture -ShutterSpeed -ISO \\ -FocalLengthIn35mmFormat . \u0026gt; stats.csv 1 directories scanned 13 image files read $ cat stats.csv SourceFile,Model,Aperture,ShutterSpeed,ISO,FocalLengthIn35mmFormat ./DSC09535.JPG,DSC-RX10M3,4.0,1/60,1600,600 mm ./DSC09568.JPG,DSC-RX10M3,4.0,1/30,64,200 mm ./DSC09540.JPG,DSC-RX10M3,4.0,1/250,400,600 mm ./DSC09556.JPG,DSC-RX10M3,4.0,1/25,64,35 mm ./DSC09550.JPG,DSC-RX10M3,5.6,1/250,64,600 mm ./DSC09574_DxO.jpg,DSC-RX10M3,4.0,1/1600,64,300 mm ./7DX_0300_DxO.jpg,NIKON D7500,5.6,1/800,100,300 mm ./DSC09574.JPG,DSC-RX10M3,4.0,1/1600,64,300 mm ./DSC09570.JPG,DSC-RX10M3,4.0,1/30,64,600 mm ./DSC09564.JPG,DSC-RX10M3,4.0,1/40,64,300 mm ./DSC09558.JPG,DSC-RX10M3,4.0,1/125,64,400 mm ./7DX_0300.JPG,NIKON D7500,5.6,1/800,100,300 mm ./DSC09539.JPG,DSC-RX10M3,4.0,1/250,400,600 mm If you don’t want the numeric values to be formatted, use the -n option to extract raw numbers. Example:\n$ exiftool -csv -n -Model -Aperture -ShutterSpeed -ISO \\ -FocalLengthIn35mmFormat . SourceFile,Model,Aperture,ShutterSpeed,ISO,FocalLengthIn35mmFormat ./DSC09535.JPG,DSC-RX10M3,4,0.01666666667,1600,600 ./DSC09568.JPG,DSC-RX10M3,4,0.03333333333,64,200 ./DSC09540.JPG,DSC-RX10M3,4,0.004,400,600 ./DSC09556.JPG,DSC-RX10M3,4,0.04,64,35 ./DSC09550.JPG,DSC-RX10M3,5.6,0.004,64,600 ./DSC09574_DxO.jpg,DSC-RX10M3,4,0.000625,64,300 ./7DX_0300_DxO.jpg,NIKON D7500,5.6,0.00125,100,300 ./DSC09574.JPG,DSC-RX10M3,4,0.000625,64,300 ./DSC09570.JPG,DSC-RX10M3,4,0.03333333333,64,600 ./DSC09564.JPG,DSC-RX10M3,4,0.025,64,300 ./DSC09558.JPG,DSC-RX10M3,4,0.008,64,400 ./7DX_0300.JPG,NIKON D7500,5.6,0.00125,100,300 ./DSC09539.JPG,DSC-RX10M3,4,0.004,400,600 As you can see, the shutter speed has changed from a fraction, like 1/25 to a floating point value like 0.04 and the mm unit has been dropped from the equivalent focal length field.\nUpdating EXIF data One of my frequent requirements is to ensure that post-processed files have the right EXIF data. This is often needed when stitching panoramas or image stacks from multiple source images. The easiest way is to make exiftool copy the data from the corresponding raw file.\nexiftool -tagsfromfile ../../DSC_2177.NEF superblend*.jpg Here, the -tagsfromfile option picks up all the EXIF data from the first file and overwrites it into the subsequent files.\nSometimes, a bad image editor might generate files with wrong EXIF data. The following example fixes such occurences by matching JPEG files against their corresponding raw (NEF) files and copying the data from the latter to the former.\nfor c in `ls -C1 *.jpg`; do exiftool -P -overwrite_original -tagsfromfile ../${c/.jpg/.NEF} $c; done; Note the two new options here:\n-P is used to ensure that file modification date/time is not updated to the time when the command is executed -overwrite_original is used to modify the original file and not save a backup of the original (refer to documentation for more details) Date/Time Adjustments It’s really convenient to have the timestamp of the processed photos match the time when they were originally shot. However, some image processors export files with the latest timestamps. One example is Nikon Studio NX, that not only doesn’t adjust the modification time, but also adds a new EXIF tag for when the processed file was generated. Here’s how I fix this:\nexiftool -overwrite_original \u0026#39;-FileModifyDate\u0026lt;DateTimeOriginal\u0026#39; \\ \u0026#39;-ModifyDate=\u0026#39; . The above command operates on all files in the current directory (.), deletes the ModifyDate tag created by Studio NX and sets the actual file modification date (FileModifyDate) to the time when the picture was taken (DateTimeOriginal).\nAnother somewhat recurring requirement is to adjust all EXIF timestamps by some difference. This happens when I’m travelling with multiple cameras, whose clocks are not in sync, and sometimes I forget to match the timezone to that of the destination. Fortunately, it is really easy to make these adjustments with exiftool.\nFor example, the following shots were taken at the same time but with different cameras. Unfortunately, the clock of one camera was way off.\n$ exiftool -p \u0026#39;$filename $DateTimeOriginal $Model\u0026#39; DSC09574.JPG 7DX_0300.JPG DSC09574.JPG 2020:05:03 13:31:50 DSC-RX10M3 7DX_0300.JPG 2020:05:03 15:08:22 NIKON D7500 By the way, I have used formatted printing capability of exiftool here using the -p option, but I’m not going to discuss it here.\nTo get the correct time, I need to add 1 hour 36 minutes and 31 seconds to all photos from the DSC-RX10M3 camera. Easy peasy. Here’s how:\nexiftool -alldates+=\u0026#34;0:0:0 1:36:31\u0026#34; -if \u0026#39;$Model eq \u0026#34;DSC-RX10M3\u0026#34;\u0026#39; . Here we use the alldates tag, which is a shorthand for 3 different date fields, and shift it forward using += operator by a delta represented as Y:M:D h:m:s for year, month, date, hour, minute, second respectively. To shift the date backward, we could use the -= operator instead.\nThe -if option is used to inspect tag values and only act on files where the values satisfy the condition in the expression. E.g. In the above example, the dates are adjusted only for files shot with “DSC-RX10M3” camera model.\nEditing IPTC Metadata Having been a Flickr user since 2004, I had been maintaining titles, descriptions, and keywords (Flickr “tags”) very carefully. After 15 long years, I decided I needed to have that metadata in my photo files offline too, and not just on Flickr.\nOne of the biggest challenges there was to maintain a consistent keyword set for cameras and lenses. I tag each of my photos with the body and lens that was used to click the photo.\nThankfully, exiftool comes in really handy for inspecting the camera and lens data from the original file and adding the appropriate keywords. Here’s how:\nexiftool -P -overwrite_original -addtagsfromfile @ \\ \u0026#39;-keywords+\u0026lt;${model;s/NIKON //;s/ //g}\u0026#39; \\ \u0026#39;-keywords+\u0026lt;${lensid;s/(AF-.|[A-Za-z]{3,}|ED)//g;s/\\s+/ /g;s/^ *//;s/ *$//}\u0026#39; Let’s break it down, option by option.\nThe -P option ensures that the edited file has matching timestamps as the original The -overwrite_original prevents creation of backup files The -addtagsfromfile option is a variant of -tagsfromfile that allows repeated copying of values into the same tag to be appended rather than overwritten. This is important because here we are adding multiple values into the keywords tag @ is a reference to the destination file as the source file too, since we are reading values from the same file and writing them back -keywords indicates that we’re editing the (IPTC) keywords tag + is to add to it and \u0026lt; means to copy the value from the right hand side of the \u0026lt; operator ${\u0026lt;tag\u0026gt;;\u0026lt;expr\u0026gt;[;\u0026lt;expr\u0026gt;...]} is a way to read tag and apply expr regex substitutions in it. E.g. in the first one, I need to strip NIKON and strip away whitespaces from the model name so NIKON Z 6 would be tagged as Z6 in the keywords. The last line is a bit more complicated due to the rules I follow to generate the lens keywords. The result is Model, Lens pairs in the keywords like this:\nD750, 70-200mm f/4G VR D7500, 300mm f/4E PF VR D7500, 500mm f/5.6E PF VR Z6, 500mm f/5.6E PF VR Z6, 70-200mm f/4G VR Z6, Z 24-70mm f/4 S A File Import Tool One challenge with the above example for adding camera and lens tags was that as a stand-alone script, I had to remember to run it for every batch of files that I import. It would be a lot easier if the tags got applied automatically at the time of importing the photos in the first place. Thankfully, exiftool also provides features for file movement and copying. All I had to do was add one line at the end of the previous example.\n#!/bin/zsh exiftool -P -addtagsfromfile @ \\ \u0026#39;-keywords+\u0026lt;${model;s/NIKON //;s/ //g}\u0026#39; \\ \u0026#39;-keywords+\u0026lt;${lensid;s/(AF-.|[A-Za-z]{3,}|ED)//g;s/\\s+/ /g;s/^ *//;s/ *$//}\u0026#39; \\ \u0026#39;-Directory\u0026lt;CreateDate\u0026#39; -o . -d $HOME/Pictures/%Y/%Y-%m -r $SOURCE Let’s examine the parts of the last line.\n-Directory\u0026lt;CreateDate informs exiftool that we would be using data from CreateDate tag for synthesising the file destination path -o . indicates that we need to copy the files, rather than move them (seems to be a special case for this usage) -d provides the destination, in this case $HOME/Pictures/ being the base directory under which subdirectories in the format of YYYY/YY-MM are created as specified by the %Y/%Y-%m format string. -r $SOURCE makes exiftool scan $SOURCE directory and its subdirectories for files to be copied. Please note that if any file doesn’t have the CreateDate tag in it, that file would not be copied. This command imports all new files into folders of the following structure, while also applying camera and lens tags and preserving the file timestamps as in the camera.\n2021 ├── 2021-01 ├── 2021-02 ├── 2021-03 ├── 2021-04 └── 2021-05 Searching by EXIF data One of the most powerful uses of exiftool on a daily basis is being able to search through the images based on EXIF data. For example, I was interested in examining the general “look and feel” of photos shot in the medium telephoto field of view. One challenge was that the exact focal length for different format sensors would be different. Luckily, we have the FocalLengthIn35mmFormat tag that shows a normalised value of focal length.\nNext, all I had to do was to list out all the photos that fell within a range, which I chose to be 105mm to 200mm. Here’s the command I used:\nexiftool \\ -if \u0026#39;$FocalLengthIn35mmFormat# \u0026gt;= 105\u0026#39; \\ -if \u0026#39;$FocalLengthIn35mmFormat# \u0026lt;= 200\u0026#39; \\ -p $PWD/\u0026#39;$directory/$filename\u0026#39; \\ */Finished \u0026gt; teles.lst Let’s go through this option by option.\nThe first -if option checks whether the FocalLengthIn35mmFormat is \u0026gt;= 105mm. We need a numeric comparison here so the tag value needs to be extracted as a number rather than a string. There are two ways to do this. One is to append a # character at the end of the tag name, as we have done here. The other is to use the -n option to apply to all the tags. The second -if option is very similar and checks whether FocalLengthIn35mmFormat is \u0026lt;= 200mm. The -p option is used to print the file names in a specific way – the first part is the full path of the present working directory as captured by $PWD followed by a trailing /. This is then followed by the directory in which the matching file was found using the $directory tag and lastly we print the $filename with a / between the two to make the complete path. The search space is all the directories matching */Finished path and the results are saved in a file called teles.lst This part is pure shell, nothing to do with exiftool. By the way, I used XnViewMP to open the above file list as if it were a “virtual directory” containing all of those files, which then, among other things, allowed me to view a fullscreen slide show of all matching photos.\nAnother search example is a riff on the above, except I’m more interested in photos with a 30mm FoV +/- 5mm, but shot with a specific lens, which we match by a regex on the LensId tag.\nexiftool \\ -if \u0026#39;$FocalLengthIn35mmFormat# \u0026gt; 25\u0026#39; \\ -if \u0026#39;$FocalLengthIn35mmFormat# \u0026lt; 35\u0026#39; \\ -if \u0026#39;$LensId=~/24-70.*S/\u0026#39; \\ -p $PWD/\u0026#39;$directory/$filename $LensId\u0026#39; 2020-* \u0026gt; zoom-30.lst Closing Thoughts \u0026amp; References While I’ve verified all of the examples here to be working as per my usage, differences in our environments (most importantly, Operating Systems) might cause some of them to not work.\nThe explanations are my understanding of the official documentation and various forum posts. Please feel free to point out any inaccuracies that need correction.\nFor further information look at the exiftool documentation or look through the forum for answers. There is also a vast collection of helpful resources.\n","permalink":"https://tahirhashmi.com/posts/2021/05/06/exiftool-examples/","tags":["cataloguing","EXIF","exiftool","software"],"title":"exiftool Examples"},{"categories":["Technology"],"contents":"I have a properly working Seagate Backup Plus Hub. However, I’m now using a Samsung 870 QVO for Time Machine backups on my Mac, despite its bottom-of-the-pile TBW (durability) rating. It actually makes sense.\nThe Backstory I have a 2017 iMac (bought that year as well) which I use as my primary home computer for photo/video editing, a bit of programming and a lot of Netflix and HBO Go as well. About a year ago, I got myself a bunch of disks and Bluetooth headphones in a short span of time.\nI plugged in a 1 TB Seagate Backup Slim HDD for extra storage, along with a 2 TB Seagate Backup Slim for Time Machine backups. The two of them later got augmented with a 6 TB Seagate Backup Plus Hub HDD, which I partitioned into a 4 TB Time Machine volume and a 2 TB archival volume.\nRoughly around the time that I made these “enhancements”, I started observing very frequent Bluetooth failures. My keyboard or, more often, trackpad would randomly disconnect.\nThe Suspects Reading through countless posts on apple.stackexchange.com and discussions.apple.com, I came across the following top suspects:\nToo many devices on the Bluetooth interface\nToo much interference in the 2.4 GHz frequency range\nI only had a Magic Keyboard 2 and Magic Trackpad connected wirelessly to the iMac, along with my Bluetooth headset. So, I got rid of the thing that was disposable – the headset.\nNo improvement.\nI switched from the Magic Trackpad to Magic Trackpad 2 (mostly because the former used AA batteries while the latter has internal rechargeable battery). That… didn’t achieve much either.\nBack to the Disks This is a post about backup disks. Why are we digressing? On the disk side, I changed things a little bit – the 1 TB Backup Slim was replaced with a 1 TB Samsung T5 Thunderbolt SSD. Not a speed demon like the NVMe’s (that overheat quickly) but still 4x faster to write and 6x faster to read than the HDD.\nI moved all my photo/video working data to the SSD and the system was significantly more responsive… except when it wasn’t due to the remaining two backup drives. The Backup Plus Hub frequently caused the iMac to freeze while spinning up.\nI unchecked the “Put hard disks to sleep” option and that made a significant improvement. I was still mildly annoyed with the near constant rumble of disk activity but it was a good thing to have nonetheless. Ever since I started using Time Machine, I’ve had several minor accidents averted due to the backups.\nSerendipity Last week, I had another one of those issues when all my Bluetooth devices – keyboard, trackpad and mouse (wife’s preference) – simply refused to connect. I reconnected them one-by-one using a lightning cable but the trackpad kept failing. I tried resetting Bluetooth using the debug menu (⌘⌥⇧-click on the Bluetooth icon in status bar) and then it struck me – during all this drama, the Backup Plus Hub was furiously busy writing.\nA bit more of “googling” soon landed me at a lengthy discussion where people were discussing how USB-A devices were causing Bluetooth connections to fail – some claiming interference, others claiming sleep/wake issues. I looked at my iMac’s 4 USB ports and they were all occupied – three without any devices on the other end but one with the Backup Plus Hub.\nThe big noisy USB-A HDD had to go.\nThe Replacement If you’re thinking about replacing your backup HDD with an SSD, the first thought is the price shock. In terms of cost per TB, even a SATA III SSD could be 8x the price! Is the price really worth it? For a backup disk?\nTo answer that, we need to know what the expected benefits of an SSD for Time Machine would be.\nAside from the sluggish sleep/wake cycle that causes a mac to occasionally freeze, the other aspect is the frequent disk activity that always puzzled me for a drive that just took hourly backups. When I investigated this, it turned out that there’s a lot of Spotlight indexing activity that goes on in the Time Machine volume. And, as of Mac OS Catalina, it is impossible to disable indexing on a Time Machine volume.\nSo, the Time Machine disk will be busy most of the time.\nTalking about price per TB, one of the new options, especially for high capacity drives is the QLC (Quad Layer Cell) SSDs that are about half the price per TB compared to “regular” SSDs. If you read up about it, you’ll find that these SSDs have a longevity compromise.\nData storage cells in SSDs degrade a bit every time they are overwritten. Multi-layer cells degrade faster. Thankfully, manufacturers now publish a metric called TBW (Tera Bytes Written) in the SSD specifications to indicate just how much data can be reliably written to the disk before its cells start to fail.\nThe 4TB Samsung 870 QVO has a TBW rating of 1440. Is 1440 TB of writes a high number or low number? Perhaps it’s good to decide on a life expectancy for a drive. I pick 10 years – that’s the amount of time in which any data written to the SSD would potentially fade out. This duration is different from degradation due to rewriting. It’s also a reasonable duration after which the existing disk size would be deemed insufficient for future use, making the disk’s replacement imminent.\n1440 TBW over 10 years translate to 16GB of data written per hour per day every day for those 10 years. My Time Machine backups, on the other hand clock in at ~ 800MB per hour on average. That’s a whopping 20x headroom in longevity measured by TBW for a “cheap” SSD.\nResults I’ve had my 870 QVO attached through a USB 3.1 Gen 2 interface along with the T5 for about 2 days now. Things are looking good. The trackpad even feels more responsive, where it used to be sluggish earlier on. It took a bit of “challenging the convention” to go with the 870 QVO but it’s delivering on the promise of improved responsiveness from the iMac and more stable Bluetooth peripherals.\n","permalink":"https://tahirhashmi.com/posts/2020/09/10/using-a-qlc-ssd-for-backups-makes-sense/","tags":null,"title":"Using a QLC SSD for Backups. Am I Insane?"},{"categories":["Technology"],"contents":"One of the decisions a CTO has to frequently make is whether to build some piece of functionality in-house or buy it from a third party vendor. In this post, I share my framework for making these decisions.\nI also include a case study each for a buy decision and a build decision. Added bonus – some thoughts on whether you should sell something you’ve decided to build.\nFunctional Complexity One of the most important determinants of buy-vs-build decisions will be the ability to spec out the entire functionality. Often we know what the primary functionality should be – e.g. being able to show metrics on a graph (a monitoring solution). The devil is in the details, though, and spending some time in discovering peripheral functionality is very helpful in avoiding a situation where you jump head-first into building something seemingly simple and then get stuck.\nE.g. how does the monitoring solution handle server-churn for the services it is monitoring? Does it rerun the entire query when auto-refresh is turned on? etc.\nOne effective way to discover peripheral functionality is to look across competing products that offer the solution and see how they differentiate themselves from competitors. Each of those differentiators is functionality that is relevant to some customer, and could be relevant to you as well.\nTechnical Complexity Knowing what to build (functionality) is a starting point. The next is to understand whether you have the capability to build that functionality. It’s not uncommon to find that functionally complex products are not technically complex and vice-versa. Functional complexity really arises out of the uncertainty in specifying the desired behaviour in a way that is known to be easy to code.\nTechnical complexity arises out of the ability to meet the functional requirements in repeatable and reliable ways. E.g. interactive information retrieval remains a technically complex domain despite years and years of R\u0026amp;D into the field. The business requirements and scale requirements keep growing to outpace any simplifying developments that have taken place in the field.\nHaving said that, this is one of the less difficult criteria for evaluation. Provided you have a good enough understanding of the functional complexity, you know whether you have the skills to build it in-house or whether you can hire to build the skill-set.\nOperational Complexity Things that are easy to build could be deceptively difficult to operate. Keeping something running reliably with predictable performance under stress is especially hard.\nHaving spent the last 10+ years handling “discount sale” events for book publishers to game engines to e-commerce platforms, I have a special distaste for all the F/OSS technology that works beautifully at small scale, only to blow up spectacularly under high load at a time when the business critically depends on it to hold up well.\nAnd therein is a signal for you to understand whether DIY might be a good idea. If you see a product for which there’s no shortage of F/OSS offerings, yet there are “enterprise” offerings that are highly profitable, it’s a strong indicator of operational complexity.\nIn general, if you want to treat some technology as a black box, don’t use F/OSS [that you operate yourself]. If you’re operating something on your own, it’s best if you have built it or you have the ability to benefit from the openness of its source and understand exactly how it works under the hood. Another alternative is to understand its behaviour empirically by stressing it beyond your foreseeable requirements and figuring out how you’ll handle its failures. Both of these – understanding through code or empirical behaviour – are expensive propositions.\nUnique Business Value The last evaluation criteria (but not the least important by any means) is how the component you’re to buy or build acts as a differentiator for your business.\nThe differentiator consideration is extremely important. I’ve been in many situations where people argue that we should build something in-house because it’s business-critical. Well, Internet is business-critical but pretty much no one is building their own telco network, are they?\nThere is a very dire consequence of deciding to build something that is not a differentiator for your business – lack of long term investment into the component. When deciding to staff a team, the business will always prefer to allocate more to components or teams where they can see a clear, tangible business benefit. The less that a component contributes to business differentiation, the less focus it will have from a staffing perspective.\nSoon you’ll find that the component that was cutting-edge at the time you built it has turned into a dinosaur few years down the line while the state-of-the-art has progressed much further, potentially becoming even more cost effective.\nHow to measure cost in Buy vs. Build Here’s a retrospective mini-game for your decision-making maturity on cost comparisons. You’ve already played this before. Now see what level you’ve landed at.\nLevel 1. Technology Cost: You compare the cost of servers and other operational components and see how much you might save.\nLevel 2. Staffing Cost: You throw in the CTC of the developers who will be building and operating the component.\nLevel 3. Business Development Cost: Now this is a bit tricky. If you allocate the budget from Level 2 cost (i.e. your sharp engineers) to building direct business value how much additional revenue would you gain? If you pull existing engineers who are directly building business value, how much revenue would you lose? For a high growth startup that’s still executing on its product plan, this cost could be really high.\nLevel 4. Loss of Business Cost: This is even more tricky than Level 3 and requires comparing monetary impact of SLAs and Quality of Service. Things to consider here include impact on revenue per minute if the component fails and impact on revenue if the component degrades in performance or reliability. Loss of revenue due to reduced engineering and business productivity due to poor performance is another cost factor.\nLevel 5. Business Opportunity Cost: This is the most interesting aspect of making a buy vs. build decision. In a nutshell, it captures the impact of lack of investment (build) or control (buy) in the evolution of the product under consideration. It’s also closely tied to the differentiation factor. If it’s a low differentiator built in-house, the most likely future is that it will get understaffed, leading to growing Level 4 costs while the rest of the world advances in capability. In-house high differentiators have the potential to increase future differentiation to the extent that they could become sources of direct revenue.\nCase Study 1: Chat Server – Build Not too long ago, at Tokopedia we needed to build peer-to-peer and group chat capabilities. We already had some experience with using 3rd party solutions for reference. Here’s how the above decision framework applied to this decision.\nFunctional Complexity: chat functionality is well understood and we were able to nail down most of the core and peripheral functional requirements with clarity. This was validated against the core feature-set from 3rd party solutions.\nTechnical Complexity: chat servers have traditionally been notoriously complicated pieces of technology to build. Keeping end-to-end latency under predictable bounds while handling hundreds of thousands of concurrent connections is a well known problem (cf. C10k or C10M Problem).\n10 years ago, one of the big technology decisions used to be “which webserver”. Apache2 was the most widely deployed webserver for C/C++ or PHP while the Java folks swore by TomCat and later Jetty, etc. That was until Go (yeah, golang) arrived and suddenly the application was its own webserver. What’s more, you could use that webserver to do raw TCP I/O using websockets, which were supported out-of-the-box in Go before any mainstream webservers picked them up. Anyway, I digress.\nOne of the coolest gifts that Go has given to software development community is the select statement. Combined with goroutines and the rapid benchmarking capabilities, chat servers have become trivial (I don’t use the t-word lightly) to build with Go. Fortunately, Tokopedia is a Go shop and double-fortunately, we also had engineers who could wield this capability into a reliable working solution.\nOperational Complexity: Given that we decided to build our own chat server from scratch, we had full control on its behaviour and failure patterns. We could dig right down into the TCP implementation of Go if needed, leaving very little behaviour that we couldn’t understand or control. We also had a scale target to design for and we frequently ran stress tests to push the implementation. This allowed us to develop implementation and operations hand-in-hand for a low maintenance solution that could be operated with minimal overhead.\nUnique Business Value: Chat is one of the most crucial touch-points that a business has with their customers. Having the ability to connect the user with a business representative via chat messaging is a very basic functionality but we saw that as an opportunity to build a lot of differentiation.\nBeing able to build workflows over chat, being able to integrate our own business artefacts into the chat protocol and being able to control the experience end-to-end has a very high business value. We’ve been continuously enhancing our protocol to support all kinds of business use-cases. Having full control over the solution, rather than building to a 3rd party API, means that we can do the integrations at whatever level is most optimal, rather than only doing surface-level integrations.\nCost: We worked through pretty-much all the levels of cost evaluation. While I can’t share a lot of data on this, we did find that our Level 1 and Level 2 costs were 10x lower at full scale for one of the use cases.\nCase Study 2: Log Platform – Buy Of late, one of the other considerations that we had to make at Tokopedia was on how to provide a scalable log aggregation platform that could be used across the whole company, not just for application specific logs but also being able to do cross-application analysis at high speed of interactivity. Here’s how the decision framework applied to it.\nFunctional Complexity: On the surface, log aggregation is a well understood functionality that has limited scope. However, our prior experience and analysis of 3rd party offerings revealed important peripheral functionality such as the ability to handle instance churn in a containerised environment, highly responsive interaction, analytics and alerting on log data, etc.\nTechnical Complexity: Technically, log aggregation can be simple or complex depending on the scope of functionality. I mentioned earlier that interactive information retrieval at scale is a complex problem. We could build log aggregators that were interactive at low volume and supported basic filters but basic log aggregation wasn’t sufficient functionality and building full scale log analytics is a technically complex problem.\nOperational Complexity: Given the high technical complexity, our only options were to assemble and operate a F/OSS solution as a black box. That didn’t go very well for us, mainly because understanding a log platform is extremely costly – whether as a white-box (reading the code) or empirically (understanding failures through stress testing). We didn’t want log platform maintenance to be someone’s full-time job and it wasn’t easy to keep ingestion and query times low enough to be acceptable as “real-time, interactive” performance while ingesting TBs of log data daily – and be able to scale it 10x for the discount sale spikes.\nUnique Business Value: Log aggregation and analysis is one of those components that are business-critical but not differentiators. It is extremely important to have a reliable log platform because it’s the foundation on which we build our failure-feedback during development, testing and – most importantly – production operation. However, we did not find any use cases specific to us that couldn’t be satisfied through off-the-shelf solutions. Thus the business differentiation was very low.\nCosts: Again, while it’s difficult to share hard numbers on this, our analysis revealed that our Level 2 through Level 5 costs would have been higher for an in-house solution. High enough to overcome any savings in Level 1 costs, and the gap increased with increase in scale. So we decided to go with a 3rd party solution instead.\nWhen to Sell What You Build Here’s a bonus! While most of us stop after making a buy vs. build decision, the same framework used to make that decision can help in deciding whether you should sell what you’ve built. We just need a couple of extra points to consider.\nExternally Relevant Differentiation When you decide to build something that has high complexity, most likely it’s because it has high business differentiation value for you or you have done it in a way that alters the economics of third party solutions. This means that you’ve build something that is not only hard to build but is also different from existing solutions in some meaningful ways to you.\nGiven that your business is not operating in isolation, there may be others who might find that same kind of differentiation meaningful. So you could potentially sell your solution to other parties either commercially or as a F/OSS offering. However, there’s one more thing to validate before you make that decision.\nCost of Multi-Tenancy When you build a solution in-house, you do it for your company, for your use cases. Selling this solution externally means that you have to do a lot of additional work to separate out your company as a tenant of your solution. That’s followed by ensuring that your solution can work for multiple tenants in a way that they don’t step on each others’ toes or discover each others’ business secrets. Add to that the requirements for cost computation and usage audits and more robust end-user security.\nSometimes the cost of multi-tenancy itself becomes too high. Maybe the solution that you built turned out to be profitable precisely because it wasn’t multi-tenant. One of the reasons why enterprises buy enterprise solutions is because any platform in a large company inherently needs to support multi-tenancy. These companies don’t just have different teams, they have teams that belong to different business units that need to calculate their internal charge-backs for shared technology and don’t take loss of reliability due to a sister unit’s misdemeanour very lightly.\nConclusion I do find that different markets have different appetites to buy or build. There’s a lot of “emotional” decision-making that goes on. Functional vs. Technical complexity is often not differentiated and the wrong trade-offs between short term and long term prospects are made.\nI have documented here my decision framework for making buy-vs-build decisions and I hope it helps you make better decisions. Conversely, you can help me by adding to the framework.\n","permalink":"https://tahirhashmi.com/posts/2020/04/18/buy-or-build/","tags":null,"title":"Buy or Build?"},{"categories":["Technology"],"contents":"The technology universe is in a constant state of flux with new advancements arriving faster than one could keep up. A technology leader, in this scenario, needs to look for something durable to build the foundation of their new (or improved) technology organisation.\nSpending 36 months leading the charge (and occasionally failing) at a fast growing business that’s powered by technology can teach a lot. Coming off the back of a career built with customer-facing development teams at companies serving over 100 million customers, I now have some idea of what it takes to make a strong technology foundation for a modern business.\nAfter a reflective 3-week slate-wiping vacation, I’m good to write down and share my thoughts.\n0. The Technology Landscape In order to understand the rationale behind the foundational traits, it’s worth spending some time to understand what the overall landscape of the technology domain looks like. This would allow us to identify the essential traits of a technology organisation that would help it succeed.\nReaching a wide audience is easy – and expensive. Internet Marketing tools and social media provide an incredibly easy, albeit expensive, way to get people directly to your product. This is different from OOH, TV, Radio and Print marketing that could only take your brand to the audience but not your product. In order to leverage your spend on Internet Marketing, you need to ensure that:\nYou are ready to accept any amount of traffic that marketing directs to you. A website or app that crashes due to traffic is a waste of money and opportunity You provide a smooth glitch-free experience to every user every time There is a large supply of developers yet there’s a talent crunch. This seemingly paradoxical situation has an easy explanation: the hiring requirements are too specific. We don’t just look for smart, committed problem solvers, we hire people who know programming language X and database Y and message queue infrastructure Z.1 Even if the job description doesn’t include all of this, I’ve seen enough people get dropped in interviews because “they don’t have enough experience with our tech stack”.\nCompetition is mostly copy-paste. There are very few domains where it’s possible to bring an innovation to the market and not have it replicated sooner than it can become your forte. Rapidly improving execution, along with consistent innovation, is the key to sustained leadership in the market.\nDigital Transformation. Half a century ago, only large corporations could think of “computerising” their office operations like finance, payroll and leave management because computers were hard to procure, maintain and program. Now, almost every pop \u0026amp; mom store has spreadsheets to do these jobs, with no programmers and IT support staff. Similarly, offline businesses are now looking to set up transactional access through online channels. They can’t afford to be left behind and they can’t afford to hire and maintain large software development teams either.\nBased on the above scenario, I can identify the following three essential traits that a technology organisation should exhibit:\nSpeed. To thrive in today’s business environment, a business needs to leverage economies of speed, irrespective of whether they have economies of scale. Traditional software development teams roll out new features in a 6-month time frame. The modern software development organisation needs to be able to roll out entire business models in that time. And I’m including the conception and ideation time in this window too! This is what enables copy-paste competition and what can thwart copy-paste competition as well.\nReliability. In his 1975 book, Mythical Man Month, author Fred Brooks Jr. argued that it’s important to build a throw-away prototype before building a production ready version. It is telling of the advancements in software development practice as well as added environmental pressure to deliver, that 20 years later, the author acknowledged throw-away prototyping as a mistake borne out of the waterfall model.\nToday, we need the prototype to be production ready because no closed-room estimation of a feature’s effectiveness could be better than testing in the market and having a tight loop from feedback to incremental change and release to market. If prototyping and production deployment can’t be done by the same team with the same tech stack and the same skill requirements, the overheads pile up. It’s crucially important to have prototypes be robust and reliable enough for production from the beginning.\nEffectiveness. The agricultural revolution ca. 10,000 BC allowed humans to settle down in communities and the key to scaling up those settlements from hamlets to villages to towns and cities was to grow even more crops. The key to scaling up in the industrial revolution was to build bigger machines that processed more raw material and finished more products per time.\nThe key to scaling in IT revolution is to do more with computers. The more we focus on transferring human knowledge to computers the more the humans can achieve – and there’s a lot to achieve. After all, there have been five agricultural revolutions since 10,000 BC, the most recent being half a century ago! We need to be able to deliver more business value through smaller teams with lower average experience and skill level.\nNow that we have established the three traits of a competitive technology organisation, let’s look at the top 5 competitive advantages that enable these traits in an organisation. Spoiler: there’s no AI/ML, Blockchain or IoT involved.\n1. The Public Cloud Not being on a public cloud platform today is a disadvantage. Back in 200x, being able to provision hardware with the click of a button was a novelty. Now it’s an essence. But that’s not even the point here.\nPublic Cloud is more about commoditisation of development and production environments. Look carefully at the product offerings of mainstream cloud providers and you’ll notice how you can choose from a variety of capabilities as well as bundled solutions. It’s like going to a computer store and either picking and choosing parts (and relying on yourself for ensuring cost effectiveness, interoperability and end-to-end optimisation) or just picking up a ready-to-use laptop/tablet… or smartphone that matches your budget and requirements.\nThe Cost Angle One common perception to address is that of cost effectiveness of managing your own (usually virtualised) physical infrastructure. I’ve come across anecdotal figures of 3-5x difference in cost. However, this only includes the cost of primary compute resources (CPU, memory, storage, network). The elephant in the room that is usually not addressed is the cost of speed and cost of reliability. Most modern businesses have so much business to lose due to lack of reliability, that they would readily buy it and recover more as revenue for every dollar spent as “reliability opex”.\nLeasing a data centre, running CloudStack or Kubernetes, etc. on it is easy. Making everything work together to form an integrated “production runtime environment” is difficult. Managing the environment 24×7 is hard. Making it developer friendly is an incredible design challenge. Managing it while ensuring end-to-end optimisation and reliability is a whole different ballgame. Think redundancy, failover, backups, audits, security and access controls, incident management, 24×7 monitoring, performance bottlenecks….\nNow consider that a production environment consisting many components typically exhibits the “weakest link” phenomenon – it doesn’t matter much how robust a few of your components are or how many components are robust. A single weak link can spoil the whole party. Given the business pressure to deliver more, faster, with understaffed teams, the occurrence of weak links is unsurprisingly frequent.\nThere’s no shortage of companies that offer to manage these components better for money – I’m talking about stuff like enterprise backup solutions, enterprise monitoring and analytics systems, and so on. They are great at solving a specific problem, but while buying these solutions a-la-carte can improve the components they apply to, ensuring global optimisation is still your responsibility.\nThe upside of using public cloud is that you get a more well integrated, globally optimised production environment. You get mature infrastructure operations, security and disaster recovery capabilities without having to acquire talent for them first. Without having to hire supervisors for that talent. Without the risk of failing time consuming certification processes when you’re audited for raising funds or going public.\nThe Capability Angle I’ve had the good fortune and opportunity to develop my career in a highly competitive environment. So much so that while I’ve usually been recognised as having strong technology skills, I still believe it’s my weak point – after all, I’ve never worked at Google or Microsoft! Anyway, the teams that I worked at throughout my career had a strong “we can do it” attitude and for us to buy a solution was demeaning. This environment worked great for our growth as technologists at an individual level but for the businesses? In some cases, not so much.\nThe reason is simple. It’s not a question of capability alone. It’s also about focus. If you lead a team of smart, strong developers, would you rather have them focus on building the competitive business capabilities faster or would you rather have them rebuild what has been built elsewhere many times over? (The justification for re-invention often is “we can customise it for our business” and “we can do it better/cheaper”. My empirical observation is that those assertions are usually valid but also usually not worth the loss of focus. Lack of focus is immeasurably costly).\nA business has some core competencies that differentiate it from competitors and it has a lot of auxiliary supporting functionality that is required to make a complete product for end customers. The core competency requires a lot more focus than the auxiliary functionality, but both sets require equal level of maturity. With public clouds, you no longer have to build your business critical functionality with the same level of knowledge commitment as an auxiliary function. For example, you might very well use carefully optimised self managed database instances and custom tuned VMs for your core, high volume, mature transaction processing systems while using something like AppEngine or Lambda for low complexity applications. You can test new features in the market with something that gets you started quicker and move to something that’s more cost effective if and when your scale and talent pool justify the move.\nTalking of talent pool, most graduating developers now acquire hands-on knowledge through public clouds. The more of your stack that runs on standard public cloud offerings, the more ready-made usually free training material you can leverage as part of your developer on-boarding process.\nAny product offering that’s already part of a public cloud is now commodity. It’s not adding value to your business unless it actually is the business you’re building. Trying to duplicate that functionality as part of your technology capabilities is a distraction from your core business and it’s going to have diminishing value over time.\n2. Elasticity Elasticity is simple to understand: it’s the ability to add or remove resources on-the-fly with minimal effort and without the risk of unexpected change in functionality. The resources could be anything: more CPU, more memory, more storage, more bandwidth… etc. This resource elasticity must also translate into cost elasticity. In my rulebook for 2019 and ahead, if any technology component is not elastic, it’s not viable.\nElasticity and Software When I try to apply the elasticity mandate to every technology component, I realise how much more there is to be done, how it can bring about a paradigm shift in design of applications. Being elastic requires the ability to chunk all resources into small units and quickly add or remove those units without disrupting functionality. Most of us are pretty comfortable with being able to add more servers behind a load-balancer for our stateless application layer or sharding databases. That’s how we scale. So what’s the deal with Elasticity?\nElasticity is about doing scalability in style. If adding more instances to the application requires filing a change request that someone spends a day to get to and an hour to provision instances, set them up and update DNS records, you can claim scalability but not elasticity. Elasticity is all about how those instances are added – how quick is the decision-making, how quick is the turnaround, how failure-resistant is the operation, how consistent is the application functionality in the event of scaling and how hands-off is the whole process.\nThis has interesting side-effects such as the need for application software to treat instance failure/shutdown as a normal operation rather than exceptional event. It also shifts the need to recover from instance failure to instance replacement. This is a subtle difference on the surface but has great consequences. Why? Because recovering a failing instance requires knowing the cause of failure and knowing how to remediate based on that cause. This troubleshoot-and-fix method of failure recovery is complicated and time consuming (high MTTR). With elastic applications, failure can be handled by simply replacing a failing instance with a new one, while optionally quarantining the failed instance for forensics. The faster the replacement, the lower the MTTR. (There are related strategies for handling total failures, but that’s outside the scope of this discussion).\nHaving an elastic application layer just scratches the surface. As I seek to apply the principle of elasticity to more and more technology components, I realise how much there is to desire in the area of elastic data persistence and networking. This brings us nicely to the next part of this discussion.\nElasticity and Vendor Selection Building a technology organisation for speed, reliability and effectiveness requires keeping the NIH Syndrome at bay and that means leveraging third party vendors for the functionality. My definition of “vendor” in this article is broad enough to include non-commercial Open Source Software as well.\nVendor selection is tough. Mostly, every provider in a domain covers the same feature set as others. In today’s world of copy-paste competition, this is hardly surprising. To make matters worse, the SaaS model makes it very easy to “hide” production deficiencies under the hood.\nSo far, I’ve found that bringing elasticity as a deal-breaker tends to differentiate mature and immature vendors fairly well. It is also a criterion to judge how well the vendor can fit with the desire to leverage public clouds, which was the competitive advantage I previously talked about. For commercial vendors, it also simplifies cost considerations as it converts most of the cost into “opex” that can scale with usage.\nAs I said earlier, elasticity is about scaling in style. A competitive technology team must not only target scalability, but also bring more functionality into the realm of being elastic.\n3. Lean Development Environment One of the biggest differentiators between an effective technology team and an ineffective one is not the average technology skill level of its members, but the maturity of the team’s software development methodology. In other words, a team of less skilled developers with better methodology would outperform a team of more skilled developers with poor methodology. Wouldn’t it be wonderful if you could succeed with competent talent instead of battling for the elusive “rockstars” in the talent market? Of course it would! But here’s another kicker: top talent actually sticks around in companies with great culture, and that includes the way the daily grind of software development works.\nWhile there is no specific methodology that I recommend, I can list down a few properties that good methodology would exhibit. These properties directly contribute to the Build-Measure-Learn loop that is one of the principles outlined in the book, The Lean Startup. A Lean Development Environment is one that enables the methodology to seamlessly embed into the process of creating and maintaining software, rather than feeling like an add-on or overhead.\nFollowing are the properties that a Lean Development Environment should embody\nEffective Change Annotation The first version of any useful system starts with “clean code”. The team that writes it feels a sense of pride at how pristine and up-to-date the design is. But if it’s a useful piece of software, it will be used, small bugs will be discovered, minor edge cases will be handled. It will scale in usage beyond what it was envisioned to do. Within six months, the code’s authors won’t be able to recognise what they wrote and why.\nAnd then it happens. An unusual bug surfaces. The troubleshooting leads to a particular line of code that seems too absurd to have been written. Was there a reason why this code was written this way? Changing it to fix the bug causes another test to break. Is that test still valid or does it need to be updated too? That’s when you wish you knew what was going on in the mind of the developer who wrote that code.\nThis wish is granted by effective change annotation. Any change in code should be annotated with information that answers the “why, when, by whom” questions. There should be a traceable reverse path from any line of code to the commit that applied it and its author. The commit message ideally explains the change, but for good measure, it also links to a task in a task tracker, which has more context around the change. The task links to a technical design document and a specification document (or PRD). This is the most effective method of connecting the documentation and code that I have ever practised.\nThe team that did this best among ones that I worked with was at Yahoo! ca. 2005. We were tasked to build a multi-lingual, multi-format Reviews \u0026amp; Ratings platform that would cater to all of Yahoo’s properties throughout the world serving half a billion users. The team took a cue from Philip Tellis, who refused to do any work if there wasn’t a Bugzilla entry for it. Through strict discipline and a clever combination of CVS and Bugzilla, we managed to ensure that no code was committed for production deployment if it didn’t have a corresponding Bugzilla ticket. The dopamine kick came from writing “Fix Bug #XXXXX” in the commit message to automatically mark the bug as fixed.\nThis practice was immensely helpful in incorporating customer-specific requirements since they had to be converted into platform-wide capabilities that could be used by any other customer. More often than not, this required being able to refer back to an ancient design choice, understand how changing it would impact the system and working out the peripheral changes required to bring in the new capability. Sounds a lot like what happens in a typical Build-Measure-Learn cycle, right? Except the code is no longer an opaque wall of incomprehension.\nTelemetry Infrastructure When we write code for a new application, we focus on getting the functionality right first. We don’t really think much about the diagnostics and telemetry we should be getting from the application. However, it’s impossibly tough to manage an application in production if it’s not sending out any signals about its functioning.\nSignals related to health checks, latency and error rates are so fundamental to production operation of networked services that they should not even need to be coded up manually. Anything that requires manual effort has a chance to be skipped inadvertently or by ignorance. This is the reason why I’m a huge fan of metrics-enabled infrastructure components, such as envoy proxy. Just last week one of my teams in Tokopedia debugged an OOM incident using our in-house continuous profiler for Go. The continuous profiler had been quietly recording profiles every 5 minutes. The developers simply located the spike on a graph and analysed it. The cost to developers for getting this capability? One never-changing line of code in application initialisation.\nOperational metrics is one aspect of telemetry. However the Build-Measure-Learn cycle requires more than measuring network parameters and success rates. The need to perform deep data analysis makes it important to integrate event generation and consumption as a first class capability in the stack from development environment all the way to production.\nOne of the companies where this was most culturally well integrated was, unsurprisingly, Zynga. Even as far back as 2010, they could overlay change events on computational and business performance metrics to easily narrow down what change created what impact. Some of the high-brand-recognition startups of today still require ad-hoc work from Data Analysts to figure out this kind of information.\nThe way Zynga achieved this was through a clever system to capture events in a hierarchical model that was fixed throughout the company, yet flexible enough to allow each event to provide its own meaning to the hierarchy. Crucially, everyone from the Product Managers to the Software Engineers could speak and understand this hierarchy. The events to be captured were specified as part of the product specifications. This was combined with a well integrated multi-variate testing (experimentation) framework to allow deep analysis of player behaviour, spending habits, game progression blockers, and even system issues such as bugs or exploits.\nContrast this with how much effort and money teams spend on building or renting log aggregation and analysis services. If I had a penny for every thousand log messages that getting pushed into these aggregators… wait, there are companies that get these pennies and they are getting quite rich!\nFrom Laptop to Production: Build/Deploy Pipeline The Build-Measure-Learn loop wraps from Learn back on to Build. The tighter the Learn-Build link is, the faster the team can move forward with enhancements. Once a change is done and tested in an individual’s development environment, every other step until production release is an overhead for the developer. The faster and more reliable this journey is, the speedier, more confident the developers are with making changes.\nOn the process side, the intra-team and cross-team collaboration models have seen a lot of changes over the last few years. There was a period of confusion as git – an SCM created to handle a very large codebase with contributors from all over the world – caught the fancy of small teams of developers (thanks mostly to github). It took a while for developers to experiment with fancy development models until they realised that developing closed software within a team or company requires different methods than developing software in the open, across the world.\nThe CI/CD pipeline is a great place to infuse a lot of automation. The build environment, for example, can be sandboxed to only pull dependencies from a local repository, wherein the dependencies have gone through an automated or manual security review. The build environment can have “tricked-out” versions of language SDKs and standard libraries to inject additional security, auditing and diagnostics tooling, that the development environment may not require.\nCurrent advancements in infrastructure technology such as Kubernetes, Infrastructure-as-Code, GitOps, etc. have made this journey super high speed, especially for Cloud Native development environments. Teams that are not already in a position to leverage these advancements are going to lose out on velocity and reliability under change. The good news is, public cloud providers and independent SaaS vendors are already adding a lot of value into this ecosystem so the barrier to setting up a great pipeline is not so high. The caveat is that it does make the team dependent on one cloud provider or SaaS.\nA Lean Development Environment is hard to build without embodying its principles from the start and having a relentless focus on automation. Adding emphasis on automation to the culture triggers a self-reinforcing cycle that leads to improved maturity across all the above properties. A team that scales through automation grows to add new capabilities or specialisation in the team more often than adding headcount that have the same skills.\nPerhaps the most advanced Lean Development Environment in the world is being run at scale at Facebook. That gives them a solid competitive technology advantage against everyone else trying to build yet another social network.\n4. Test Driven Development Most technology organisations spend too much effort on software testing, but they don’t test the software enough. This may seem paradoxical but it is actually a result of unsuitable methodology borrowed from manufacturing industry. Industrial testing involves verification that each of the thousands or millions of units of mass produced items is identical and meets the same set of quality expectations. On the other hand, software development is about change, not about producing identical copies of the same bits of data.\nPlease hold that thought while I briefly recount some of my personal experience with software testing. I spent the first three years of my career building web applications and rewriting a payroll processing system (from PL/SQL to MSSQL – no “coding”). I used to build the UI and its back-end, take the resulting UI for a spin using dummy data and aasdffg input. The new payroll system ran as a batch in parallel with the existing system and we just eyeballed the output to find differences to be fixed.\nIn my next job at Yahoo!, I had some big responsibilities to fulfil and the inadequacy of my testing practices was laid bare by my manager when she told me quite simply, “you write buggy code”. Luckily, I happened to be reading Kent Beck’s book, Extreme Programming Explained, which talked about “Test First Programming” and other things. It sounded extreme enough to make a difference so I tried it. Soon enough test first programming took me to a state where I could make large changes with speed and confidence.\nChanging Software with Speed and Confidence “Test Driven Development” is all about making change with speed and confidence. Unfortunately, outside of software development, testing has very little to do with change. We get our blood tested when we feel sick. We sit on tests (exams) when we need to prove our possession of skill or knowledge. Chemical labs do tests to ascertain the purity of their products. Factories do tests to determine that a unit manufactured today is identical to one produced last month.\nThat makes it a lot more difficult for software developers to get motivated to follow TDD practices, because on the surface it just looks like a hurdle or overhead in the process of creating change through programming.\nDespite the great success from my Yahoo! project, I didn’t always work in teams following TDD. Not until I discovered Go programming language and its built-in unit testing capabilities. That was back in 2012 and since then, I have never written a non-trivial program without following TDD. Only after several years of regular TDD practice did I come to understand the full scope of its benefits. So, last year I discussed go test in a tech-talk and wrote about it in a long blog post titled, The Best Feature of Go.\nThe Programmer’s Assistant In traditional waterfall process, testing comes after the implementation is finished and just before shipping the software. On the other hand, TDD practices encompass the full gamut of software development activities, acting as a “programmer’s assistant” at every stage, helping them make progress with speed and confidence. This is illustrated and exemplified in the blog post that I mentioned previously. To quickly recap, here are the points of assistance:\nTests help the developer work out the program design by starting with a skeleton and validating the intended implementation as it happens. Difficult testing is a sign of insufficient modularity or lack of functional cohesion\nTests help the developer switch to the “user” mindset by actually trying to use the software’s public interface\nTests help as shared API contracts – breaking API tests indicate impending integration failure\nTests help as examples and documentation (especially prevalent in F/OSS now) to speed up integration and adoption\nTests help in faster refactoring and identifying breaking changes\nTests help in assessing impact of change on performance\nTests help in finding safety and security issues before deployment in production environment\nTests help in confident releases to production through CI/CD automation\nTests help in automated operations and validating failure recovery mechanisms\nTests help in validating the non-functional properties of software (scalability, reliability, efficiency, responsiveness)\nWhen Not to Test One of the biggest psychological detractors to TDD or Test First Programming is reading too literally into the title. Test First Programming doesn’t always mean that you have to start with a unit test for each and every task. During the conceptualisation and design phase, often a developer would be testing a few premises – verifying that the available technology is viable for a particular way of solving the problem.\nThe test for viability or proof of concept does not necessarily require unit testing, although it really is a test. For example you might want to try out mutual exclusion among distributed processes through a remote lock. Trying out the remote locking mechanism to synchronise a couple of dummy processes spun up by hand doesn’t need a unit test. The moment you’re convinced that it’s a viable plan and decide to use it in a project is the point where you should start writing a unit test. In fact, I’ve often found it helpful to write rigorous unit tests to even convince myself of the viability of a solution. Occasionally, I found that the preliminary evaluation gave a false positive on viability. For example, a benchmarking unit test for our remote locking example is highly recommended to understand its scalability limits.\nThis inflexion point between proving something as viable versus applying the solution to the project being worked upon is something that often gets missed. Initiation of a project’s implementation unassisted by tests is likely to cause some corner-cutting in modularity in the haste to see something working. That corner cutting makes retrofitting of unit tests more burdensome causing further lack of motivation to test.\nThe bottom-line, then, is that TDD is not required while exploring the solution space but tests should be put in place as soon as the implementation start taking shape.\nDevelopers and the Test Organisation One of the more controversial decisions for a technology organisation these days is whether or not to have a testing team or QA department. There are many ways to address this but the most polished organisations tend to hold two principles as invariant:\nPeople involved in software production must be ultimately responsible for quality of implementation\nThere must be a human evaluation for all human-computer interactions (GUI, UX, etc.)\nThe best engineering organisations hold both these invariants while striving to improve execution on both. Facebook’s engineers, for example, are required to have a high level of ownership of code quality through unit testing, code reviews etc. At the same time, everyone at Facebook runs the pre-production version of the app, so they act has human evaluators of the app’s HCI too.\nThe question, then, is no longer about having a QA department since quality assurance is a shared responsibility. What does make sense is to have a group of people focused on improving the infrastructure, tools and execution of automated and human testing activities.\nHaving a software development culture rooted in rigorous automated and human testing is no longer a hallmark of technology giants. It’s a fundamental pre-requisite to having a competitive technology organisation.\n5. Abstraction of Evolution Software development has gone through several large paradigm shifts over the last 50 years or so. However, the essence of programming – data input, logical processing and data output – has largely remained. This is illustrated by the fact that even today, there are some systems running COBOL from ’60s on dated hardware architecture.\nHow is it that businesses get locked into a period of technology? How can a business make itself adaptable to technology change?\nThe Roles that Matter A typical enterprise technology group consists of several functional divisions. There are infrastructure engineers, network engineers, system administrators, emergency responders, security engineers, database administrators, testers, release managers and developers.\nTo really understand which of these technology roles are truly, uniquely part of the business capability, we could look at what headcounts are part of “cost centres” and what are part of “revenue centres”. Another dimension to consider is, which of these can be contracted out to a third party without revealing business secrets.\nThe answer to the above would be different for different businesses. I reckon, though, that for most technology driven businesses the real value is in the business logic code, and the most important technology people are the developers.\nThe Innovation Triumvirate While we agreed (or agreed to disagree) that the most important technology role is that of a developer – the person who writes business logic and makes it work in a production environment – there are two other very important roles that fuel innovation: the product owner and the data analyst.\nThese roles map to the “Build-Measure-Learn” loop of innovation in Lean businesses. It’s not necessary that there be three different people playing these three roles. Very often, there is a pair of people – the business person and the technology person – who split the roles among themselves. In fact, the “data analyst” role is something that the other two should be able to play in a hands-on manner to some extent.\nThe Technology Churn Treadmill Technology churn is a very real phenomenon in the software industry. Most of the technology innovation in the last two decades has been driven through open source software, most of which advances in increments and usually has the problem of choosing among too many alternatives that are minimally supported and have short shelf life.\nThe alternative is to build on top of “mature” technology that has been around for 10+ years – not a bad choice, except that it might leave your developers with FOMO (fear of missing out).\nThe impact of the tension between the need for stability vs. urge to innovate is that most developers are either permanently busy experimenting with the next shiny thing or planning to quit the job where they can’t be experimenting with the next shiny thing. And that’s bad because developers are a part of the Innovation Triumvirate.\nIf only there were a way to jump off this unhealthy treadmill.\nThe Program and the Platform The most important architectural decision that a business needs to make in order to assure longevity and agility of its technology practice is to separate the program and the platform.\nIn its most simple essence, the program is the implementation of the business logic, along with the required data, telemetry and mission control dashboards. The platform is the underlying infrastructure that the program runs on.\nOne of the most illustrious examples of this separation is the UNIX family of operating systems and the programs built on top of it. The first UNIX was written in 1970. Today, everything from the most powerful computing infrastructure in Google and other technology giants to the little smartwatch on your wrist runs some variant of this operating system. How did UNIX achieve such longevity and yet remained at the forefront of innovation unlike the stagnant COBOL systems?\nOne of the defining features of UNIX was the “system call interface” – a set of C APIs that captured the basic intent of what a developer wanted to do, along with offering a set of expectations from the underlying machine of what it did when the system call was invoked. Here’s what a system call looks like, pulled from the latest MacOS Catalina documentation:\nread(int fildes, void *buf, size_t nbyte); read() attempts to read nbyte bytes of data from the object referenced by the descriptor fildes into the buffer pointed to by buf. This is the system call that you would make to read some data from… anything! Note that filedes is just an integer that identifies some notion of a file (or collection of bytes) without actually saying anything about where the file is to be found or how it is stored on specific hardware. Storage devices have gone from magnetic tapes to floppy disks to magnetic disk drives and now NVMe solid state drives, but the caller of the read() system call doesn’t need to care. UNIX would make the machine fulfil its promise of performing the read. For reference, here’s the documentation from the 1973 AT\u0026amp;T SysV UNIX manual:\nA successful, innovative Platform is built upon a collection of programming intents that it fulfils in a well defined manner. The set of intents should be minimal and orthogonal. Orthogonality in software design is the ability to combine any set of features with meaningful, consistent results. The intents that the platform supports should completely abstract the infrastructure, yet be low level enough to be able to support a wide variety of business requirements through various combinations. A great example of such intents would be the RISC CPU architecture which is now universally deployed in smart devices to supercomputers and personal computers.2\nSeparating the Program and the Platform is a powerful and purpose-driven way to keep innovating from one point of stability to the next, without getting worn down by incessant experimentation or stagnating with fear of change.\nThe Way Forward To move forward, we look back at the five competitive advantages in technology that have been covered so far.\nA technology business that is built as a Program that runs on a separated Platform which enables Lean Development using TDD as a stabilising keystone and works on top of an Elastic infrastructure in the Public Cloud has very strong competitive technology advantages.\nIf that’s not what you have right now, you might want to plan for it soon because every business is being transformed into a technology business and it’s not fun to have it driven by a handful of technology giants that already have these advantages.\n","permalink":"https://tahirhashmi.com/posts/2019/01/19/5-competitive-advantages-in-technology/","tags":null,"title":"5 Competitive Advantages in Technology"},{"categories":["Technology","Programming"],"contents":"I’ve been programming since the late 90’s and I’ve done quite a bit of coding in C, C++, a lot of it in PHP and some in Python as well. On the front-end I’ve done some JavaScript and I’ve also had the misfortune of programming in Java 😉\nI started programming in Go in 2012 and since then I haven’t wanted to program in any other language. I’ve had a handful of large Go implementations across two companies and by now I have my own short list of favourite features.\nOne of those features is not mentioned very often but it has changed things significantly for me, and that’s what I’m going to discuss here.\nThe Good Features Without further ado, here’s the rundown to my top features of Go.\nCompiler \u0026amp; Syntax In terms of convenience of programming, compiled languages have been a bit difficult to handle. Especially languages like C++ or Java that are almost impossible to write in without some kind of IDE Support.\nGo on the other hand is very concise and simple. Things like type inference etc. make it very easy to code in – it almost feels like a scripting language. The error messages from the compiler are also very helpful, unlike, say, the C++ STL error messages.\nThe language syntax and conventions focus a lot on intent rather than expression. If you look at languages like Perl or Scala, you’ll come across many operators or syntactic sugar to express some logic, which is not the case with Go. While it’s very nice to write with those operators, it makes things very difficult to read later on. Go uses very straightforward imperative syntax – just clearly state what you intend to do with a few keywords and operators.\nAnother interesting aspect about Go is that if the programs compile, they usually work. This is one of the properties of well designed languages. I first observed it with Python, where if I wrote a program it usually worked correctly the first time more often than it did with C or C++ or PHP. This reduces the time to finish quite a lot.\nLastly, the performance and reliability you get from a Go program in the first hour or day of effort is phenomenal. Getting the same level of performance in, say, Java would take hours to optimise the code and figure out the features to use, etc. That makes Go a highly productive language.\nSmall Feature Set Go is only slightly bigger than C, and C is a very concise language. I remember when I was in college and I had to appear for a C programming exam. I read the Kernighan and Ritchie book cover to cover over the weekend and aced the test. That’s not something you can do if you want to learn C++ or Java or most other mainstream languages.\nBecause it’s small, Go is also very easy to learn and remember. All the features that are present in Go are orthogonal and minimal. Orthogonal means that you can combine any feature with any other feature in meaningful ways. So the number of things you can do with a small set of features is very large because the number of meaningful combinations of features is large. Minimal means that there are not too many different ways to do the same thing. This is unlike the Perl philosophy of TIMTOWTDI (there is more than one way to do it). In Go, as in Python, there’s only one good way to do something.\nThe net result of this is that there are very few “knowledge islands”, unlike in large languages. With large languages like C++ or Java, you will find many programmers that are only familiar or comfortable with a subset of the language. There are very few people who know the entire language end-to-end. “Knowledge islands” make it very difficult for people to exchange ideas with each other if there’s insufficient overlap in knowledge between them.\nStandardised Formatting This is one of the most famous features of Go and it improves accessibility of code by an exponential factor. That in turn improves collaboration. When you see your code vs. your team member’s code it looks exactly the same. When you see code written in your team vs. code written in another team, it still looks the same. This reduces the psychological “ours” vs. “theirs” cognitive barrier and one of the effects of this is in making open source code more accessible.\nSensible Unicode \u0026amp; Strings This is another feature I like a lot, though it doesn’t get talked about much. Go is one of the very few languages that get Unicode and strings right.\nOne of the things that I did back in 2003 was to implement a binary XML syntax – a more efficient XML serialisation format – and I enjoyed using C++ strings library a lot for that. I could read bytes directly off a file and use all the string manipulation features to deal with them. That’s something I found missing in Java.\nIn Java if you want to do any kind of string manipulation – say, parsing the first few bytes of a header or do substring matching , etc. – you would first have to convert the bytes into strings. And strings are Unicode code points in Java. One of the consequences of which is that it’s not just inconvenient to program with, it’s also inefficient. Whenever you need to transform the bytes into strings, you also necessarily have to do a memory copy because the underlying data types are incompatible.\nGo very cleverly uses UTF-8 representation of a string as the basis of its string type instead of Unicode code points. Perhaps it’s because Rob Pike is the inventor of UTF-8, but it’s a very good design decision anyway. Since UTF-8 ubiquitous, it makes writing network programs a lot more convenient compared to, say, Java – or Python 3 for that matter. I don’t know why Python 3 went the Java way.\nChannels \u0026amp; Goroutines Another headline feature of Go is, of course, Channels and Goroutines. It can be argued that Go did to concurrent programming what Java did to memory management – make the respective task significantly safer and simpler in an industrial strength programming environment.\nGo’s concurrency model is based on the formal theory of Hoare’s CSP (Communicating Sequential Processes) Model, and allows writing concurrent programs in a more declarative manner. Simply preceding a function call or method invocation with the go keyword causes it to execute in a concurrent context.\nChannels allow unidirectional or bidirectional data exchange between goroutines with built-in blocking primitives for synchronising between senders and receivers. Unlike NodeJS or async Java frameworks, coordination through channels enables concurrent logic without the proliferation of callbacks, which become very difficult to understand and reason about as the size of code grows.\nselect Statement The select statement is, I think, the party piece of Go’s concurrency features. It is a very simple, declarative way to combine multiple blocking events (channel reads or writes) and branch off some logic based on which of the events unblocks first.\nIt allows writing some of the most difficult concurrency patterns in an easy to understand and safe manner. Timeouts \u0026amp; cancellation, back-pressure, worker pools, etc. are easy to implement. Even more sophisticated synchronisation and scatter/gather logic is made possible with simplicity using select.\nIntermission With all of these wonderful features and some more that I’ve not even covered, Go permanently altered my programming capabilities and the kind of programs I wrote.\nThe first production system I wrote in Go was a real-time multi-player game that earned over a million dollars in a year and ran on just one server, the other one being a warm stand-by.\nOver the years I also developed/co-developed a user activity rate limiter, a reverse proxy, a micro-service simulator, a deployment orchestrator, an auto-scaler and even a bespoke datastore!\nThe best feature of Go, however, changed the way I program.\nThis feature that I’m going to talk about next acts as a stand-in for the user so you can code from the mindset of a user rather than a programmer. It improves documentation. It can be a very significant guide for improving the design and modularity of your program. It improves the speed and reliability of refactoring and debugging. It finds unnecessary or dysfunctional code, and it gives rapid feedback on performance characteristics of your program as it is being developed.\nCan you guess what this feature is?\ngo test Most people already know of go test as a built in unit testing framework that comes as part of a standard Go installation. Which, by itself, is a pretty significant improvement over many other languages where you need to make your own choice of a unit testing framework and then worry about making it work with the rest of the ecosystem like editors, build tools, reporting tools, etc.\nHowever, there’s more to go test than meets the eye. Let’s look at what all go test can do for the programmer.\nDesign Phase Assistance from go test The first point where go test can facilitate program development is by simulating the user of the program. Most of the time, when we start writing a program, we write a main function to “try out” the program. A better way to do this in Go is to create a test file. If you’re writing a package mypkg, the test file should declare itself as package mypkg\u0026lt;b\u0026gt;_test\u0026lt;/b\u0026gt;. This makes the test file an outsider to your package, so you need to import your package into the test file to access its functionality.\nThis little trick immediately allows you to switch roles between a developer and a user. It can guide your API design. It can help you decide what symbols to export and what symbols to keep private. By ditching main for a test package, you take the first step towards integrated testing. And, as they say, getting started is half the job done!\nAnother significant way that go test helps in program design is by forcing you to think about ease of testing. Monolithic, do-it-all functions are hard to test, so balancing your urge to implement with the need to test naturally leads to improved modularity and improved cohesion. As an example, when developing a micro-service, I typically use the following strategy:\nImplement all of the business logic as native APIs with native data structures — this is the package whose unit tests exhaustively cover business logic testing\nImplement data load/store from native data structures to database — this package is devoted only to data handling and the unit tests only cover data transformations\nImplement RPC (HTTP etc.) as a wrapper using the previous two packages — for this package the tests only deal with request interpretation and response serialisation, not business logic\nLastly, go test offers an excellent way to write example code that shows up in the go doc documentation at the appropriate places and is verified for correctness. If you want to document a function, just write func ExampleFunction() {…} in a test file and this code will show up next to the documentation of the function, Function. Similarly write func ExampleStruct_Method() {…} to document a method, Struct.Method(). If you want to document a use case, write func Example_useCase() {…} to document an entire use case. Adding a block comment starting with Output: at the end of the example’s implementation will make go test execute the example and match its output to the comment so you know whether your example works. go doc would document the output as part of the example.\nImplementation Assistance from go test If you write some straight-forward table driven tests for your package’s public API, those tests act as your compatibility guarantee while you’re refactoring things around. If they pass, your refactored package is still working as expected. You can organise your test functions into top-level tests that test out scenarios and have subtests within them to test more granular functionality.\nIf, after a refactoring change, your tests don’t compile, it’s a clear indication of changes in the package APIs. If the tests compile but fail, it indicates a change in functional behaviour of your package. The more granular the tests, the easier it is to locate the faulty change.\ngo test not only helps with ensuring functional correctness but it also helps in verifying non-functional aspects. If you write concurrent tests and run the tests under a race detector ( go test -race), you’ll be able to catch any data races that, if left undiscovered, can cause your program to crash while it’s running in production.\nIt is also possible to write some long running tests to verify deeper behaviours of your implementation. For example, my package smartcb has a few long simulation tests that don’t execute by default but get enabled when invoked as go test -tags sims.\nIt is even possible to use go test to discover code bloat. Usually, you would run go vet and among other things, it will find unreachable code for you, that you can simply delete. However, this doesn’t cover all situations. One excellent property of writing exhaustive unit tests is that if you get a test coverage report and it’s not 100%, you are either not testing your code thoroughly enough or you have some code that is not testable because it represents a logically impossible scenario. This is a very powerful way to discover unused functions or types that otherwise keep accumulating as a piece of code goes through multiple maintenance cycles.\nPerformance Optimisation from go test One of the most powerful features of go test is the benchmarking infrastructure. The ability to monitor the performance characteristics of code as it is being developed and assess the impact of code changes on performance is a goldmine.\nJust write a few benchmarks for top-level package APIs in your test files. Then, to find how fast the functions/methods are, run:\ngo test -bench To observe multi-core scalability, with various values of N, run:\ngo test -bench -cpu N To see what parts of your code are slow, run:\ngo test -bench -cpuprofile To see what parts of the code eat up memory, run:\ngo test -bench -memprofile Run go test -bench after every commit to find performance regressions if you are writing something performance sensitive.\nThink that’s something too hard to do? Think again. Here’s a video that shows just how long it takes to test a non-trivial program, benchmark it for performance and analyse the benchmarks for performance issues.\nClosing Thoughts Having experienced the power of go test and its influence on my programming practice, the following statement sums up my view about it.\nGo test is like a programmer’s assistant. An Ironman’s Jarvis. A Batman’s Alfred. It happens to run unit tests too.\n— Yours Truly\nThis post is based on my talk at Tokopedia Tech-a-Break. Following are the accompanying slides.\nThe Best Feature of Go – A 5 Year Retrospective from Tahir Hashmi\n","permalink":"https://tahirhashmi.com/posts/2018/06/01/best-feature-of-go/","tags":["Go","Golang","programming"],"title":"The Best Feature of Go"},{"categories":["Photography","Reviews"],"contents":"Aero India is a biennial air show that takes place on an airfield on the outskirts of Bangalore. As per Wikipedia, it’s the world’s largest air show after the one in Paris. I first came to know of it when a friend of mine armed with a Nikon D70s and a non-VR 70-300 f/4-5.6 came back with some fabulous shots way back in 2007. Ten years later, I got to visit the show with a Nikon D7100 and an AF-P DX 70-300mm f/4.5-6.3G VR.\nYou might already be aware of the fact that AF-P lenses don’t play nice with older cameras like the D7100. I had the choice of mixing and matching between D7100, D3300 bodies and AF-P 70-300 VR or AF-S 55-300 VR. Having tried out all four combinations, I settled for D7100 and AF-P 70-300 VR. Wondering why? Here’s why:\nD7100 vs D3300 First off, let’s talk about why I chose the “incompatible” D7100 body instead of the fully compatible D3300. It’s not about IQ. Both have comparable 24MP sensors and I shot 12-bit on D7100 (more on that later). It’s actually about AF.\nD3300 has a 9 point AF system whereas D7100 has 51 points. More than the number of AF points, though, it’s the possibility of AF-C with d9 (set point + 8 surrounding points) tracking AF that puts a big gap between the two bodies. I don’t have a 39 pt body like recent D5xxx or a D7000, but both of them offer d9 tracking mode, so I expect them to perform similarly. With a really long lens and high speed action, d9 mode helps in holding focus more reliably.\nI did not use d21 or d51 because one, the direction of motion of aircraft is highly predictable, thereby making it less difficult to keep them positioned at the active AF point. The other factor is that the D7xxx line has a really low resolution matrix meter (2016 pixels), which makes it difficult for the camera to know what the subject is. See slides 6 through 8 of this dpreview slideshow to know more.\nThe other thing missing in D3300 is the ability to enable back-button AF. I normally use the shutter button to trigger AF, but when I’m shooting with telephotos, there’s nothing better than the agility afforded by BBAF. I have saved a custom setting (U1/U2) on the D7100 for BIF (Bird In Flight) shooting that switches to BBAF, among other setting changes.\n55-300 VR vs AF-P 70-300 VR Despite it’s super slow AF speed, I have shot quite a few BIFs with the 55-300 VR.\nBlack Kite. D7100 with 55-300mm VR at 300mm\nThat VR bit is rather useless, though, since at the shutter speeds I use for BIFs, (1/1000s – 1/2000s) it’s more of a hindrance to smooth panning and tracking than a benefit. The inability to disengage VR on the AF-P 70-300 VR with the D7100 was a significant downside making me strongly consider going ahead with 55-300 VR.\nWhat did switch me to AF-P 70-300 VR, though, was its responsiveness in tracking. And it’s quite sharp at the long end compared to 55-300 VR. A couple of days before the airshow, I tested the new zoom’s tracking ability. It yielded some convincing shots, even with the D3300. But that was with VR turned off.\nBlack Kite. D3300 with AF-P DX 70-300 VR at 300mm\nThe Air Show I was quite certain of using the D7100 for the air show, but I still had doubts over the lenses, so I carried both. When I reached the venue, I tried a few settings with both the lenses, finally settling for the newer model.\nOne of the problems that plagued me was that I had to be really conservative with bursts, owing to the limited buffer of the D7100 despite using U1 95MB/s cards. That slowed me down quite a bit but not enough to actually eat up all 24 GB (16+8 GB) of memory I had available, in less than 45 minutes! I switched to a regular Class 10 card to continue capturing the SAAB Gripen that was roaring through the skies. Bursting? No way. Any good shots? Sure, like this one:\nSAAB Gripen. D7100 + AF-P 70-300 VR at 285mm\nIn the few minutes I got between Gripen’s landing and the next show by Suryakirans coming up, I switched from 14-bit raws to 12-bit, formatted both my fast cards and decided to start from scratch as I had another 3 hours of the air show left, with repeat performances from some of the aircraft that I missed.\nOnce I got settled with the settings and got into a shooting rhythm, it was smooth sailing from there on. I did wish for a few things to be better with the lens, though.\nLoss of Focus on Zooming Possibly the biggest frustration with this lens in the field was loss of focus on zooming. The lens was reasonably fast to acquire focus while the aircraft was at a distance or taking off. It also held focus admiringly well while the aircraft was on approach, even at high speeds like the F-16 Falcon that put out the fastest show with the widest stage of all. However, as the aircraft got close enough to start clipping the frame at 300mm, I tried to zoom out but lost focus in the process.\nDassault Rafale. D7100 with AF-P 70-300 VR clipping the frame at 300mm\nThis happened every time I tried. Though it re-acquired focus in a second or two, it seemed like eternity and it was pointless anyway, since I was trying to time the shot precisely as the aircraft was right in front. As a result, I had to shoot at a shorter focal length and crop later.\nF-16 Falcon. D7100 with AF-P 70-300 VR at 250mm, cropped to 500mm FX equiv.\nVR Interference The other problem I faced with the lens was with VR staying on all throughout. While panning in a straight path was less of a hindrance, though it surely was, the bigger issue was with smaller aircraft performing quick direction changes. The VR caused my adjustments to lag, leading to a jarring experience. I could also not get panning shots at slow shutter speeds to get some motion blur in aircraft propellers. airshow.se Viking. D7100 with 70-300 VR at 300mm. Frozen propellers.\nVignetting \u0026amp; CA The lens exhibits CA and also vignettes very badly at 300mm, even at f/8 – f/11 apertures that I was using. This is a big problem while shooting against a uniform background such as clear sky. As of this writing there’s no lens correction profile for this lens in DxO Optics Pro (my raw editor of choice) so I had to use manual vignette and CA correction, which too were not satisfactory enough.\nLong End Sharpness I was highly interested in shooting the aircraft at the moment of switching direction while they were the farthest away from the show area. Many such shots were encumbered with heat haze, but of the ones that weren’t I found that there was quite some loss of sharpness at 300mm, compared to the shots at approx. 250mm.\nDassault Rafale. D7100 with AF-P 70-300 VR at 277mm, cropped to 800mm FX equiv.\nUser Errors Along with the lens shortcomings, I should also mention that I spoiled some shots out of my own mistakes. The most stupid of these was jerking on the shutter button! Despite many years of shooting experience, I still found myself spoiling some long shots that needed precise timing, due to the nerves. If I had the ability to burst at will, I could have mitigated some of it. Times like these are when one would also wish for a _heavier _lens, which the AF-P 70-300 VR is not by a long shot.\nConclusion When out shooting fast action like an airshow, the D7100’s slow buffer and the AF-P DX 70-300mm VR’s niggles leave one wanting for better. However, for anything that allows a bit more breathing space between shots, the 70-300mm VR is a very satisfying lens and should make a very compelling sports combination with a compatible D5xxx body, given the price.\nAs for me, despite some gear anxiety and in-field fumbling with multiple issues, it was an experience that I enjoyed a lot. For the next time, though, I have committed to an upgrade to 128GB of SD cards, a D7200 and a 70-200mm f/4 VR.\nTo see more of the interesting aerobatics, please have a look through my Flickr Album.\n","permalink":"https://tahirhashmi.com/posts/2017/03/13/d7100-af-p-70-300-vr-airshow/","tags":["AF-P 70-300","D7100","Review"],"title":"D7100 and AF-P DX 70-300 VR on an Airshow"},{"categories":["Photography","Photography"],"contents":" Here’s a picture whose EXIF I have stripped so you won’t know what equipment was used to shoot it.\nIf you had to guess the sensor size, considering that this is a minimally edited photo, what would be your guess? FX? DX? m43? 1″? 1/1.7″? 1/2.3″?\nSmall sensor systems are all the rage these days. 24-1200mm zoom in a handy little package. How about that? The problem is, tiny sensors come with compromises. You have to bear with the compromises in every situation. How would it be, if you could deal with the compromises only when you needed to?\nA Prime as a Zoom Let me come to the point right away. I need a super telephoto solution. The solution needs to be light and hand-holdable. The options go all the way from a 1/2.3″ sensor P\u0026amp;S with humongous zoom range to non-portable monstrosities like the 200-500mm VR Nikkor. Based on size and weight considerations, I feel like I can’t do better than the 300mm f/4 PF VR Nikkor. Every other solution would be an IQ compromise, or be bigger and heavier than I want it to be.\nShould I go for this prime or, say, a Nikon 70-300mm CX (190-810mm eq.) or something in between (e.g. m43)? The answer lies in the realisation that all these options are working on a real focal length of 300mm. The equivalents are merely crops on the 300mm output.\nIf I have to crop, why should I not crop digitally? I can mount the 300mm f/4 PF VR on a 24MP FX body and digitally crop down to 900mm (eq). Imagine a hand-holdable 300-900mm f/4 lens! If I switch to a DX body instead, I could go further down to 1200mm. 450-1200mm f/4. With VR. Under 1000g. That’s going beyond the reach with a 1″ sensor. Arse kickin’!\nThe IQ Penalty The question that follows is, what’s the IQ cost of such a compromise? Well, I’m assuming that the digital crop factor can be stretched up to 3x. That yields a 2000×1333 px image with a 24MP sensor. Is that good?\nWell, it exceeds the “Full HD” screen resolution. At 300 dpi, it also exceeds the 4×6″ print resolution requirements. I’ve actually printed enlargements all the way down to 120 dpi with satisfactory (to me) results. That means I can blow a good 2048 px image up to 12×18″. Not too small, that.\nThe Proof All of the above theory sounds good, but where’s the proof that such heavy digital cropping will deliver? Take a look at the image at the top. Click through to the Flickr page for a full-size 2048 px view. Now guess the equivalent sensor size. You might be surprised to know that this digital crop is equivalent to shooting with a 1/1.7″ sensor (4.5x crop factor). I used a 20mm lens on a DX body (D3300) to shoot this and cropped it all the way in to a 90mm (FX) equivalent. Does the image look bad? It’s quite acceptable to me. Had this been a 300mm lens, the alternative to cropping would have been shooting with a 1350mm lens. That’s not happening with me. Very unlikely.\nHere are a few more examples of extreme cropping. AF-S Nikkor 20mm f/1.8G cropped to 60mm (1″/CX Sensor) AF-S DX 55-300mm VR Nikkor cropped to 810mm (1″/CX Sensor) – 1/13s hand-held AF-S DX 55-300mm VR Nikkor cropped to 810mm (1″/CX Sensor). Shot on a 12MP Nikon D90\nConclusion With the great amount of pixel density available in today’s large (DX/FX) sensors, there’s a lot that can be achieved with “digital zoom” or cropping in post-processing. When paired with a good lens and technique, you can make a large sensor realise the convenience of crop sensor bodies and the IQ penalty scales with the extent to which you are pushing the crop. You don’t always need 1200mm. You don’t always have to, therefore, live with the IQ of a 1″ sensor or smaller.\n","permalink":"https://tahirhashmi.com/posts/2016/08/21/factoring-the-crop/","tags":["Cropping","Sensor","Telephoto","Zoom"],"title":"Factoring the Crop"},{"categories":["Photography","Reviews"],"contents":"Early this year, I acquired a used 20mm ƒ/1.8G Nikkor lens. I only have DX bodies and the 20mm works more like a wide-normal prime than an ultra-wide, as on FX.\n20mm ƒ/1.8G Nikkor on a D7100. It’s big. Why use 20mm ƒ/1.8G? on DX? I had long wanted to have a small, fast 24mm prime for my DX bodies. While the 35mm ƒ/1.8 DX is compact enough, I don’t like it that much. The FoV (51mm FX equivalent) is too narrow for general photography and too wide for portraits. I do most of my portrait work with a 50mm ƒ/1.8G, so I need something to cover the wider end.\nThe 20mm is not a bad option. Its 30mm FX equivalent FoV actually matches that of some very popular smartphones, which is what I intended this lens for – having a smartphone camera equivalent with much superior image quality.\nHow Good is the 20mm ƒ/1.8G? Sharpness The lens is incredibly sharp. The 24MP DX sensor is the most demanding configuration for lenses across any system but there’s nothing that the 20mm ƒ/1.8G leaves on the table in terms of detail. The corners are not as sharp as the centre but that’s expected from a rectilinear wide-angle lens. Anyway, it easily out-resolves the wide zooms available for DX. While you can find MTF numbers etc. published by professional reviewers, I can tell you that at ƒ/8 in daylight, I need to reduce the default sharpness setting on my RAW converter for this lens. This is in the league of 85mm ƒ/1.8G performance, which itself is well known for its sharpness. Epic Sharpness Rendering This is my first lens with Nikon’s Nano Crystal coating and while I’ve not found any solid endorsements for the coating from pro photographers, I feel it gives a different “look” to the pictures. Its contrast and edge acuity feels a lot like that of the Zeiss T* equipped lenses, like in the Sony RX100. The biggest impact of this coating is felt with high ISO images where the extra edge definition and micro-contrast helps the noise-reduction algorithms retain the details better. High ISO details. ISO 3200, ƒ/1.8. Visible texture in the “JBL” logo. Close-Up Shooting The 20mm ƒ/1.8G focuses down to a minimum distance of 20 cm from the sensor, which roughly works out to 7 cm from the front of the lens. That is pretty close for most situations. This is not a macro/micro lens, though, since its wide FoV means that even at this close distance, its maximum magnification is only about a quarter of a true macro lens. Having said that, this is good enough for some funny people photography and taking pictures of food, gadgets, etc. This 2 inch tall tower looks great from the 20cm MFD, even with a 2.2x digital zoom. Shortcomings While the 20mm ƒ/1.8G is a cracker of a lens, it does have a couple of issues, both to do with its applicability. The first is its size and weight. With an 82mm diameter and 355g weight, this is one big prime – twice as big as the 35mm ƒ/1.8G DX. I would have loved for it to be about the size of a 50mm ƒ/1.8D – that noisy little lens. I wish for a DX DSLR equivalent of the Fuji X100, but there’s a bulk tax that Nikon’s DX shooters must pay for the lack of proper DX primes.\nThe second issue is with AF accuracy on the D7100. While the lens focuses on the right thing with the D3300’s 11-point AF system, the 53-point AF module sometimes doesn’t focus on the intended target. 3D tracking AF is too unpredictable to use. AF-S works most of the time but close focusing against a distracting background needs some care. I think it’s got more to do with the combination of a large aperture and wide focal length rather than operational defects in the equipment.\nConclusion The 20mm ƒ/1.8G Nikkor makes for a great general purpose lens on DX bodies, but its size and weight hamper the portability a bit. Nonetheless, its combination with the current Nikon 24MP DX sensors is great for low light social photography or taking pictures of food, etc.\nThe nice thing is, if you pair your DX body with an FX body, this very lens would give an ultra-wide FoV on FX. There is a narrow range of focal lengths (19-25mm, I reckon) where the DX crop factor changes the perspective significantly enough to allow the same lens to be used as two different lenses in an FX-DX pairing. The 20mm falls well within that range.\nJust like a smartphone. Only bigger and way better. ","permalink":"https://tahirhashmi.com/posts/2016/08/16/nikkor-20mm-on-dx/","tags":["20mm f/1.8G","DX","Lens","Nikon","Review"],"title":"AF-S Nikkor 20mm ƒ/1.8G on DX"},{"categories":["Photography","Reviews"],"contents":"A few days ago I got the opportunity to try out a Nikon D750, along with the new 20mm f/1.8G Nikkor lens. It wasn’t the best of times for trying out new toys for me, because of which I couldn’t get any presentable shots. I still got some shooting to do with it and I would go ahead to include those non-presentable shots anyway. Nikon D750 with 20mm f/1.8G Nikkor AF-S 20mm f/1.8G Nikkor This is the first lens I have tried, that sports the Nano Crystal coating on it. I had very high expectations of this lens’s colour rendition due to the N coat and I am glad to say, I wasn’t the least bit disappointed. The photos from this lens have a nice and crisp feel to them where many other lenses, in comparison, would look “washed out”. Here’s a picture of me just out of bed, shot with a Nikon D7100: D7100 \u0026#43; 20mm f/1.8G Nikkor at ƒ/1.8, ISO 1600 The picture looks decently crisp. The colour rendition is somewhat similar to what I get from the Carl-Zeiss optic in an RX100, but of course, there’s a lot more tonal depth to it – it’s got the 3D-Look™. White-balance is deliberately under-corrected.\nThe lens is also able to focus surprisingly close – you can almost take it right up to the subject!\nExtreme close-up with D750 \u0026#43; 20mm f/1.8G Nikkor at ƒ/4, ISO 1250 I have long been looking for a wide-angle prime lens for my DX body D7100, with 24mm being the desired focal length. Until now, Nikon only offered the super-expensive 24mm f/1.4G Nikkor and the dated and slow 24mm f/2.8D Nikkor. This lens, while being a bit wider, is significantly more affordable than the 24mm f/1.4G and significantly better than 24mm f/2.8D. Would I buy it? I’m not sure yet. It’s a bit heavier and bulkier than I imagined. Besides, given its performance on FX, it seems like a wasted opportunity to have it without a full-frame body. Which brings me to the D750.\nD750 Image Quality A lot has been written and said about D750’s image quality on the Internet and I am not going to add a lot more to it. I have previously done stupid things to be convinced that despite all the fanfare around small sensors these days, the full-frame advantage is undeniable. I am glad to report that my hands-on experience in this matter is as expected.\nSo what else is there to write about? Well, there’s Dynamic Range and ISO-less shooting.\nThe following, rather drab, dawn shot looks like a fairly easy to handle scene despite some impressive FoV coverage by the 20mm lens on D750 and nice sharpness and tonality.\nD750 with 20mm f/1.8G Nikkor at ƒ/4, ISO 100 In reality, though, this shot registered a -3EV under-exposure on the camera’s meter. All of the mid-tones and shadows you see here have been boosted in post-production with DxO Optics Pro 10. The result is that you can see the bluish hue of dawn light on the top-right corner of the orange wall progressively blend into the bright orange glow of the sodium-vapour lamp. Nearer still, you can make out the distinct greenish hue of the fluorescent tube illuminating the ground in front of the security booth while the rest of it is bluish from the dawn light.\nTake my word for it – this is a scene I am intimately familiar with and never before have I seen it captured with such nuance as in the above photograph. This is a true example of what you can achieve with the immense dynamic range of the D750’s sensor. To those who say they don’t care about dynamic range, I have one word: idiocy. Oh, and take a look at what the capture looked like prior to all the shadow boosting: The above shot, uncorrected for brightness. ISO-less shooting is the technique that exploits the capability demonstrated above. If you look at the Dynamic Range curve for sensors like that of the D750, you would observe that they start with stellar DR at base ISO and lose about 1 stop DR for every stop of ISO increase. In theory, then, if you shoot at base ISO and under-expose by a couple of stops, you would get the same DR as a normally exposed shot at a couple of ISO stops higher. The benefit of shooting at base ISO is that you don’t blow highlights, unlike when you boost ISO. Of course, you can only use this technique if you are shooting 14-bit lossless RAWs and are using a good RAW converter.\nI tried a shot with the D750 + 50mm f/1.8G at ISO 100, that I later boosted up by 4 EV. That shot turned out to be better than a normally exposed shot with D7100 + 20mm f/1.8G! On the left, below, is the uncorrected shot. On the right, the shot after +4 EV boost and in the middle, 100% crop of (slightly out-of-focus) hair detail.\nD750 Handling While I did really like the images coming out of the D750, I wasn’t that impressed with the body. One highly talked about feature of the body is its grip, which most reviewers have reported as being deeper and easier to grip. I didn’t really find it all that great. I still prefer the chunky grip on the D7100 that, along with back-button focusing, I can really squeeze on to and get greater shooting stability. As for carrying the camera during down-time, I think b-grip wins over a camera grip.\nAnother thing I didn’t like about the D750 was the view-finder – the AF confirmation blink, to be more specific. From the D3300 through D7100 to D750, the AF confirmation cues in the viewfinder get progressively more prominent. The D3xxx feels too subdued to me, while the D750’s was outright jarring in that it flashes the entire view-finder red, instead of just flashing the active focus point. I don’t want that… no way!\nComing, then, to the top LCD which has been slimmed down on the D750. Frankly, it’s pointless. It shows the shooting parameters alone, that I find more useful to look at through the view-finder. I don’t specifically remember what I was looking for in the LCD and missing, but it was frustrating to repeatedly not find it there.\nOne thing I did like about the D750 was the Live-View mode, which I switched into since neither the view-finder, nor the top LCD were to my liking. I must say, the LV mode on this camera is leaps and bounds better than on models like the D7100 or D3300. Aperture adjustment works. CDAF is fast enough to be usable, though not fast enough to match that of even relatively “slow” models like the RX100 or early NEX units. I like the fact that the LCD tilts, although I was surprised to find that it was more cumbersome to move than the one on my old NEX-5. It wasn’t as nice and bright as the one on D7100. Does it not have those white pixels?\nD7100 vs D750 The final question is, would I like to replace my D7100 with the D750? While the improvements in IQ and shadow boosting capability along with Live-View improvements make me go, “oh yeah”, the overall “feel” of the camera is decidedly less pleasant than that of the D7100. I hated the AF confirmation blink in the view-finder. The tilting LCD needs to move better and look better. While ISO-less shooting sounds exciting, it is easier said than done because the preview in the LCD becomes too dark to be usable. All said and done, the answer is, “I’m not sure”.\nNow, if only the Sony Alpha MILCs did lossless 14-bit RAW….\n","permalink":"https://tahirhashmi.com/posts/2015/03/24/nikon-d750-20mm-f1-8g-nikkor-tryout/","tags":["20mm f/1.8G","D750","FX"],"title":"Nikon D750 + 20mm f/1.8G Nikkor"},{"categories":["Photography"],"contents":"The Problem One of the big issues I face while photographing outdoors is that of having to carry a big 1.5 kg camera, and usually additional lenses and accessories weighing another 1-2 kg, all the time while out shooting for the day. The default carrying solution – neck strap – has a couple of disadvantages:\nIt’s extremely tiring for the neck muscles It’s very fiddly, and gets in the way too much Most of you might already have faced problem #1. Problem #2 is something you might not realise, until you start shooting without the camera strap. I let my cameras go without the strap a couple of years ago, and now I can’t even think of having to shoot with the strap on the body.\nFor quite some time, I used to carry the camera in an unzipped LowePro AW Rezo 140 (discontinued) camera bag. It solved the problem of fidgety handling, but my neck still complained.\nDIY Solution This year I looked for alternate solutions, especially two-body solutions, since I’ve found carrying a couple of bodies vastly quicker and more flexible than carrying one lens or switching lenses. After doing a few trials and Internet reading, I decided to go for either a shoulder harness or a waist-belt mounted solution. The first attempt was a DIY system. I repurposed the AW Rezo as a waist pack for one camera and fabricated a twin-strap shoulder harness for the other. Here’s how my solution looked:\nThe shoulder harness is carrying a D7100 + 16-85mm VR while the waist pack is carrying a D90 + 55-300mm VR, an RX100, my wallet and hotel keys and small accessories.\nI really loved the shoulder harness and it performed well on walking and biking excursions but it’s only suitable for a small body with a short lens because the lens sticks out in the front. The waist pack was very useful but a bit restricting in mobility and a bit too ungainly.\nEnter b-grip While looking for professional carrying solutions, I came close to settling for BlackRapid Double, but its insane price and extremely dangly solution put me off of it. The other solution I liked was b-grip Belt Holster and eventually bought it. The first time I put it on, it felt so weird that I cramped my leg out of undue stress. I had to wear it with/without mounting the camera several times at home to get accustomed to it. Finally, I used it during my latest trip to New Delhi and here’s what I have to report about it:\nSturdy System: The designers of b-grip have paid a lot of attention to small details that make sure your camera gets a sturdy and stable seat when nestled in the holster. The quick-release plate has a unique screw lock that prevents the plate from coming loose in repeated use – one of the best solutions I’ve found for essentially the biggest risk in various carrying solutions. WDS Works: b-grip features a contoured holding plate that incorporates what the makers call the _Weight Distribution System _(WDS). This system actually works and is not just marketing jargon. Without WDS design, there would be a lot of torque exerted on the belt by the camera and it would tend to make the belt dig into the waist while making the camera bang against the legs. No such worries with WDS. The camera sits rigid. Low Fatigue: I had the holster on for 8 hours, with a D7100 and 16-85mm VR clipped on and I didn’t feel any fatigue. The broad nylon tactical belt that it ships with actually helped in reducing lower-back fatigue by virtue of being tightly wrapped around the waist. Ever since getting rid of the neck strap, I used to walk with the camera in hand to avoid fiddling with camera bags. The b-grip was so quick and convenient that I simply clipped the camera on as soon as I was done clicking, even if I had to take it off again just half a minute later. No more wrist strain from holding the camera while walking! Flip-Out Stand in QR Plate: This might seem like a frivolous detail, but is actually very helpful in the field. The b-grip quick-release plate comes with a flip-out stand that allows putting the camera down on a table-top, etc. without causing the camera to topple forward under the lens’s weight. There are a few issues with it, though:\nStress on Tripod Screw: Essentially, the weakest point in the b-grip and most other systems in my mind is the tripod screw of the camera itself. I fear a lot for its threads getting sheared due to stress because it’s not designed for hanging the camera by. I would consider having a tether running to the shoulder to prevent a disaster in case the tripod screw fails. It took me a lot of painstaking research to come up with the DIY twin-strap harness that didn’t hook into the tripod screw for this reason. Sitting is Hard: With the b-grip strapped on, the harness plate must be moved to the side to be able to bend ones legs enough to sit. In the slightly cramped backseat of a car, I had to sit with the camera in my hands and yet the b-grip dug into my sides. In a more spacious setting, though, it wasn’t much trouble setting the camera aside and sitting down. No Stowing Solution: Closely linked with the above, I missed a camera bag when I had to sit for extended periods and wanted the camera to be securely stowed away. Final Verdict Based on the experience in the field, I would highly recommend b-grip Belt Harness as a carrying solution. I’ve researched several belt harnesses and though I haven’t tried them all, I doubt that any of them would exceed b-grip in security and convenience.\nBefore you go out and buy it, however, you must ascertain for yourself that a belt harness is what you want. Shoulder harnesses are almost as comfortable and the nature of these solutions is different enough that some people might prefer a shoulder harnesses over a belt harness.\nWhatever you do, though, get rid of that neck strap right away!\n","permalink":"https://tahirhashmi.com/posts/2014/12/22/carrying-cameras/","tags":["b-grip","harness"],"title":"Carrying Cameras"},{"categories":["Opinion"],"contents":"Update: I couldn’t stay away for long. That, and I conquered the sharpness gremlins plaguing my experience with the heavily used AF-S DX 16-85mm f/3.5-5.6G VR.\nWe had a good thing going for eight years or so, but it’s time now to bring it to a halt.\nBack when I started, I had no dearth of time to give to myself and I had no one but myself to spend money on. It was meant to be all for fun, nothing serious.\nSomewhere along the way, things became different. I still find some time but can’t give it to myself without some sense of guilt being felt or implied. I still have money but spending it on something that no one other than me values doesn’t seem to be sensible any more. The fun is still there but seriously, there’s too much disappointment to make it worthwhile.\nBack when I started, digital cameras used to be rubbish. It took the right kind of knowledge and tools to make a picture look good. But if you had the right equipment and knowledge and tools, your photo could stand head and shoulders above the average in terms of IQ (Image Quality). That alone used to bring about a sense of achievement. Now though, you can’t achieve anything noticeable with just a good camera and a computer because digital cameras have matured. They all make stellar pictures compared to what the average camera captured in the previous decade.\nNow that’s a really good thing for photography in general. It’s become more accessible to everyone and the general quality of photographs is increasing every year. Having access to a camera on the phones is also giving a lot of people a lot of practice. But herein lies a problem. Being a photography/camera buff today is like being an audiophile in the 80s – back then, good sets of\nspeakers used to be rare and very expensive. Now the average consumer-grade speakers can reproduce a pin drop to a thunderous explosion with equal clarity and there are credentials – THX, Dolby, what-have-you – to help recognise their capability. You don’t need to be an audiophile to find a good sounding set of speakers. Just get a middle-of-the-price-range set with some fancy stamps and 495 of 500 of your friends will approve of it.\nNow you might be thinking, hang on a minute, that analogy is bunkum. An audiophile doesn’t create music, he just consumes it. A photographer on the other hand creates something. True, that, but only to an extent. There are two types of photographer. One who shoots what he sees, and the other who makes something worth seeing. I would be monumentally dishonest if I, for a moment, claim that I am a photographer of the latter type.\nSo, all said and done, my photographs are only as good as my equipment and what I shoot.\nAnd that’s a big problem.\nIf I need my pictures to improve I’d either need to improve my equipment or I’d need to find something better to shoot. Now, I have two class-leading cameras (Sony RX100, Nikon D7100) so I am quite far down the curve of diminishing returns. Yes, my pictures would look better with a Nikon D800 and 16-35mm f/4 Nikkor or a Sony A7r with 24-70mm f/4 ZA or some such combination of full frame body with an exotic lens. But how many of my 500 friends be able to tell the difference? 5, I reckon.\nSo, let’s look at the other bit – what I shoot. Unfortunately for me, I don’t find anything that I shot in the last 6 years worth hanging on my walls. I’m just guessing here, but I think it’s because I’ve only shot common tourist locations in India. India is great, it’s beautiful and colourful and… for me, it’s all the usual stuff. People like photographs of stuff that’s different from the usual. I still find pictures that I clicked 7 years ago in California or 6 years ago in London to be better than anything else that came after that.\nAll in all, we’ve come to a point where I am thoroughly unimpressed with my photographs and I have no more willingness to throw the requisite time or money at it.\nNot at this time.\nAnd that’s not OK, because I can’t bear with myself doing a half-assed job of it, on and on and on.\nSo I must stop.\n","permalink":"https://tahirhashmi.com/posts/2014/05/20/farewell-photography/","tags":null,"title":"Farewell, Photography"},{"categories":["Technology","Programming"],"contents":"Contemporary application design discipline is deeply rooted in Object Oriented Analysis and Design and inheritance is a key concept in OOAD. Go does not support classes and inheritance in their classic OOP sense but since many of us are trained in OOP, the loss of an important design concept sometimes feels restrictive.\nEven though I knew about embedding and interfaces, their connection with classic inheritance wasn’t quite obvious. I set out to understand how I could emulate the coarse inheritance semantics in Go, without going into fine nuances. That in turn has helped me understand embedding and interfaces in a deeper way and I hope it would help me better design Go types and methods for extensibility.\nThe Task For the purpose of this exercise, we shall attempt to emulate the following class hierarchy:\nDeriving from class “Hello” The class Hello has two public methods along with its constructor. genHello() is used to generate a greeting message, like “Hello World!” while sayHello() actually prints it out. We shall derive AltHello class from Hello, which stores its own copy of the greeting and overrides the genHello() method to give us a different message.\nHere’s an implementation of the above scheme in C++:\n#include #include #include using namespace std; class Hello { private: string greeting; public: Hello(string greeting) : greeting(greeting) {\t} virtual string genHello(string \u0026amp;to) { ostringstream ss; ss \u0026lt;\u0026lt; this-\u0026gt;greeting \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; to \u0026lt;\u0026lt; \u0026#34;!\u0026#34;; return ss.str(); } virtual void sayHello(string \u0026amp;to) { cout \u0026lt;\u0026lt; this-\u0026gt;genHello(to) \u0026lt;\u0026lt; endl; } }; class AltHello : public Hello { private: string greeting; public: AltHello(string greeting) : greeting(greeting + \u0026#34; `\u0026#34;), Hello(\u0026#34;\u0026#34;) {} string genHello(string \u0026amp;to) { ostringstream ss; ss \u0026lt;\u0026lt; this-\u0026gt;greeting \u0026lt;\u0026lt; \u0026#34;Alt\u0026#39; \u0026#34; \u0026lt;\u0026lt; to \u0026lt;\u0026lt; \u0026#34;!\u0026#34;; return ss.str(); } }; int main(void) { string greeting = string(\u0026#34;Hello\u0026#34;); string to = string(\u0026#34;World\u0026#34;); Hello *h = new Hello(greeting); h-\u0026gt;sayHello(to); // Hello World! Hello *ah = new AltHello(greeting); ah-\u0026gt;sayHello(to); // Hello `Alt\u0026#39; World! delete(h); delete(ah); } The output from this program is:\nHello World! Hello `Alt\u0026#39; World! Type Embedding and Implementation Inheritance To inherit the implementation from another type in Go, we use embedding. So let’s try it out. Here is the first draft of our port of the above C++ code into Go.\npackage main import ( \u0026#34;fmt\u0026#34; ) type Hello struct { greeting string } func (h Hello) SayHello(to string) { fmt.Println(h.GenHello(to)) } func (h Hello) GenHello(to string) string { return h.greeting + \u0026#34; \u0026#34; + to + \u0026#34;!\u0026#34; } type AltHello struct { greeting string Hello } func (a AltHello) GenHello(to string) string { return a.greeting + \u0026#34;Alt\u0026#39; \u0026#34; + to + \u0026#34;!\u0026#34; } func main() { h := Hello{\u0026#34;Hello\u0026#34;} h.SayHello(\u0026#34;World\u0026#34;) // Hello World! ah := AltHello{\u0026#34;Hello `\u0026#34;, Hello{\u0026#34;\u0026#34;}} ah.SayHello(\u0026#34;World\u0026#34;) // World! } When this program is executed, we get:\nHello World! World! Wait, this doesn’t match the C++ output. What are we missing? While AltHello inherited the implementation of SayHello() from Hello, the version of GenHello() that gets called is also from Hello. That neither picks up the data from AltHello.greeting, nor the altered formatting from AltHello.GenHello(). How do we make it such that we can call the derived version of GenHello() without having to re-implement SayHello()? The first step to achieving that is to be able to access both instances through a common type, like we do with Hello* in C++.\nType Substitution Through Interfaces Go accomplishes type substitution through the use of Interfaces. Interfaces allow us to define an abstract behaviour and have different types satisfy that behaviour. Since that’s the relationship we want to establish between our two types, let’s try creating an interface for the methods of our types and see how it goes.\npackage main import ( \u0026#34;fmt\u0026#34; ) type Greeter interface { SayHello(string) GenHello(string) string } type Hello struct { greeting string } func (h Hello) SayHello(to string) { fmt.Println(h.GenHello(to)) } func (h Hello) GenHello(to string) string { return h.greeting + \u0026#34; \u0026#34; + to + \u0026#34;!\u0026#34; } func NewHello(msg string) Hello { return Hello{msg} } type AltHello struct { greeting string Hello } func (a AltHello) GenHello(to string) string { return a.greeting + \u0026#34;Alt\u0026#39; \u0026#34; + to + \u0026#34;!\u0026#34; } func NewAltHello(msg string) AltHello { return AltHello{msg + \u0026#34; `\u0026#34;, NewHello(\u0026#34;\u0026#34;)} } func main() { var g, ag Greeter g = NewHello(\u0026#34;Hello\u0026#34;) g.SayHello(\u0026#34;World\u0026#34;) // Hello World! ag = NewAltHello(\u0026#34;Hello\u0026#34;) ag.SayHello(\u0026#34;World\u0026#34;) // World! } The output from this is:\nHello World! World! The revised code has been highlighted in bold. Although, this has allowed us to unify Hello and AltHello under a common type, Greeter, it still hasn’t allowed us to achieve dynamic dispatch of AltHello.GenHello() through the interface. This is the part where I stumbled a bit because the expectation from classic OOP languages is that the above changes should be sufficient to enable dynamic dispatch.\nWhat is actually happening here is that even though we can reach Hello.SayHello() from an AltHello instance, whether we go through an interface or not, Hello.SayHello() does not have the information needed to switch to AltHello.GenHello() where needed. To understand why, let’s look at the method implementation in isolation:\nfunc (h Hello) SayHello(to string) { fmt.Println(h.GenHello(to)) } The parts in bold should highlight the problem more clearly. The receiver of SayHello() method is defined as an instance of type Hello. That being the case, SayHello() will always call Hello.GenHello(). How do we make SayHello() call GenHello() on the correct receiver?\nEnabling Dynamic Dispatch The key to this lies in how dynamic dispatch works in, e.g., C++. The class methods implicitly receive a pointer to the object, named this. Even if the pointer is of a base class type, it points to an object of the class that we instantiated, whether it is the base class or the derived class. That in turn holds a reference to its own set of methods. The same effect can be achieved in Go if we somehow emulate the this pointer. How do we do that? Like so:\ntype Greeter interface { SayHello(Greeter, string) GenHello(string) string } func (h Hello) SayHello(g Greeter, to string) { fmt.Println(g.GenHello(to)) } From the previous section, we had understood that type substitution can be achieved through interfaces. To solve our problem of making SayHello() call GenHello() on the right type, we pass it an instance of the concerned type through a Greeter interface parameter and make it call GenHello() on the interface parameter. Voila! Now our method can automatically call the correct instance of the overridden function and we have Dynamic Dispatch!\nHere’s the revised code in full:\npackage main import ( \u0026#34;fmt\u0026#34; ) type Greeter interface { SayHello(Greeter, string) GenHello(string) string } type Hello struct { greeting string } func (h Hello) SayHello(g Greeter, to string) { fmt.Println(g.GenHello(to)) } func (h Hello) GenHello(to string) string { return h.greeting + \u0026#34; \u0026#34; + to + \u0026#34;!\u0026#34; } func NewHello(msg string) Hello { return Hello{msg} } type AltHello struct { greeting string Hello } func (a AltHello) GenHello(to string) string { return a.greeting + \u0026#34;Alt\u0026#39; \u0026#34; + to + \u0026#34;!\u0026#34; } func NewAltHello(msg string) AltHello { return AltHello{msg + \u0026#34; `\u0026#34;, NewHello(\u0026#34;\u0026#34;)} } func main() { var g, ag Greeter g = NewHello(\u0026#34;Hello\u0026#34;) g.SayHello(g, \u0026#34;World\u0026#34;) // Hello World! ag = NewAltHello(\u0026#34;Hello\u0026#34;) ag.SayHello(ag, \u0026#34;World\u0026#34;) // Hello `Alt\u0026#39; World! } And here’s the output:\nHello World! Hello `Alt\u0026#39; World! That’s exactly what we wanted!\nConclusion There are three semantic aspects to classic OOP inheritance – inheriting implementation, type substitution and dynamic dispatch of overridden methods. We have seen how we can use type embedding to inherit the implementation, interfaces for type substitution and passing an interface parameter to a method to enable dynamic dispatch.\nAlthough Go does not support classes and inheritance in the classic OOP sense, understanding how to achieve the effects of inheritance in part or whole would help in designing applications meant to be written in Go. Go separates implementation inheritance from type substitution and makes dynamic dispatch more explicit and deliberate. This could potentially help in coming up with cleaner design or avoiding mistakes in design modification since the decision to use one of these features does not implicitly enable the others.\n","permalink":"https://tahirhashmi.com/posts/2014/01/22/inheritance-semantics-in-go/","tags":["Go","Golang","Inheritance","OOP","programming"],"title":"Inheritance Semantics in Go"},{"categories":["Photography"],"contents":"The results of the Sensor Format Blind Test are out!\nI got a total of 13 responses, which is a bit too low for a full-blown, conclusive ranking analysis. We can, however, do some meaningful analysis if we ask simpler questions. To start with,\n“Is the photo from Format X the best?”\nTo answer this question, we simply count the number of respondents who placed the photo from the given format as the best. Here are the results:\nFull Frame: 8 votes APS-C: 3 votes 4/3: 1 vote 1″: 0 votes Now, despite the very small sample size, based on the rules of statistics we can say with great confidence that fewer than 15% people would judge an image from a 4/3rd sensor as being better than that from a bigger sensor. That percentage drops to a minuscule 5% for the 1″ sensor. In other words, the IQ improvement with bigger sensors is quite perceivable, especially when we jump 2 or 3 steps across formats. It’s harder to quantify the difference among adjacent formats, though.\nNow, let’s flip the question around and ask,\n“Is the photo from Format X the worst?”\nWe’ll now count the number of respondents who placed the photo from a given format last in the ranking. Here are the results:\nFull Frame: 0 votes APS-C: 0 votes 4/3: 1 vote 1″: 12 votes Wow! That’s very poor showing by the 1″ sensor. What this basically means is that if you shoot the same equivalent picture, more than 95% viewers will perceive your image as being good quality if you shoot with a full frame or APS-C body, with 4/3 not being too far behind. However, if you shoot with a 1″ sensor camera, at least 85% of the people will be able to tell the loss of image quality compared to other formats.\nAll of this is expected, right? Of course, the bigger sensors will give better performance at the same settings… but, did we use the same settings? Time now to reveal the actual cameras and settings chosen.\nTest Scenario Revealed The sample images in the poll were chosen to give the best chance to the “DoF equivalence” (explained further down) argument and see how strongly it holds. Click to load DPReview’s New Studio Comparison Tool in a new window\nCameras I selected these cameras as the best representatives of their respective formats available in the comparison tool. I hope you wouldn’t disagree that these are very good cameras, especially the E-M1 and D7100. I would have picked Nikon D800E for full frame but it wasn’t available. I also tried Canon 5D Mk III but to me it appeared very slightly worse than D800, although it was still quite a bit better than Nikon D7100. The J3 was a bit of a surprise to me as I expected RX100/RX100 II to be the best, but they weren’t. I did not consider the X-Trans sensor equipped Fujifilm cameras since it’s quite different in characteristics and hence not directly comparable with other sensor types.\nISO Settings I started with the best ISO setting for the smallest sensor and bumped up ISO by 1 stop for each format to account for reduction of aperture at the same shutter speed. Thus, we were comparing a D800 at ISO 1600 with a J3 at ISO 200 and the J3 was still comprehensively beaten. I didn’t choose very high ISOs because the performance gap at high ISOs increases further. Try ISO 25600 through 3200 from FF to 1″ and the differences show up more clearly.\nOther Settings The “Web” size was chosen for comparison since this is where we see the maximum downscaling and the differences between formats are reduced. RAW format was chosen to take the in-camera JPEG converters out of the equation since they introduce a lot of variables. “Daylight” lighting was chosen because it matches the low ISO scenario more closely and avoids stressing white-balance correction in monochromatic light.\nCommentary Conventional wisdom says that having a bigger sensor generally yields better image quality. Proponents of smaller systems argue that in practical situations the differences between image quality are nullified. The practical situations being,\ndownscaling of images for viewing on screen or small prints and, “DoF equivalence”. The first point is fairly easy to understand. The second one is a bit tricky. DoF (Depth of Field) is an indicator of how many things (front-to-back, not sideways) are in focus in a given frame. All things being equal, a small sensor camera would have a greater DoF than a large sensor camera. Therefore, to shoot an identical frame with the same DoF, a large sensor camera needs to stop down (narrow down) its aperture, which in turn would require increasing ISO to maintain the same shutter speed. Higher ISO = worse performance. Here’s an example of DoF equivalent settings across formats in an indoor lighting situation:\nFormat Aperture ISO Shutter Full-Frame f/5.6 ISO 8000 1/80s APS-C f/4 ISO 4000 1/80s 4/3 f/2.8 ISO 2000 1/80s 1″ f/2 ISO 1000 1/80s DoF equivalence is not always a big deal. In challenging low light you would more often try to use the widest aperture allowed by your lens than trying to maintain DoF. E.g. in the above scenario, I’d shoot Full-frame at f/2.8, if I could. Often deep DoF is required outdoors where the daylight is enough to allow shooting at low ISO even while stopped down. In conditions where you do trade DoF equivalence for ISO, you’ll get better IQ because you are more likely to improve lens performance (increased resolution, reduced vignetting, reduced flare) when you stop down. This poll anyway shows that you get perceivably better IQ from bigger sensors – even when the apertures are optimum for all formats and the results are scaled down significantly.\nNote: The bit about “optimum aperture” is quite important. Almost all except some very expensive prime lenses operate significantly worse at their widest aperture than a few stops down. However, beyond a certain value stopping the lens down further makes the image blurry due to diffraction. While full frame lenses perform well from f/2.8-f/16, 1″ lenses start getting diffraction limited at f/5.6 – even before their corner resolution reaches full potential! The requirement for ultra-thin DoF in practice is also not such a great issue either. DoF reduction is one of multiple ways of subject isolation. More importantly, trying to manage backgrounds only through DoF reduction can lead to situations like these:\n“Check this out, your eyes are so sharp, you can see me holding the camera in them!”\n“Yeah, but why does my earring look so blurry and my nose appear as if it’s beginning to vaporise?”\nIn practice, I’ve often found myself stopping down the 1″ RX100, so it’s not like any of the surveyed formats can’t produce decent background separation. If you know the tricks, it can even be achieved with pre-historic cellphone cameras with wide-angle lenses!\nConclusion The choice between cameras is never easy and choosing a sensor format has a long term impact. One question that comes up is whether it’s worth the price and kit size to go for a bigger sensor.\nThis poll shows that the IQ advantage of a bigger sensor doesn’t get completely wiped out, even with a lot of levelling factors involved, though in some situations the difference between adjacent formats is quite small. It also shows that the 1″ sensor is still below most people’s “good enough” bar.\nSo, a Nikon D3200 + 18-140mm f/3.5-5.6 VR DX Nikkor (US $1000 approx.) has a better image quality proposition than a Sony RX10 ($1300) despite its f/2.8 lens with the same zoom range. For only 10% more money, a Nikon D610 + 50mm f/1.8G Nikkor would give better IQ for equivalent shots and 2x higher ISO capability than an OM-D E-M1 with 25mm f/1.4 Leica.\nIf you’re on a tight budget and simply want something that most people consider good, any current m4/3 or APS-C system (depending on your kit size preferences) would be a good choice. If you’re image quality conscious, though, don’t get carried away by all the equivalence chatter. A full frame camera will deliver the results. Every time.\n","permalink":"https://tahirhashmi.com/posts/2013/11/08/sensor-format-blind-test-results/","tags":["performance","poll","Sensor"],"title":"Sensor Format Blind Test Results"},{"categories":["Photography"],"contents":"TL;DR Rank the four shots below from best to worst. Mention the position as top-right, bottom-left, etc. Submit your ranking as a comment. Rank the above images from best to worst. Mention positions as top-right, bottom-left, etc. The Fuss These are interesting times for camera enthusiasts. Digital cameras have advanced to a level where most cameras are capable of publication quality snaps and you don’t even need to carry a camera to take good pictures, thanks to smartphones.\nOne big debate this has fuelled is regarding the importance of sensor size now. Some people believe, as they always have, that bigger is better and a full-frame sensor will always win.\nSome people believe that beyond a certain threshold (1/1.7″? 1″? 4/3?) sensor size becomes irrelevant because the rules of equivalence (same DoF, same shutter speed, same FoV) dictate that all sensors will be operating in a similar fashion since they’re seeing the exact same amount of light, for the exact same amount of time.\nFurthermore, it is argued that most pictures are never going to be printed and hung on walls. Instead, they will be viewed on phone, computer or TV screens at a nominal resolution of, currently, 2MP (HDTV resolution) or in future, 8MP (4k TV resolution). At such resolutions, minor differences in image quality become immaterial.\nTo find out how deep the shade of grey is between the black and white of FF supremacy vs. FF irrelevance, I’ve devised a simple test. The image you see above/below is a PNG screenshot of DPReview’s Studio Comparison Tool, loaded with camera test shots from cameras with different sensor sizes. I have chosen the web resolution from the tool to match the “common use” scenario, i.e., sharing pictures on the web.\nNow, can you rank these shots from best to worst? Mention the position as top-right, bottom-left, etc. Submit your ranking as a comment below. Rank the above images from best to worst. Mention positions as top-right, bottom-left, etc. Once I have obtained enough responses, I’ll publish a summary and also reveal the link that will load this very comparison in DPReview’s Studio Comparison Tool, thereby revealing the cameras chosen.\nThanks in advance for participating!\nUpdate: The poll is closed and the results are out!\n","permalink":"https://tahirhashmi.com/posts/2013/10/31/sensor-format-blind-test/","tags":["poll","Sensor"],"title":"Sensor Format Blind Test"},{"categories":["Technology"],"contents":"A few days ago, my BBF (Big Boss Forever) Vijay R asked the following question:\nAny resources on how to map OO design (controlled state change via methods) to RESTful services? #help\n— Vijay Ramachandran (@vijay750) October 24, 2013\nHere’s what I think about it. There are a few things that are very different about designing HTTP APIs as compared to language-native implementation design:\nThe goal of an HTTP API is to minimise coupling and facilitate interoperability, which is less of a concern when the usage environment is restricted to a single programming language and its runtime\nHTTP APIs must be designed to maximise stability and minimise change over time. On the other hand, implementations must be designed to support business evolution.\nImplementation design often has a lot of artefacts that result from addressing the constraints of the implementation environment (e.g. GoF Patterns) whereas HTTP APIs usually don’t have a target environment to deal with.\nGiven the above differences, it’s clear that a 1:1 mapping between a language-native API to an HTTP API will be sub-optimal, unless there is an adapter interface that is designed with the intent of language independent export. The above list of priorities gives some guidelines about how to design such an interface:\nExchange Plain Old Datatypes or something like C-Structs. This comes from the study of coupling characteristics of software components, which indicates that Data Coupling is the most desirable, low level of coupling. Data Coupling, by definition, involves exchange of Plain Old Datatypes.\nProvide Coarse-grained Methods that exchange a minimal data “representation” of business resources. Give the clients basic CRUD APIs and hide all validation and business logic behind their implementation. The client should only have to deal with what data they want to save or retrieve, with minimal knowledge of how it is done, other than the rules of business that the client has the liberty to select. This is the essence of Representational State Transfer (REST, yes, that one).\nUse Ubiquitous Protocols and Wire Formats since we don’t know whether our API will be consumed by tunnelling middleware (proxies, translators, etc.), dumb end-points (archivers), smart end-points, humans, or even monkeys, perhaps.\nFollowing the above thought process, it is clear that seeking to directly map a language-native OO interface will result in something more like RPC than a REST interface. There is nothing wrong with RPC – it’s just a different (tighter) level of coupling compared to REST. Whether to directly map the OO API to RPC end-points or provide a more loose data level coupling (or something in-between) then becomes a question of intent – who are we writing the API for.\nAfter serving the role of an API provider for several years in various projects, I spent the last few years as the consumer of a third party API. I’ve come to believe strongly in the above 3 design principles. A data oriented API with few, uniformly designed methods is the easiest to work with. Most importantly, though, the worst pain that an API provider can inflict on a client is to make them update their code.\n","permalink":"https://tahirhashmi.com/posts/2013/10/28/mapping-oo-interfaces-to-rest/","tags":["Interface Design","REST","webservices","architecture"],"title":"Mapping OO Interfaces to REST"},{"categories":["Photography","Reviews"],"contents":"It’s been more than six months since I have had a Nikon D7100 for my own and I have been thinking of writing down my thoughts about the camera. However, these six months with the camera have been a bit of an up-and-down journey and it’s only now that I feel like I know the camera enough to write about it.\nThe D7100 is an enthusiasts’ camera and it will make you earn that title before you can be get along with it. Its 24MP sensor is like that brutally honest friend who wouldn’t flatter you and shine a spotlight on your weaknesses instead. Once you’ve overcome those weaknesses, though, the end result is something that can make a casual snapshot look like a stunning, frame-worthy photograph. The abundance of twiddly bits and sweetest ever handling in this line of Nikons appeal to the enthusiasts and set it apart from the latestD5xxx series that has the same sensor but with much crippled functionality and controls.\n“The Houseboat” 1/60s・ƒ/5.6・ISO 100・16mm I’ve been a big fan of mirrorless cameras since the Sony α NEX came out. Back in December last year we got an Olympus OM-D E-M5 in the family and I wrote this after handling it:\nSince I got my own DSLR, I haven’t suffered the covet-thy-friend’s-property syndrome… until yesterday. The OM-D E-M5 is that good.\n— Tahir (@code_martial) December 26, 2012\nI even dedicated a song to it. The D7100, despite being a big and heavy DSLR with a medieval reflective trapdoor, has purged all desire for OM-D from my system. How did this come to pass? Read on to find out.\nThe Upgrade Before the D7100, I had been using the Nikon D90. Though it came out in mid-2008, I only acquired it in early 2010. Since then I also got a Sony α NEX-5, which I later swapped with Sony RX100 in 2012. The RX100 with its tiny 1″ sensor crammed with 20 mega pixels, superb Carl Zeiss optics and new software aids gave the D90 a really tough competition. The gap in IQ between these two cameras was so little that I found myself picking up the RX100 for convenience a lot more often than the D90.\nWhen the D7100 was announced in February this year, its specifications pushed all the right buttons for me – weather sealing, dual SD card slots, more twiddly bits, top-of-the-line AF module and 24 glorious mega pixels without anti aliasing – wow! I decided to go for it since it was clearly superior to OM-D E-M5 (my then favourite) in IQ, I already had lenses for it and the Fuji X-Trans equipped bodies didn’t make a big impression on me.\n“Shimmer” 1/500s・ƒ/16・ISO 125・300mm Handling Having started out with a Nikon D80, I’ve always liked the hardware controls scattered around the DSLR’s body because they make shooting fractionally (in some cases, significantly) faster. The D7100 takes speedy control to a whole new level.\nYou get to toggle Auto-ISO on or off with the front command dial while the rear dial, as always, adjusts minimum ISO. Similarly, AF area modes can be switched from the front dial while the rear dial adjusts AF mode. Both these settings previously required digging into the menus. Reviewing shots for shake or focus accuracy is now a 1-click operation. The OK button in the middle of the D-pad can be programmed to zoom in to a pre-defined level of magnification and again zoom out to full area.\nShooting modes can be selected via a dial stacked below the exposure program dial and both dials now have push-button locks to avoid accidental changes. Mirror-up mode gets moved among shooting options, so even less digging through the menus while you’re on the tripod and you want to lock up the mirror. Just switch to MUP mode, click once to raise the mirror and once more to take the shot and drop the mirror down. Simple!\nThe D7100 also comes with a new Auto-ISO implementation that adjusts minimum shutter speed according to the focal length of the lens attached, instead of keeping it at a fixed value. By default it selects 1/ speed. E.g. for a 50mm lens it uses 1/80s (slowest shutter speed above 1/75). However, you can change it to be 2x or 4x slower/faster than the default. This is the most sensible and most capable Auto-ISO implementation I’ve come across anywhere.\nThere are many other tweaks and options that make it really quick and non-fussy to use the D7100 with great control over its operation without requiring a visit to the very long menus.\nImage Quality While the D7100 is an undisputed delight on the usability and handling front, I’ve had a really tough time with IQ. The 24MP sensor has a bit of a Jekyll and Hyde personality. Having used a 10MP D80 and a 12MP D90 earlier, I wasn’t prepared for the carnage that viewing 24MP images at 1:1 resulted in. Couple that with the fact that the RX100 with its Carl Zeiss optics gives highly detailed output on the 20MP sensor and I was downright disappointed.\nThe reliance on PDAF (as against the slower but more accurate CDAF on RX100) leads to quite a bit of focus inconsistency, especially with fast primes. While shooting people, again with fast primes, they sometimes move in the fraction of a second between focus lock and shutter release. I mostly shoot hand-held so camera shake also shows up now-and-then. All these problems become glaringly evident on the D7100. To add to the agony, most consumer zooms don’t appear very sharp at 1:1 on 24MP sensors either.\n“Earl Grey Tea” 1/20s・ƒ/5.6・ISO 250・35mm Over the last few months I have both grown accustomed to ignoring pixel level flaws and become more cautious in my shooting style to minimise such defects. In a way, this also helps while shortlisting keepers from a shoot. Identifying minor differences in contrast or sharpness is much more difficult with lower resolution sensors.\nIn terms of noise, the sensor again has its quirks. Up to ISO 3200, the results are pretty manageable. At ISO 6400 and above, there’s some low frequency noise which is easily removed without affecting overall sharpness, but there’s also some high frequency chroma noise, which becomes a bit difficult to handle.\nAll of the above pixel-level agony is only one side of the story, though. Scaled down, the images from D7100 look really great – significantly richer tonality compared to D90 and RX100, massive Dynamic Range (13.7 stops as per DxO Mark) and great ability to resolve detail. All these characteristics come together to give the photos a subjective quality that a few years ago I could exclusively (and easily) find in photos from full-frame cameras.\n“Edge of the Field” 1/1000・ƒ/8・ISO 400・16mm Final Word Ever since I got serious about photography, I have been a pixel peeper. My involvement with photography had a lot to do with working the RAW files on the computer, trying to get the maximum sharpness and maximum highlight detail with the least noise, etc. I’ve always wished that the camera would do better than it did.\nThe D7100 is the first camera that doesn’t leave me dissatisfied. It can show more detail than I usually care about. It has more dynamic range than I can project meaningfully in a JPEG image or print. It doesn’t get bogged down by noise even at ridiculously high ISOs. In short, it doesn’t give me an excuse to blame the equipment.\nInstead, it makes my own flaws obvious to me. It doesn’t tolerate sloppy shooting. It makes me want to shoot better things. It makes me want to shoot things better. It wants me to rise up to meet its level of finesse. And it makes my journey towards better photography pleasurable by being such a joyful thing to operate.\n“Sunrise Over the Mohalla” 1/160s・ƒ/5.6・ISO 100・50mm ","permalink":"https://tahirhashmi.com/posts/2013/10/28/six-months-with-nikon-d7100/","tags":["camera","D7100","Photography","Review"],"title":"Six Months with Nikon D7100"},{"categories":["Opinion"],"contents":"I woke up this Tuesday morning to a shock.\nOMG this Yahoo! bar on top of @Flickr is fugly. If this bar doesn’t go from Flickr, I will. pic.twitter.com/dolGKR8jJu\n— Tahir (@codie) July 2, 2013\nThis bar is wrong in a lot of ways. The way it stands out in all its ugliness from the rest of the Flickr visual design speaks volumes about how it was executed (thoughtlessly, in a draconian way), how little the Flickr development team have a say in how things work (I can’t imagine a product owner who would allow such visual desecration of their product without a protest) and how little Yahoo! as a company cares about its vocal community (no attempt made by Flickr staff to respond to the outpour of grief in their help forum).\nThe newly redesigned Flickr reminded me that I’ve been on Flickr for a very, very long time – close to 9 years. My joining of Flickr pre-dates its acquisition by Yahoo!. I have been with Flickr through the several major design changes to its layout and functionality and through the waves of people from Flickr community leaving for other competitors. More importantly, I was generally quite vocal in my defence of Flickr and the changes it brought about. The only time I previously considered looking for alternatives was when the site had not received any significant updates in ages.\nI think I was naïve. I think the people who left earlier had better foresight.\nI revoked my Pro account yesterday. Barring a few days of discontinuity between manual renewals, this is the first time in 8 years or so that I’ve dropped to a free account on Flickr.\nI don’t intend to continue with any more public photo uploads to Flickr. I haven’t decided which service to use next, partly because it’s a difficult choice. Flickr is a great website. Despite years of neglect in community building, it still has strong community participation. There are other photo hosting websites, other photo sharing websites and other online communities. But there’s none that brings all these aspects together the way Flickr does.\nLooks like I will have to find not one but two alternatives – a showcase and a community to replace my daily Flickr fix with.\nPS: Some people recommend deleting photos from Flickr or making them private to disallow Yahoo! from making advertising money from our pictures. However, I don’t think I’m going to do that. I don’t want to break the trust of scores of third party pages that have legitimately embedded my photos instead of just flicking them without credit.\n","permalink":"https://tahirhashmi.com/posts/2013/07/04/the-lost-flickr-of-hope/","tags":["flickr"],"title":"The Lost Flickr of Hope"},{"categories":["Opinion","Photography"],"contents":"To Shoot RAW or JPEG? I have been shooting photographs regularly for over 7 years now. I spent the first year shooting with a 2 Megapixel phone camera. Since then, however, I’ve almost always had RAW capable cameras and shot RAW compulsively. And why not? I get 16x or 64x more colour depth than JPEGs. I don’t have to bother about setting the right white balance, contrast or sharpness. I don’t have to choose between monochrome and colour at the time of shooting. I can figure all of that out on the computer during RAW conversion and even try out different settings for the same picture at my leisure. Why would I give up all this and shoot JPEG?\nThe Switch The reason, unsurprisingly, is that I find a lot less time these days to spend in front of a computer processing photos, which means that I could either shoot fewer shots, or save time on post processing.\nSome time last year, I started shooting in RAW + JPEG mode quite regularly. It allowed me to quickly share the obligatory pictures I took at social events with the participants who would be quite impatient to upload them to Facebook, etc. What I found was that often the JPEGs were great even for pictures that I cared about personally. After several months of this exercise – and due to quite some friction with handling Sony ARWs in my Nikon-centric workflow – I decided to give JPEG-only shooting a try. My Sony RX100 turned into a JPEG-shooter.\nI still prefer to shoot RAW in all conditions. However, forcing myself to shoot JPEGs on my backup camera has helped me get over the fear of not shooting RAW. The practice I get from shooting JPEGs has helped me gain more confidence with the way I shoot RAW and not keeping my friends and family waiting for weeks to see the “finished” photos has been a bit endearing as well.\nWhat does it take to shoot satisfying JPEGs?\nThe Shooting Discipline Since JPEG files have very little latitude for post-processing, it is important that the photograph be finished to as much of an extent as possible in the camera. This requires a bit of extra consideration while shooting. However, I found the change to be less drastic than expected.\nI have always been particular about using the right colour mode (Landscape, Portrait, Vivid, etc.) while shooting RAW even though it’s inconsequential because it helps me visualise the end result while looking at the preview in camera. This setting alone can nail contrast, sharpness, saturation and the overall “look and feel” of the photo. It is very important to get this right since the colour mode dictates the tone curve that would be used to down-sample from 12 or 14 bit sensor output to 8 bit JPEGs. The 4-6 bits discarded per channel better not be the ones that you need the most for your end result or you’ll have a tough time trying to work without the lost data in post processing.\nThe next most important thing for JPEG shots is white-balance. The white-balance setting for RAWs is totally inconsequential since it is just a multiplier applied to RAW channel data for Red:Blue ratio (warmth) and another for Green channel strength (tint) during conversion. However, extreme multipliers (e.g. in Tungsten, Fluorescent, Shade WB) tend to discard significant amounts of data from a channel (Red, Green, Blue, for the mentioned settings respectively). Having the wrong WB setting while shooting JPEGs makes it almost impossible to fix in post-processing. This is where a camera with better Auto-WB computation proves very helpful.\nAn exciting new development in recent cameras is the addition of “creative effects”, which you can use to add some more macro-customisation of the output as it is being recorded. This helps a lot in saving post-processing time by accomplishing more specialised results such has high/low-key shots, colour strength/weakness, dioramas, etc. right at the time of recording the photograph.\nFinally, since we are shooting JPEGs to avoid post-processing time, it’s important to get the exposure, composition, horizon tilt and framing right, as much as possible. Having done all of these right would get you the holy grail of being able to use your photo “SooC“.\nExamples Following are a few JPEG samples that I am particularly satisfied with and feel that they match the results I could have extracted from post-processing RAW output.\nThe picture above is a great example of using the right colour mode. This flight landed just a few minutes ahead of sunset (see the long shadows) and I wanted to capture the late afternoon atmosphere so I used the “Sunset” colour mode on the RX100. That got me the exact look I wanted and allowed this picture to be uploaded as-is with zero post-processing.\nThe above picture is an example of why it’s important to get the WB nailed as far as possible. I wanted to highlight the blues and suppress the greens in this shot, so I chose “Fluorescent” WB setting to achieve that effect. It got me to the ballpark colours but was a bit too strong on the effect, so I shot it again with Auto WB. Later while adjusting the colours in post-processing, I found that it was much easier to get the desired look from the “Fluorescent” setting than from the Auto WB setting. The latter led to increased noise – the usual outcome of pushing a JPEG to its limits. This picture also happens to be an in-camera HDR done superbly well by the RX100.\nThe above picture is an example of how you can use some of the conveniences of a modern camera. This picture is shot hand-held in “HDR Auto” mode. In this mode the camera automatically decides how many bracketed shots to take and what their EV deviation should be, it fires all the shots in a single click of the shutter button, aligns the images and does the tone-mapping. All of this happens in accordance with the colour mode you select (Vivid, for the above picture). This picture too has been posted with zero post-processing adjustments.\nThis last example shows the usage of creative effect modes while shooting JPEGs. This shot is in the “Rich Tone B\u0026amp;W” mode that creates a monochrome HDR photograph with a single click of the shutter. This too has been uploaded with zero adjustments in post-processing.\nConclusion Shooting photos in RAW format gives immense creative freedom in finishing the photograph and also simplifies the shooting process by requiring only that the photograph be framed and exposed reasonably well. However, this freedom comes at the price of time spent in post-processing on the computer.\nIf you already have an end-result in mind, you could save a lot of time and effort by trying to get the photograph as envisioned right in the camera itself. Contemporary cameras offer the JPEG shooter with a lot of creative options that make it even more quick and convenient to get the envisioned end result.\n","permalink":"https://tahirhashmi.com/posts/2013/05/11/shooting-jpegs/","tags":null,"title":"From Shooting RAWs to Shooting JPEGs"},{"categories":["Photography","Reviews"],"contents":"Last weekend I was in Goa on a leisure trip, which gave me an excellent opportunity for some photography. I carried the RX100 for landscape and street photography. The D90 also came along mainly for long range shooting with the 55-300mm VR and low light shooting with the 50mm f/1.8D. This trip allowed me to sort out some things related to the pros and cons of using a big DSLR vs. a small compact. Here’s how the cameras fared.\nLandscapes I used the RX100 for wide-angle landscapes and the D90 for telephoto landscapes, especially sunset shots, and beach candids. One peculiar issue with the beach scenes was the haze from the sea breeze. The RX100 RAW files offered very little possibility of recovering tonal data from the low contrast scenes. I should probably have tried ETTR, but then the idea was more of “Point \u0026amp; Shoot” action. The D90 RAW files offered much better scope of recovering contrast and I’m much more pleased with the results from the D90.\nThe other nagging problem with the RX100 was its inability to acquire focus fast enough. My friend and I were walking along the beach after sunrise and his Canon G12 consistently beat the RX100 at face detection and AF speed/accuracy. I guess the reason is that RX100 uses CDAF with very tiny pixels and having a hazy environment plays against that.\nScore – RX100: 0, D90 1\nStreet Photography This is primarily a review of the RX100 in these situations, since I didn’t use the D90 for this. I did a 12km walk through the town from Calangute to Sinquerim and back with the D90 slung on my shoulder and RX100 in my hand. I loved using the RX100 on the street. I mostly shot in Aperture priority, Auto ISO with the lens ring used for zooming (mostly set to 35mm or 50mm equiv.). The RX100 gave excellent results on the street with details being resolved to full 20MP. It was also good to have all the DoF available without using small apertures, thereby leading to lower ISOs.\nApart from the IQ, the other nice thing about the RX100 is that it is 100% operable single-handedly. There are no buttons or dials that can’t be reached from the right hand. Having the lanyard wrapped around the wrist allows for loosening up the body grip a wee bit to access the buttons along the right edge. Thanks to the customizability of the controls, I could adjust the RX100 for my needs as quickly as I do on the D90. Perhaps faster, in some cases because I could hold the image in the screen while single-handedly changing the settings – something that wouldn’t be possible with most controls on D90 that require taking your eyes off the viewfinder unless you know the buttons by touch. After 3 years of regular usage, I still can only operate the command dials and AF point selector by touch.\nThe only nagging factor again was RX100’s AF. For reasons beyond me, the RX100 confirmed focus while being just in the ball-park whenever I tried shooting without halting in my walk. That’s something I’ve rarely seen with the D90.\nScore – RX100: 1, D90: 0\nLow Light Low light photography was a mixed bag. The RX100 worked well for indoors with enough light to expose at 1/15s, f/1.8 ISO 3200 or brighter. Doing long exposures at night with RX100 was not that much of a success. You just can’t do astro-photography with the RX100, for example. The D90 + 50mm f/1.8D was complementary to the RX100 in that it was slower to focus and gave darker looking shots in the kind of light that RX100 could handle. However, the DSLR outclassed the compact where long exposures were concerned.\nI actually like using the RX100 for low light photography. I like the way it brightens up shadows and the fact that despite its small sensor size, it’s got so much data from its 20MP sensor that NR algorithms can do a pretty good job on its output. But the reasons are not just limited to photographic capability. A crucial factor is that most of my low light photography happens at social events – meeting up with friends, dinners, parties, etc. The non-intimidating RX100 just feels more “at home” in these situations than a big DSLR.\nScore – RX100: 1, D90: 1\nHandling One of the biggest questions I sought to answer with this trip was on the handling front – how does a compact, especially one that’s not the best handling of compacts available, fare against a DSLR, which is designed to be a champ at usability? At the outset, both styles of interface have their benefits. The compact overcomes its lack of twiddly bits with customizability and some clever UI design. This means that neither of these two cameras is at a disadvantage to the other where handling is concerned. To make things even more difficult, each camera has a crucial advantage that the other can’t have by design.\nThe RX100 simply excels at single-handed operability. Most of it depends on which 7 functions you assign to the “Fn” button. This is something that you would figure out over time, as you get to use the camera and learn which options you tend to access most. It took me 2 months to arrive at the configuration that I have now. The reward is that I can use the camera properly in situations where I couldn’t even think of using the DSLR.\nI occasionally take pictures of my kids while playing with them or putting them to sleep. I need one hand to engage with them or hold them. Being able to use the camera with the other gets me more pictures. While on this trip, I took the RX100 with me while I was up in the air parasailing. I could actually use the camera while suspended in the air from a parachute. That’s just brilliant! And something that I won’t do with the D90 even in my dreams!\nThe D90, though, has its own redeeming feature – TTL Optical Viewfinder. I’ve tried the viewfinder on Fuji X100 and hated it. I’ve tried the EVF on OM-D E-M5 and it just didn’t feel right. Nothing can connect you with the scene like a good TTL Optical Viewfinder. That’s something you can’t get in a body that doesn’t have a mirror and that’s something which dramatically changes your shooting experience and ability to shoot at the right moment.\nScore – RX100: 1, D90: 1\nConclusion The rather disappointing conclusion is that neither camera has a big lead over the other, which doesn’t help at all when trying to decide what format to go for. The interesting thing, though, is the areas in which the cameras gain over each other. I never expected the RX100 to lose out on IQ and performance in broad daylight. Nor did I expect it to hold strong against the D90 in low light.\nThe one contentious area is Street Photography, where I didn’t even use the D90. Would the scores have changed if I had used the D90? I don’t think so. What the D90 gains with its AF performance, it loses in stealth and lower resolution. One of the situations I’ve not reviewed above is also shooting out of a vehicle. I’ve shot out of moving vehicles with the D80 and the D90 but somehow, I never got the same success rate as I got with the RX100 set to 1/800s, f/2.8, Auto ISO.\nI’ve shot regularly with a 2 camera kit since I got my NEX-5, two years ago. I think a 2 camera kit is a winning strategy, with one camera covering the normal – tele range and the other covering wide – normal range. The big question is, what type of body do you use for each? If you’re looking to minimize weight, a small sensor body will yield a much smaller telephoto package. I think, however, that having a TTL OVF is vastly more important for telephoto shooting. Having a small camera for wide/normal shooting also works very well for street photography.\nSo that’s my recommendation: use a 2 camera kit wherever needed, with a good, small mirrorless for wide-to-normal focal lengths and a DSLR for telephoto.\n","permalink":"https://tahirhashmi.com/posts/2013/03/08/d90-vs-rx100-in-goa/","tags":["D90","Handling","Landscapes","lowlight","RX100","Shootout","Street","Travel"],"title":"Nikon D90 vs. Sony RX100 in Goa"},{"categories":["Photography","Reviews"],"contents":"My gear has evolved from P\u0026amp;S to DSLRs over the last five years, and each year I end up buying a new camera. The focus this year, was on reducing the size and weight of the camera gear, unlike previous years where I had been looking to acquire the latest sensor technology (although I was tempted to swap my D3100 with D3200!).\nWhat started out as a quest for cheap backup P\u0026amp;S, ended up as a story about Nikon 1. But not before I had analyzed every single camera in the INR 10k to 25k price bracket. Thanks to Flipkart which has a decent range of cameras. But somehow I resisted the urge to order J1 from Flipkart, and instead went to a Nikon store in Lajpat Nagar, where to my surprise I got a better deal.\nWhen Nikon launched the 1 series, I was one of those who ridiculed their decision to go for a piddly 1 inch sensor, with a meagre 10MP resolution. More so because of their outrageous pricing of the kits. Perhaps Nikon’s think tank never updated their market study which seems to have been based on the trends prevalent five years ago. Back then, these cameras would have been ground breaking, earth shattering, but things have changed since MFTs hit the market. Pricing has become a sensitive issue, in an already overcrowded CSC market. Thankfully Nikon acknowledged this fact, and dropped the price significantly for both its cameras, a move which seemed to work in their favor as J1 (and V1) dominated the CSC markets in Japan and Europe.\nJ1 in Action\nForm Factor : J1 is not Sony RX100 or Olympus XZ1, but its a sufficiently compact ILC, and light too Prior to buying the J1, I had mostly been carrying my DSLR kit – D3100, Tamron 17-50mm f/2.8 VC, and Nikkor 18-200mm f/3.5-5.6 VR, to my leisure trips and social events. Throw in a battery charger, UV filter and polarizer (both of which fit snugly on either of the lenses), and the kit would weigh two kilograms or thereabout. Those who have carried so much weight to day long sightseeing trips would agree that the dead weight spoils the experience.\nJ1 is light and how! The body weighs in a mere 235g, add a couple of lenses 10-30mm f/3.5-5.6 VR (115g) and 30-110mm f/3.8-5.6 VR (185g ONLY!) and you get a useful range of 18mm – 198mm (DX format) for just 535g. For the sake of comparision, D3100 weighs in a good 455g for body alone and similarly Olympus PEN E-PL1 weighs 335g and has a heavier kit lens by comparision.\nIn hands, the J1 is good to hold, its not feather light and that helps in keeping the hands steady. I have taken it to a few wedding events and it hasnt been a distraction, to say the least. The only gripe that I have is regarding the neck band that Nikon has provided in the kit. It would have made more sense to put a wrist band instead. I didnt even bother to put the otherwise flashy neck band, instead used an old wrist band that came with my Canon Powershot.\nLook and Feel : Solid build quality J1, like V1 has a solid build quality. Better than my D3100, and more reminiscent of my old D80. Both lenses have metal mount, and great rubber grip. The zoom rings are sufficiently smooth to operate and have no creep. In white avatar, the camera seems unimposing to the subjects, which is particularly good for clicking those natural, life-like photos where the subjects dont consciously pose for the camera. At weddings, I have often received cold stares and unwelcomed looks by the professional photograhers, even when I wasnt obstructing their field of view. Thankfully the J1 makes it easier for me to sneak in and take advantage of their lighting setup without the professionals bothering too much.\nUser Interface : A curious camera After using it, I labelled Nikon 1 J1 as a curious camera. It is definitely not a DSLR and nor is it a P\u0026amp;S. The UI, button layout and handling are all different. Yes, you find similarities with DSLR when you zoom the lenses, and to P\u0026amp;S when you use the buttons, but there are subtle differences and unique features (like unlock lens to switch on the camera). It takes time to adjust to the new UI (which isnt a bad thing), it kept me engaged for a while before I became familiar with the controls. For example, there is no Fn button to control ISO, but if you exit the “Menu” after adjusting ISO, J1 remembers the last option, so the next time you hit “Menu” you are directly taken to ISO settings. The menu is extensive but intuitive and easy to remember. Not once have I felt that UI has come in my way while setting up the camera for that quick shot. Infact, the camera feels more like a point and shoot, once you set it up properly. I mostly use Shutter priority mode along with Auto ISO, in order to harness the brilliance of the EXPEED 3 technology. The EV comp and ISO settings are fairly easy to reach just incase I want a greater control over the output.\nThe absence of PSAM dial / shooting modes is inconsequential for me since I hardly use it even on my DSLR. For those shots, where I would like to use manual mode, I would generally have enough time to delve into the menu and change the settings – a fair compromise.\nPerformance : Stellar metering and auto focus J1’s start up times are fast, and continuous shooting mode is a delight. The metering is brilliant in most situations, and focus is fast enough even in low light. In broad daylight, J1 performs as expected, yes, the auto focus is the fastest that I have seen in this form factor, and dare I say the most accurate, even comparable to D3100. The LCD brightness is good, so much that it can be lowered a bit without affecting legibility, in order to conserve battery. Overall, I have been very pleased with J1’s performance.\nImage Quality : Gritty but detailed output; Shoot RAW for best results The JPEGs are generally pleasing with typical Nikon colors, and the RAW output is definitely a notch above the JPEGs. The images are gritty even at base ISO (even when compared to D3100, let alone D7000), but the 1inch sensor captures plenty of details. The sensor size is no excuse for poor low light performance, however J1 fares well in absolute terms, comparable to what I used to get from my old D80. IQ is decent until ISO1600, beyond that things go down hill and require careful processing to get acceptable results.\nLenses : A lot of options to choose from Although 10-30mm kit lens is sharp, it is still the weakest lens in the lineup. Following up with excellent results that I got from the kit lens, I decided to buy 30-110mm f/3.8-5.6 VR and I must say this lens absolutely shines on J1. With a complex construction and different coatings, it is a must have for your Nikon 1. The VR works surprisingly well and the images come out sharper than Nikkor 18-200mm VR, and are mostly comparable to Nikkor 70-300mm VR II. Nikon has introduced a slew of lenses which include ultra wide zoom, pancake, fast prime, and travel zooms. The pricing however, remains to be a bit on the high side. I would have preferred the lenses to be atleast 25% cheaper than their DX counterparts. For those who would like to couple their Nikon DSLR glasses with J1 there is a handy FT1 adaptor, however, I have not seen it in action as yet, so I dont know for sure if it taxes the auto focus performance of the DX/FX lenses.\nBattery Life : Good enough, doesn’t warrant buying a spare On my last run, I have been able to get 350+ shots with VR always on, flash fired on atleast 50 occasions and the battery status still shows 50% usage. Pretty decent.\nConclusion : Not just a sidekick My quest for a back up camera, concluded with me buying a Nikon 1 J1 and replacing my old Nikkor 18-200mm f/3.5-5.6 VR with a shiny 30-110mm f/3.8-5.6 VR. My revised kit now consists of Nikon 1 J1, 10-30mm VR, 30-110mm VR along with Nikon D3100, Tamron 17-50mm f/2.8 VC. With J1 taking a more active role of a pocket sized tele-shooter, and not just being a sidekick to D3100.\n","permalink":"https://tahirhashmi.com/posts/2013/02/25/nikon1-j1-review/","tags":["j1","nikon1"],"title":"Nikon 1 J1 Review"},{"categories":["Photography"],"contents":"I am in the middle of a choice between interchangeable lens systems and one of my pressing needs is the selection of a system that lets me have a smaller, lighter camera bag. Now is not a bad time to be shopping for cameras, with so much choice being available across price brackets and IQ variance. As always, having a lot to choose from is another way of getting confused, so I decided to do some objective data analysis to figure out what’s what.\nThe choice is between 35mm (full frame or FX in Nikon parlance), APS-C, Micro Four-Thirds (MFT) and 1″ (CX in Nikon parlance) sensor platforms. It’s worth noting that there isn’t any FX mirrorless interchangeable lens camera on the market so far, so going FX means using a DSLR with a mirror and built-in OVF. Similarly, there’s only the Nikon 1 system available in the 1″ format.\nCrop Factors Crop factor is the ratio of the field of view between FX and the sensor in question. One of the things it tells is the focal length required on a 35mm sensor for a given focal length on the sensor in question. E.g. the crop factor of APS-C sensors is usually 1.5, which means that a 100mm lens on APS-C would give the same kind of frame as a 150 mm lens on FX.\nCrop factor also determines the increase in Depth of Field (DoF) for a given aperture value between FX and the sensor in question. E.g. a 100mm lens at f/4 on APS-C would give the same frame and DoF as a 150mm lens at f/6 on an FX sensor. One thing the crop factor doesn’t change, though, is the metering. A scene that requires 1/100s shutter speed at f/5.6, ISO200 on FX would still require 1/100s at f/5.6, ISO 200 on a CX sensor. I.e., a smaller sensor allows you to shoot at wider apertures and lower ISO or higher shutter speeds for the same DoF.\nLastly, the square of the crop factor indicates how much larger the FX sensor is compared to the sensor in question in terms of surface area. E.g. FX is 1.52 = 2.25 times larger than APS-C sensors.\nSo, let’s take a look at the crop factors across the systems and see how they compare. Here’s a chart:\nAs you can see, the biggest gap is between FX and APS-C. The smallest is between APS-C and MFT. All of them are fairly close to 1 stop (1.414) so you can assume that you change a stop’s worth of equivalent aperture while going across the spectrum.\nThese crop factors have interesting consequences. They mean, for example, that a 50mm f/1.8 “normal prime” would behave like a 135mm f/4.9 telephoto lens on a CX camera, except for exposure metrics.\nTonality Tonality, or colour depth, is the ability of a sensor to distinguish very similar looking colours from one another. This metric is what gives photos from larger sensor cameras that “3D look” while the pictures form cellphones and small cameras look very “flat”. I have pulled the best in class tonality data from DxOMark scores for each format. The chart below shows how they compare.\nThis chart shows an almost linear progression across formats, except that the best CX format sensor (Sony CyberShot RX100) is nearly as good as the best MFT sensor. The good news here is that if you’re looking for colour depth in your photos, you are much better off in 2013 than just 5 years ago. The colour depth of the RX100 (22.6 bits) is about the same as that of the Nikon D90 (22.7 bits), despite the latter having more than 3 times the sensor area.\nThe not-so-good news, of course, is that an FX sensor is still quite far ahead in terms of tonality, and this becomes especially important if you are a landscape photographer shooting in snow or haze. Portrait photographers would also get much better skin tone gradation with FX sensors.\nDynamic Range Dynamic Range (DR) is a measure of the maximum variation of light and dark areas in a scene before the sensor loses the ability to resolve colour or tone, rendering the dark areas as greyish black and bright areas as white. DR is especially important for shooting outdoors in daylight or indoors with overhead lighting or spotlights. People who shoot Christian weddings would also want to be careful about the DR of their sensor since they often need to shoot the groom wearing a black suit along with the bride wearing a white dress.\nThe DR variance, again based on the segment topping sensor scores on DxOMark across formats is shown below.\nThis is an interesting chart because the variance in DR doesn’t have any correspondence with the crop factors. The best FX and APS-C sensors are quite close to each other, but there is a huge gap from APS-C to MFT. In fact the RX100 outperforms its MFT siblings in DR despite a sensor that’s almost half as large._ _\nNoise Now we come to the most important metric for a sensor, i.e. noise performance. We all recognise noise as the speckles and freckles that we see all over a photograph. That is the most visible effect of noise, but it’s not the only one. Noise also reduces a sensor’s tonality and DR quite a bit. What’s even worse, a noisier sensor would also perform worse at its base ISO for long exposures compared to a less noisy sensor.\nSo what does the Noise progression look like across formats? Well, here it is. No surprises.\nThe chart above is based on DxOMark‘s listing of ISO measurement at which the degradation in image quality becomes perceptible. I have converted those numbers into “stops” for a more easy comparison. E.g. according to the chart above, the best CX sensor (Nikon 1 V2, in this case) is 3 stops noisier than the best FX sensor (Nikon D3s).\nSensor Performance Conclusions Based on the above charts, we see that there’s a significant gap between every segment, except MFT and CX, wherein the RX100 is acting as a bit of a leveler, albeit with 1.14 stops poorer noise performance.\nIf you are an outdoors shooter, it’s probably not a bad idea to downsize since the key metrics for outdoor shooting (Tonality and DR) are far more level across the formats, while the kit size reduction is significant. If you shoot indoors without studio lighting, though, you should strongly consider having a large sensor system.\nAnother important thing to remember, though, is that today’s sensors in any segment match up to or outperform the sensors of the next segment from 5 years ago across all metrics. If you are satisfied with your 5 year old equipment’s IQ, you are ready to downsize without consequence.\n","permalink":"https://tahirhashmi.com/posts/2013/02/09/sensor-size-vs-performance/","tags":["noise","performance","Sensor","size"],"title":"Impact of Sensor Size on Performance — a Contemporary Survey"},{"categories":["Programming"],"contents":"Recently, I sat down to refactor a Go application with a high-level design objective in place. The application had two conceptually separate entities implemented in different files but mashed into a single package. I needed to separate them out into their own packages. I wasn’t using an IDE — just Emacs with basic formatting and non-contextual auto-complete aids.\nI started out by creating a new directory for the package to be split out and moved the files that contained most of the relevant code into that directory, without thinking of the consequences. I could just invoke the compiler and let it guide me through the process of fitting the pieces of the puzzle together. One of the nice features of modern compilers is that they don’t continue dumping out errors beyond a limit. This allows fixing a program in small steps, going by the changes in errors produced by the compilers.\nThe first thing the compiler told me about was all the variables that got hidden due to things being moved into a new package. While working that out, it also helped me discover that the interface being used by Entity A to access Entity B (one that moved to the new package) only had private methods. Whoa! This is a semantic issue, which automatic refactoring tools that help with moving code around or creating new classes etc., can’t deal with.\nNext, I tried to access the invisible variables by importing the new package, but the compiler complained, “ import cycle not allowed“. Nice. I could work out the dependency tree and information passing after having separated the packages, instead of first figuring out the dependencies and then moving code. See how the compiler is guiding me toward better design as well?\nAt this point, some of you might think this is a daft way of going about refactoring and I should have worked out the entire design on paper before touching the code. But is it daft, really? Here’s what refactoring guided by a “good” compiler is allowing me to do:\nIt lets me access code as I’m working out a design. Any design or refactoring done without referring to code is prone to be erroneous.\nIt ensures that I don’t miss out on a code path. The compiler checks more code paths than I can bother to follow in my mind and it reveals problems with those paths.\nIt allows me to do top-down refactoring. I make the big, disruptive change that satisfies my design objective first, and the compiler guides me through the details of making that change work.\nIt allows me to evaluate the impact of different design decisions on code immediately, instead of having to guess.\nThe best thing, though, is that I reduce the mental context to be carried. This is a significant benefit since it allows me to refactor in smaller sessions. I can commit partially fixed code and carry on from where I left the last time. I don’t need to remember what needed to be done because the compiler reminds me about it.\nI followed roughly the same refactoring practices while coding with interpreted languages, but having to execute the program to find errors added a level of complexity, not to mention more of print/step-through debugging. It also left more for manual code inspection to ensure all corners were covered since having 100% code coverage in tests is not always feasible.\nUsing a compiler that does static analysis just improves the whole process considerably. Using a compiler along with a high productivity language like Go makes it fun as well!\n","permalink":"https://tahirhashmi.com/posts/2013/01/23/the-compiler-as-a-refactoring-aid/","tags":["Go","Golang","refactoring"],"title":"The Compiler as a Refactoring Aid"},{"categories":["Programming"],"contents":"Last week, I conducted a 2 day Go Workshop at my workplace. It was fun.\nI started day 1 with the excellent Go at Google presentation by Rob Pike, followed by my own presentation of Go’s key features. The rest of Day 1 was spent taking the Go Tour.\nThe coolest thing I did was on day 2. I mirrored my laptop on the projector and went through a fresh install of Go from source on my [newly allocated] dev box. Everyone else had identical dev environments so they could follow what I was doing on-screen. At the end of it everyone had a functional Go workspace!\nWhat’s even better, I showed them how super easy it is to create packages and use them in a program. From conducting the installation to demonstrating package creation, it only took 1 hour. This included 10 minutes fiddling with Vim settings for syntax highlighting — the most difficult part of the entire session 😉\nPresenting slide decks is one thing. It’s a totally different experience when you do a live demo with no prior preparation except the confidence that what you’re about to demo will work without much fuss.\n","permalink":"https://tahirhashmi.com/posts/2013/01/21/go-workshop/","tags":["Go","Golang","Workshop"],"title":"Go Workshop"},{"categories":["Programming"],"contents":"Go is a fresh new programming language, that has come out of Google and is primarily targeted towards server development. It is developed by some very accomplished computer scientists, like Ken Thompson and Rob Pike. I recently launched a significant new product built with Go at work, and it has proved itself out very well in terms of developer productivity and performance. So much so that many other teams are also giving it a go (oh, how punny this language’s name is).\nThink about it. How often do you come across a programming innovation that improves productivity and computational performance?\nThe Path to Go Before I discuss Go, I would like to talk about how I was led to it. I started working in 2001 and handled multiple languages for the first couple of years while working on short web application development projects. My absolute favourite at that time was C++. In 2004, I started writing lots of code in PHP, non-Object Oriented. Though initially I hated it, especially for the fact that I couldn’t catch any errors without running the program first, I learned two important programming lessons for life:\n“Object Oriented” is just one way of programming, not the one way of programming. It’s not even the best, most of the time.\nScalars (integers, characters, strings, floats, booleans), sequences (lists/arrays) and maps (key-value pairs) as built-ins are enough to support almost all data models.\nWhile PHP was great for writing small scale applications quickly, it was hard to test, hard to debug (too much implicit type conversion) and SLOW. When I thought of taking a break from PHP, I jumped into Java in 2007. Java is perhaps the most depressing language I’ve ever programmed in. Too restrictive, too inexpressive (in other words verbose) and too “heavy”, with all the IDEs, frameworks, build environments etc. that needed to be set up before getting started. I also did not accept the JVM ideology. It’s too high a price for phantom portability.\nIn 2008, I started with Python. I had first played with it about five years earlier but this is when I got on to it full time. I loved Python. It was multi-paradigm, it was very expressive with brilliant stuff like list comprehensions, generators, decorators, **kwargs, etc. Its biggest strength, though, was that all this expressiveness was so cohesively designed into the language that the WriteCode-Run-FixCode-Run… cycle frequently had 0 iterations. New code that I wrote worked as intended more often than in any other language.\nThere was, however, a problem with Python. It was SLOW. I tried optimisers like Psyco, which transformed into Pypy but didn’t deliver too well. Unladen Swallow came and withered.\nWhen I started programming in Python, it appeared that Google had a lot of interest in the language. It’s creator, Guido van Rossum, was on their payroll. As days passed and progress on Unladen Swallow slowed, we heard of a new language from Google — Go. It would have been a bit easier to dismiss Go earlier on as an academic product, had it not been for the reputation of the people behind it, Rob and Ken, and the fact that it came from one of the most hard-core commercial software companies on earth. Back in 2009 when Go was announced, I did not understand much about why it was great, except that it had a lot of modern programming constructs and that it was fast.\nLet’s hold this thought and go on a tangent for now.\nIn late 2010, when I moved to Zynga and got back from Python to PHP, I had the time to read up on some stuff. One of the books I read was this book, which offers a kind of gestational walk-through of programming language features using an academic programming language called Oz. Somewhere in its discussion, the book introduces a concept of “dataflow programming” and “declarative concurrency”. Now, you might want to follow the preceding link and briefly acquaint yourself with this concept because it is the centre-piece of Go’s language features.\nWhen I learned about dataflow programming, I recalled how big a deal it was at Yahoo to be able to fetch data from multiple data sources in parallel and have resolution policies like fastest-source-wins, or wait-until-N-of-M-responses, etc., each with its own timeout handling and so on. The standard solution was a Java daemon with XML documents to describe the intended dataflow. It wasn’t pretty. I wished for this capability to exist in a language that, unlike Oz, wasn’t academic.\nI did not realise at that time that Go was what I was looking for.\nSome time early last year we started discussing about trying out other programming languages — compiled languages, to be more specific — to get better-than-PHP levels of performance. That’s when I again started exploring Go and discovered that it had gathered a lot more backing at Google and gone GA, version 1.0. In the middle of last year, we started bouncing ideas about a new project which required handling concurrency. The PHP shared-nothing architecture wasn’t going to work. I thought it was the right time to take Go for a spin.\nWorking with Go I got a real feel for programming with Go while working on the project’s prototype. Here’s what I observed:\nCompiler assistance: It felt really good to get back to a compiled language. The compiler not only checks for code that can theoretically result in a runnable program, but also disallows things that can lead to problems later on. This isn’t a unique property of Go, but it’s there and that’s good.\nStandardized formatting: A standard tool to enforce formatting rules that is not subject to change based on the team members’ or leader’s opinions is a welcome feature for lowering the “not my code” mental barrier.\nSmall language: Go is a very compact language. It’s not bursting at the seams with features. It’s a lot like C in that regard, minus the “undefined behaviour” proliferation. That makes it easy to learn and start programming with.\nBatteries included: The Go standard library has all the bits and pieces needed to write a server program, whether it be a webserver, raw socket server or anything else that requires access to OS built-ins.\nEasy concurrency: Go channels and goroutines work very nicely to handle concurrency. The best bit is that unlike Node.js, Twisted, etc., you don’t have to write your entire application as event handlers, which makes things easier. Unless you’re a hardcore JavaScript programmer.\nEasy to jump in: Time to get started with Go is pretty short. Much less than languages that require elaborate frameworks and IDEs to get set up.\nFast time to “done”: Productivity with Go is really high. There is so much of common server programming components built into the language or its standard library that you can write fairly complex servers in very little time. Less code also means faster time to correct program behaviour.\nMulti-core, High performance: The runtime is surprisingly fast and stable. It feels spectacular, for someone used to writing servers in Python, etc. Unlike Python and Node.js, the Go runtime is capable of spawning multiple OS threads and having them execute in parallel.\nGo in Production Some of the features of Go don’t seem to be significant until you deploy it to production. The things that I really love about Go in this regard are:\nSingle Binary: Go produces statically linked binaries that are self sufficient and would execute on any system that they’re built for. This is especially great for cloud based deployments because your deployment node does not need anything beyond a bare-bones OS image. Compare this to the elaborate dependency setup that is required for most other application environments and the benefits become obvious.\nCross Compilation: Earlier I mentioned about the JVM being a bad trade-off for phantom portability. In practice, non-trivial Java programs also require porting for different systems. Coupled with dependency management on target systems, portability of Java programs becomes tougher. The nice thing about Go is that I can have my entire build environment for development and testing on my Ubuntu or OS X system and compile for RHEL in production. All I need to do is copy over the cross-compiled binary and I’m done with portability!\nBuilt-in Profiling: With just a few lines of code, you can have an HTTP end-point that will give you live profiling capability for CPU, memory, goroutines, OS threads, lock contention etc. You can use the go toolchain to hit that URL and profile the application on-demand while it’s executing and handling live traffic. That’s just brilliant!\nShareable Code: One of the biggest impediments to code sharing is the size of a project, or the amount of code it has. Even if people have access to some codebase, it’s not always easy for them to dive in and understand the code. Since it takes far fewer lines of code to do the same thing in Go compared to C, C++ or Java, the code is more accessible to others and therefore more shareable. The limited language spec also means that it’s less likely that someone might run into obscure constructs or “advanced features” that some people know of and most don’t.\nConclusion To conclude, I have found Go to be the best server programming language I have tried so far. It combines the elegance of Python with the performance of C and C++. Even if it is only 90% as easy as Python and 90% as fast as C or C++, it still works out to be a winning combination. Its benefits are not just restricted to productivity and performance, though. Go applications are easier to develop and deploy across heterogenous systems and virtualised environments. It’s also easier to share and collaborate on Go programs, thanks to a single formatting standard, small language spec and concise code.\n","permalink":"https://tahirhashmi.com/posts/2013/01/05/why-program-in-go/","tags":["Go","Golang","programming"],"title":"Why I Program in Go"},{"categories":["Photography"],"contents":"In Part 1 of this two-part series we looked at some of the dominant technology trends over the last couple of years and the impact they are having on cameras and photography. In this part of the series, I write about my impressions of a few recent cameras that I have had the opportunity to use.\nSony CyberShot DSC-RX100 Voted by Time Magazine as one of the best inventions of 2012 and declared to be the best pocket camera ever made by New York Times reviewer David Pogue, this camera has changed the way people respond to small sensor sizes.\nRX100 is a fixed zoom pocket camera with features and controls designed for enthusiast photographers. At 36mm thickness, it is just about small enough to fit into the front pocket of a pair of jeans without discomfort. Its claim to fame, though, comes from the fact that despite such a diminutive size, the camera’s 1 inch (13.2mm x 8.8mm) sensor delivers dynamic range and colour depth that matches that of the much bigger APS-C sensors, e.g. that of the Nikon D90. This excellent 20MP sensor is mated to an f/1.8-4.9 Carl Zeiss 3.6x zoom lens that is sharp enough to justify the super-high resolution.\nI acquired this camera in September 2012 and since then, I have hardly used my DSLR (Nikon D90) and I sold off my Sony NEX-5 as well. Other than the 100mm+ telephoto range, I do not miss shooting with the DSLR. RX100, in fact, has a few smarts that make it nicer to shoot with than the D90:\nFast AF, accurate metering and white-balance Astounding detail in the 20MP files captured with this camera Horizon Indicator, to show camera tilt across two axes High resolution, high brightness WhiteMagic™ LCD that makes daytime shooting a pleasant experience Front and rear rotary dials offer direct control of shooting parameters 7-option customisable “Fn” button for quick access to shooting options 3 memory presets for shooting parameters that one can customise for specific conditions Tiltable on-board flash for bounce-flash capability Another thing that RX100 does much better than the D90 and its bigger cousin NEX-5 is video recording. The video stabilisation on RX100 is amazing and the results from 50 fps 1080i video are striking.\nThere isn’t much that this camera leaves to complain about. If at all, I sometimes find the minimum focus distance at tele end to be a bit too much.\nOlympus OM-D E-M5 Voted by DPReview readers as the best camera of 2012, the Olympus OM-D E-M5 is indeed a remarkable camera. This camera is remarkable for 3 reasons:\nExquisite styling based on the Olympus OM series film cameras, with solid construction and weather sealing Excellent new technology incorporated into the camera Large selection of small, high quality micro Four-Thirds (MFT) lenses Indeed, the styling of the camera is admirable. Its modern machine tooled cuts combined with classic styling cues give it an intriguing and likable shape from the front and top. The rear side of the camera is dominated by the large LCD swivel touch-screen that makes it look and feel like a modern camera. The looks of the camera are enough to raise expectations from its performance, handling and IQ.\nHandling of this camera is quite good. Apart from the awkwardly placed on-off switch, everything is just about where it should be. The tilting LCD screen offers a better shooting experience than the built-in 1.4M dot EVF, though the latter may be used for extra stability. While using the LCD, you can also use the touch-to-shoot functionality to focus on a desired point in the scene and shoot.\nThe E-M5 features hybrid AF (on-sensor phase detect + contrast detect) that is very fast to lock on and quite accurate. The 16MP imager has great colour depth and high ISO performance. The JPEG engine from Olympus also retains its characteristic colour response to give pleasing images straight out of camera. It did have some issues with auto white-balance under fluorescent lighting.\nAnother first in the E-M5 is the 5-axis in-body image stabilisation. While most IS systems only compensate for yaw and pitch, the E-M5 compensates for yaw, pitch, roll and horizontal and sideways translation movements. This works very well for shooting stills, and also for shooting video. However, the RX100 still outclasses the E-M5 in video shooting for one basic reason — continuous tracking AF in E-M5 is very, very slow, at least with indoor lighting.\nOverall, this is a very attractive camera and just by its looks and handling, it inspires one to shoot. However, it’s still a bit rough at the edges and things like iffy AWB and slow focus tracking can be a bit disappointing. Nevertheless, this is the first camera from the FourThirds stable since the Panasonic GF1 that has held my attention and is having me think of switching to the MFT stable instead of Nikon APS-C.\nNikon 1 J1 The Nikon 1 J1 is a strange camera in many ways. In photographs, it appears to be all cutesy and properly small, in line with other compacts. When you see it in real life (I was going to say, “flesh and blood”, but stopped for obvious reasons), it turns out to be rather big and very solidly built.\nThe most striking thing about the J1 is its build quality. It is crafted out of seemingly thick metal, with large, nicely tooled buttons on top. The back-plate has some more large and well constructed buttons, along with the LCD screen. It makes the very solidly built Sony RX100 feel like a flimsy little thing.\nSo, the J1 is uncharacteristically well built. What about the performance? It has the same sized sensor as RX100 but whereas RX100 only does contrast detect AF, the J1 has hybrid (phase detect + contrast detect) AF like the OM-D E-M5. This does result in super-fast focusing with a high level of accuracy. Raw IQ is excellent and matches that of the RX100 where low light is concerned. At base ISO, RX100 has higher colour depth and dynamic range, though. The camera is also capable of up to 60 fps frame burst, which enables it to offer interesting features like Motion Snapshot and Smart Photo Selector. That’s not all, the camera can also shoot 400 fps videos at 240p and 1200 fps videos at 120p. For all this performance, though, it doesn’t offer exposure/WB bracketing or in-camera HDR, which brings us to the next point.\nThe J1, unfortunately, is a bit of a strange camera for another reason. It’s designed for P\u0026amp;S upgraders and novices, but requires an enthusiast — a tinkerer — to get the most out of it. The metering and AWB of this camera are both unremarkable, making it necessary to shoot raw and use EV compensation more often. That, however, is a problem because the camera being designed for novices, doesn’t offer an up-front control to set EV compensation. The high speed videos look like an attractive prospect, until you discover the amount of ambient light the camera needs to be able to capture them.\nAnother trade-off between RX100 and J1 is that of size vs. versatility. The J1 is big, compared to RX100 (though smaller than Olympus E-PM2) and its lenses are bigger still. But for the added bulk, you do get the ability to choose between not just the interesting Nikon CX lenses like the 18.5mm f/1.8 or the announced 32mm f/1.2, but with the FT1 adapter, you can also get your Nikon DX and FX lenses to work with it.\nOverall, it’s a very interesting camera and for its now discounted price, it offers a great value proposition. Indeed, it’s the best selling mirrorless ILC in Japan by a healthy margin.\nBuying Recommendations If you are looking for a pocket camera and are willing to spend the money, I can recommend the RX100 without reservations. It does what a pocket camera should do, and then some more, with uncompromising IQ on-the-go.\nFor a mirrorless ILC system, though, the choice is still a bit confusing, especially if you consider cameras I haven’t covered yet. There are 3 broad choices based on sensor size:\nAPS-C: This includes Sony NEX, Fuji X and Samsung NX bodies. With a big sensor comes the burden of big lenses. None of the manufacturers in this segment have managed to address lens size issues satisfactorily, though Sony made some progress with its 35mm f/1.8 prime and 16-50mm collapsible zoom. If you’re going mirrorless to reduce size, you might want to consider the entry level DSLR bodies, which aren’t significantly larger but offer better value for money.\n**MFT: **Micro-Four-Thirds platform has plenty of choice among lenses and the MFT champions, Olympus and Panasonic, have managed to make some really small, really good lenses for this system. Unfortunately, these manufacturers lagged behind in the sensor department. All that looks set to change going by what we see in the E-M5 and E-PL5, and given the sensor supply deal that Sony has struck with Olympus. I would myself have considered this system, but…\nNikon CX: being a Nikon guy, I am watching this mount with interest. The RX100 has shown the potential of a sensor of this size, and I can happily live with the IQ it returns. The CX lens selection is sparse at the moment, but at least they are sufficiently small, even with VR. The AF and continuous shooting performance of Nikon 1 bodies is beyond anything in its price bracket, or even higher. I would have gone for the Nikon 1 V2, but the lack of exposure bracketing and/or multi-shot exposure modes a la Sony’s Superior Auto is a deal-breaker for me. Let’s hope Nikon addresses this concern soon enough.\n","permalink":"https://tahirhashmi.com/posts/2013/01/05/cameras-2012-2/","tags":["Compacts","evil","mirrorless"],"title":"State of the Camera 2012: Part 2"},{"categories":["Photography"],"contents":"It’s been exactly two years since I last wrote about a camera on this blog. A lot has changed in the camera scene over the last two years and I fell in and out of love with the NEX-5 during this time as well. In this post I shall begin with talking about the recent trends in the camera market followed by short reviews of a few new generation cameras that I have tried or bought.\nMirrorless/EVIL Systems The hot new stuff these days undoubtedly is the rise of “mirrorless” or “EVIL” (Electronic Viewfinder Interchangeable Lens) cameras. “Mirrorless” is a broader term that just indicates the absence of a mirror that is traditionally used to operate the Auto-focus system in SLR cameras. Mirrorless cameras use their sensors directly to auto-focus either through contrast detection (CDAF) or phase detection AF sensors embedded directly on the main sensor. Mirrorless cameras may not have a viewfinder or allow changing of lenses. EVIL cameras include both.\nBy discarding the mirror, the bodies of these cameras can be considerably thin. By dropping out the optical viewfinder, these bodies also save space otherwise occupied by the pentaprism in SLRs. This opportunity in body size reduction for the same sensor size is the main selling point of the mirrorless cameras. They allow you to carry cameras capable of seriously good IQ in a smaller package.\nSmall Sensors As the Sony NEX system demonstrates, while it’s possible to reduce the body size considerably by going the mirrorless route, the size of the lenses does not reduce since it’s largely related to sensor size. To reduce the overall size of the package, the sensor size must also be reduced.\nTraditionally, small sensors have had a big gap up to the IQ delivered by bigger ones (APS-C/APS). However, smaller sensors have also now reached the IQ sweet spot where minor improvements in IQ are harder to perceive and more expensive to deliver. A striking example of this is the 1 inch sensor inside Sony Cybershot DSC-RX100. As per DXOMark, this sensor is better than that of the Nikon D80, despite being 3.24x smaller. It even matches up to the D90 in all except low light performance!\nThis isn’t to mean that the large sensors haven’t improved in the mean time. However, the IQ delivered by small sensor systems is now enough to make fairly large (8×12″ or 12×18″) prints without noticing defects. That really is more than most non-professional photographers would want out of their camera.\nSummary To summarize, digital cameras are now in a phase of miniaturisation as they have reached and exceeded the levels of “acceptable” performance. Miniaturisation is enabled mainly by getting rid of the mirror and pentaprism assembly from traditional SLRs and having their functions performed by the camera electronics (CDAF, Electronic Viewfinder). Systems with smaller than APS-C sensors also benefit from a significant reduction in the size and cost of lenses. The resulting cameras are not only physically smaller, but they are also cheaper than their large-sensor counterparts when you include the cost of lenses.\nIn the next part of this post, I shall write about a few recent cameras that are at the forefront of this miniaturisation and see how they compare to DSLRs.\n","permalink":"https://tahirhashmi.com/posts/2012/12/26/cameras-2012-1/","tags":["Compacts","evil","mirrorless"],"title":"State of the Camera 2012: Part 1"},{"categories":["Opinion"],"contents":"The camera industry is going through some pretty big movements these days. Since the advent of digital cameras and social networking websites, more and more people have been finding interest in taking photos and exhibiting them online to friends and public at large. Somewhere in this explosive growth of photography, veterans and early enthusiasts sometimes find themselves in a tight spot.\nBig Camera, Small Camera Since their inception, digital camera sensors have been pushing the boundaries of the quality of images they produce. For most of the last decade, it was a case of bigger is better. For better image quality (detail, focus accuracy, lack of noise), you needed to go for the largest digital sensor you could afford. That led to a lot of people lusting after Digital SLR cameras, with sensors having 10-20x more surface area than those of smaller point-and-shoot types.\nEarly this decade, though, we started seeing a proliferation of back-lit CMOS sensors, which significantly improved the light gathering efficiency of small sensors. More importantly, the improvement did not scale with sensor size, leading to a reduction in the image quality gap between small and large sensors. More importantly, they crossed some sort of an acceptability threshold, above which, quality improvements did not have as profound an impact. When these sensors got mated to always-connected smartphone devices, the ability to take good photos and immediately broadcast them to friends via social networks turned into a compelling channel for creation and consumption of photographs.\nDSLR Owners’ Conundrum A lot of people who acquired DSLRs in the middle of last decade now find themselves in a tricky situation. Using a DSLR means that you need to know a lot about how cameras work, in order to get the best out of the equipment. You need to know, for example, what ISO to shoot at, because unlike older compacts that produced noisy results at any sensitivity, a DSLR can give you very clean images near base ISO. You need to know about lenses and aperture and focal length, because the DSLRs force you to choose your lens carefully. Compact camera lenses get everything in focus most of the time, but DSLR lenses allow selective focus, which is one of the secrets behind their captivating results. Lenses are no longer just one number — 12x zoom and such. Most good-quality lenses are either primes (1x zoom) or 3-5x zooms at most. The zoom factor itself has no bearing with how magnified the image appears — there’s no comparison between the field of view of an 18-55mm lens and a 70-200mm lens, even though both are roughly 3x zooms.\nAnyhow, we’re digressing. The point is, if you wanted to take not-crappy pictures in 200x, you had to buy a DSLR and to use it effectively you needed to know a lot about how cameras and lenses work. Unfortunately, the then top-of-the-heap IQ has now pretty much been surpassed by compact cameras and smartphone cameras in terms of resolution and low light sensitivity. The only thing holding smart-phones back is that they don’t have optical zoom and have to rely on “digital zoom” (crop \u0026amp; resize) some times.\nA lot of DSLR owners from the last decade or older now feel threatened by the improving quality of compact shooters. Add to that the complex retouching and post-processing that is now available as cookie-cutter presets in applications like Snapseed, Instagram, etc., for which people spent hours in front of a computer fiddling with layer masks and such, and the feeling of threat is compounded. There’s a feeling of loss when you see someone producing as good results as you do, despite your skill and experience, because your skills have been codified in the camera’s firmware. Is the threat perception justified, though?\nSay, “Hi!” to Lo-Fi I, for one, really like what’s happening with the small camera industry right now. Comparing pictures from 5-10 years ago to what we have now, I see a significant improvement in the quality of pictures that people are putting up. The cameras’ scene detection and exposure metering has become smarter, they have better image quality baselines, the low MP counts allow these devices to capture shots in a burst and perform operations like noise reduction, DR enhancement, panorama creation, etc. on board. The artistic presets for lo-fi imagery help make the otherwise boring shots more catchy. Since most of these shots are consumed on the web, their IQ deficiency doesn’t even stand out much, compared to shots from a DSLR.\nAs an enthusiast photographer, though, you must not forget the reason why you stuck your neck out and put in your hard-earned money for a DSLR — to produce better looking photos than random Joes’. It’s just that having high resolution, noise-free, colour-corrected pictures is not enough differentiation now. You need to look for other differentiating factors. Some of it could be internal differentiators — improving your procedures to allow creation of images that typically need more expensive gear. Some of it could be looking for new frontiers to break with photography that smartphones and compacts still haven’t broken — they can’t do HD video in low light, they still shoot JPEGs, they can’t do 1:1 macro photography, they’re slow to focus. Some of it could be doing a one-up on them at their own tricks. For example, there is tremendous opportunity for being creative with multiple shot photographs that are merged in creative, non-cookie-cutter ways.\nAll said and done, it doesn’t matter which camera you use, as long as you find interesting stuff to photograph in interesting ways. DSLRs, compacts, lo-fi, hi-fi, are just means to producing something interesting.\n","permalink":"https://tahirhashmi.com/posts/2012/02/15/high-on-lo-fi/","tags":["cameraphones","lo-fi"],"title":"High on Lo-Fi"},{"categories":["Programming"],"contents":"While working on a huge code-base with several thousand source files, it becomes difficult to remember where each file is. If you use conventions like one file per class, you can at least figure out the file name. E.g. the definition of class Foo would be found in Foo.class.xx or something like that.\nThankfully, ack makes it easy to find the location files in a project. Just say, ack -g Foo.class and voila! it tells you to dig in modules/frob/model/include/Foo.class.xx or whatever abominable directory hierarchy it may be embedded in. So when you decide to kill that error at line 324 of class Foo, all you need to do is:\nvim `ack -g Foo.class` +324\nUnfortunately, typing this out is a bit tedious as well, so we bring in a little shell script (I call it vis, for vi m on s earched filename):\n#!/bin/bash # Search and edit files. Use it like: # $ vis Foo.bar [...] if [ $# -lt 1 ]; then exit 255; fi fname=$1; shift; vim `ack -g \\\\\\b${fname}[^\\/]*\\$ | head -1` $@ This will invoke vim with the first filename that ack returns for the filename pattern (yes, it can be a Perl regex), and you can pass all other vim arguments as usual after the file name. Nifty, eh? The regex above will match file names starting with the given search pattern (with some caveats).\nPS: It’s not too hard to make the script choose the editor dynamically, but people usually love to use one editor consistently. I use both Emacs and Vim, so I’m planning to make this script take the editor as an argument and have two convenient aliases, one each for vim and emacsclient which will handle passing the editor as an argument.\n","permalink":"https://tahirhashmi.com/posts/2011/11/24/find-and-edit-file/","tags":["ack","bash","scripting","tools"],"title":"Find and Edit File"},{"categories":["Technology"],"contents":"Are you based out of India and need an idea for a technology start-up? Try thinking of two of the biggest technology buzzwords of the day and mash ’em up together. Social + Cloud!\nBack in the ol’ days when system programming and desktop applications were hot, people were trying to create applications that could run on more than one OS. There was a sprouting of libraries, proprietary and open source, that promised abstraction from the operating system internals. Right now, if you think of one thing where the a lot of modern applications run, the answer would be — a social network. Facebook is obviously the dominant player here but by 2015, it will have at least one formidable competitor. Developers wouldn’t want their app to be stuck on one of these.\nThat’s where developing to a Social Network Abstraction Library (SNAiL?) would help. Especially if you’re a vendor of multiple applications. The problem is that by itself, SNAiL is only good for vendors of multiple applications. It doesn’t do a whole lot for individual application developers. Now, creating a social network app also means that it’s a multi-user, network-based application. That’s where the “Cloud” part of the equation comes. If SNAiL is available as a service like Google App Engine, wherein provisioning, elasticity and monitoring are no longer the developer’s headache, we have a win. Should there be a choice of cloud provider? For an application developer, one cloud is as good as another from a functional point of view. The main issues are with the provisioning and monitoring of those clouds, availability SLAs and pricing. If the choice is given as a SLA-pricing trade-off without naming names, it’s possible to sell options for multiple cloud providers. Cloud abstraction and Social Network abstraction together might make it a lot more appealing for smaller development houses.\nOne of the reasons why I mentioned Operating Systems and OS abstraction libraries is also to look from the successes and failures from that era. If you’re looking to build a multi-OS application these days, you’d surely evaluate Java, even if you give up on it later. Why is Java so ubiquitous as a choice for platform independence? Mainly because it went all-in when it came to providing an OS independent solution. It wasn’t just a set of APIs that abstracted other APIs. It was a new language, a new virtual OS (I don’t see much “machine” in JVM. Looks more like an OS layer to me), a complete set of development libraries that were uniformly available across OSes, and even a new way of packaging, distribution and deployment of apps.\nSimilarly, our Social-Cloud App mashup needs to go all-in for simplifying and standardising the whole development and distribution process for the applications its meant to run. That’s where I’m unable to answer one question: should it provide the ability to create apps for the browser and for the leading mobile/tablet OSes?\n","permalink":"https://tahirhashmi.com/posts/2011/09/19/new-start-up-idea/","tags":["cloud","computing","development","idea","social","solutions","startup"],"title":"Need a new Start-up Idea? Mash up Social and Cloud"},{"categories":["Photography","Reviews"],"contents":"It’s been a week since I’ve had my baby camera, i.e. the Sony α NEX-5 and I’ve been enjoying it so far. Here’s a quick low down of the ups and downs I’ve encountered with the camera.\nThe Good Outstanding IQ in any shooting conditions that can only be trumped by careful, deliberate shooting with a DSLR equipped with high quality optics Small enough to fit in my laptop bag, if not the jeans pocket Excellent iAuto mode for quick snapshots Fairly easy to use controls, esp. with button customisation offered by Firmware v03 Auto HDR and Hand-held Twilight Modes really work! Sweep Panorama works really well for hand-held shooting. You can’t do better with a DSLR until you mount it on a tripod. Focuses almost as fast as an AF-D type Nikkor with D80/D90 body Optical Steady Shot (OSS) on the 18-55mm f/3.5-5.6 Kit Lens is outstanding Movie making is super easy and fun! Top Left: ISO 12800 shot. Top Right: “Hand-held Twilight” shot @ ISO 6400. Bottom: Sweep Panorama\nThe Bad JPEGs tend to be a little less sharp by default\nChromatic Aberration is visible in high-contrast scenes, though not unusually excessive and removed in one click with Nikon ViewNX\nZooming while composing is cumbersome due to missing support from the forehead as with an OVF equipped DSLR\nCumbersome MF adjustment despite on-screen MF assist. The electronic focus ring twists indefinitely so you’re not really sure if it’s locked at minimum/infinity focus, when you need it\nThe Ugly Excessive battery drain if you take time while composing a shot due to OSS, continuous AF and LCD\nFingerprint -magnet of an LCD and screen protector not provided as part of the kit\nI’ll probably add more to this post as I get more familiar with the camera, especially when I start shooting RAW (+JPEG, of course).\n","permalink":"https://tahirhashmi.com/posts/2010/12/26/nex-5-short-review/","tags":["alpha","e-mount","nex-5","sony"],"title":"NEX-5 Early Impressions"},{"categories":["Photography","Opinion","Photography"],"contents":"As my photos get backed up and burnt on to a DVD, I thought I might just do a quick cataloguing of the software I’ve found useful for developing my photographs and what each does. I’d also outline my workflow as I present each software in the order in which it appears in the workflow. Just to set the context, I use Microsoft Windows 7 for my imaging tasks, and my camera is a Nikon D90, so I do use a lot of Nikon software.\nFile Management and Archival 1. Nikon Transfer (Freeware) This is the program that detects photos on external media and transfers them to a local disk or other external media. I’ve tried connecting the camera through its own USB interface and through the built-in card reader on my computer, but I find that the San Disk card reader that came with my Extreme III card gives the fastest throughput. Anyway, so Nikon Transfer is launched when the SD card is inserted and the nice thing about this software is that\nIt allows simultaneously downloading the photos to a secondary location, which in my case happens to be just another disk partition but can also be an external USB drive (I’m contemplating doing the latter, going forward) It detects which photos have already been transferred from a batch and skips those (configurable) It allows manually selecting the photos to be transferred. I don’t normally use this option but I’ve needed it once or twice so I thought it might be worth mentioning. My folder hierarchy for transferring these photos into is like this:\nCamera Model/Name \u0026gt; Year \u0026gt; MonthNumber-MonthName or EventName\nE.g. D90201009-September, or Vesper2010MulaniWedding, etc.\nThe RAW files get dropped directly into those folders and the converted JPEGs go into a subfolder. I drop the camera-wise separation for permanent archival.\n2. Nikon View NX2 (Freeware) ViewNX 2 is a recent release from Nikon and Nikon Transfer now comes bundled with it. ViewNX 2 allows rating and tagging photos, while also allowing for conversion of RAW files into JPEGs or 8/16-bit TIFFs. It has also started offering the basic photo editing options that are available in Nikon cameras’ retouch menu. I primarily use it to select the photos that I want to develop. The way it allows files to be compared at 100% zoom and the way it can filter the film strip on rating ranges makes it extremely easy to do so. Here’s the rating system that I’ve evolved for myself over the months:\n5 Stars: Awesome snap! Convert, retouch and upload to Flickr. Send for a medium/large print too. 4 Stars: Awesome snap! Convert, retouch and upload to Flickr. Send for a small/medium print too. 3 Stars: Good snap. Convert and retouch. Likely upload to Flickr. Maybe send for a small print. 2 Stars: Acceptable snap. Convert, maybe retouch. Unlikely to be uploaded to Flickr or printed. 1 Star: Not that great. Just meant for keeping. May not even be converted to JPEG. 0 Stars: Delete. Please note that while I ruthlessly delete items based on the star rating assigned as above, I do it only on the primary working copy. The backup copy is always there, just in case I need to revert on my decisions. It doesn’t happen often but on the rare occasion that it does, it’s good to know that you’ve got all the shots with you.\nWhen I’m done with all the processing, etc. for a month or few months, I copy the finished and shortlisted content to a more permanent backup solution (DVDs, in my case).\nImage Development Nikon Capture NX 2.2.4 ($180) This software is all that I use for developing my images. Why? Here are a few reasons:\nIt’s arguably the best converter for Nikon’s RAW files, giving rich tones, smooth gradations and excellent detail It recognises all the camera settings used while taking a photograph and applies them as a starting point while developing the image It has in-built lens correction features like distortion control, CA control and vignetting. Really awesome to have this if you’re shooting with Nikon’s own lenses It’s got the U-Point controls that make selective retouching really really easy, without requiring you to drop into complex layer masks, etc., which I could never get the hang of It saves all the development and retouch settings right within the NEF file. The ability to retain the exact settings used to create a JPEG is amazing! It supports multiple edit-versions of the same file. You want a full colour as well as a monochrome version of the same snap? Sure, your NEF can carry multiple sets of editing steps all in itself I’ve come to love the very functional interface. Though a lot of people say it’s not as great as Aperture or Lightroom, I disagree. I’ve tried LR3 and I didn’t like it as much as CNX2. Yeah, batch conversion is a pain so if you do a lot of batch conversions, you might have to get something else or use View NX2 if you can live with its glacial speed of conversion and limited retouching support. Specialised Development While Capture NX 2 handles development and retouching of single images, some times you need multi-image solutions for panoramas and HDRs. Here are the tools I’ve found useful for these applications:\n1. Picturenaut HDR Imaging (Freeware) Picturenaut HDR is not the best HDR creation software out there, but it gets the job done for me. On the positive side, it’s got a very simple interface and it’s very fast. On the negative side, it doesn’t do image alignment too well (despite the option being present). If you want the super-saturated, artificial-looking, sick HDR shots, just select “Bilateral” tonemapper, set saturation to the max, set contrast to the min and you’re done. I don’t like that, so my workflow is a wee bit more involved.\nMy HDR workflow is to create 16-bit TIFFs from the bracketed shots and “Neutral” Picture Control setting via View NX2. I then load the TIFFs into Picturenaut and through some juggling with “Adaptive Logarithmic” global tonemapper. Flying by the histogram (I’m now pretty good at interpreting histograms 😉), I get the HDR tonemapped as best as I can. The output is saved into another 16-bit TIFF. I load that TIFF into Capture NX2 for local enhacements and, thanks to the 16-bit depth, I get to recover amazing detail and colour from the shadows, if needed.\n2. ArcSoft Panorama Maker 5 Pro ($80) While trying to make this panorama out of 14 images, I spent hours breaking my head with Hugin. Hugin is Free Software, and it’s supposedly highly capable. I would’ve recommended it but unfortunately, it failed to work for me on MS Windows 7 (64-bit). ArcSoft’s Panorama Maker 5 Pro is a nifty little tool. All I had to do was feed it my 16-bit TIFFs and click one button to generate the panorama! Since panorama stitching doesn’t give a perfectly rectangular image, the tool offers a crop suggestion to crop out all the jagged edges and thereafter you can save your panorama.\nI saved it as a 16-bit TIFF again, and retouched it in Capture NX2 to remove some issues caused by uneven exposure in the source images. Oh yeah, remember to always lock focus, white-balance, picture control mode and exposure (aperture, ISO, shutter speed) while taking multiple shots to make a panorama. This tool also includes a little tutorial on capturing individual shots for a panorama. Thoughtful.\n3. HDR Alignment Tool v2.0 (Freeware) Since Picturenaut doesn’t do a great job of aligning bracketed shots, I found this neat tool that allows you to put 2 control points on a pair of images and then re-aligns them. If you have a bunch of images, you need to set 2 control points for each pair in the sequence (1-2, 2-3, …) and this tool will finally yield an aligned stack or re-aligned individuals. Unfortunately, it doesn’t to TIFF output.\nHonourable Mentions While the above mentioned software is what I’ve used so far, here are a few honourable mentions to conclude the post:\nFlickr Uploadr for Windows/Mac OS X: Excellent tool for managing your Flickr uploads. Never upload a photo without the right title, description or permissions! UFRaw: Before moving to Capture NX 2 and Windows, I used Ufraw for my NEF conversion. It’s an excellent tool, but the underlying dcraw converter doesn’t do a great job handling colours and noise reduction. If GNU/Linux or F/OSS is your thing, UFraw is highly recommended. The Gimp: Also for F/OSS buffs, the Gimp is a highly capable image editor. I suppose it’s indispensable for any imaging workflow on Linux that involves non-trivial retouching. ","permalink":"https://tahirhashmi.com/posts/2010/11/01/software-toolkit-workflow/","tags":["archival","cataloguing","development","hdr","imaging","panorama","software","tools","workflow"],"title":"My Imaging Tools and Workflow"},{"categories":["Photography"],"contents":"These are exciting times for DSLR enthusiasts. We’ve all marvelled at the creative and operational flexibility afforded by the large dial, switch and button infested DSLR bodies and interchangeable purpose-built lenses. We’ve been spoiled for the impeccable image quality afforded by the 8.5-15x larger APS-C sensors (upto 34x larger if you’re a 35mm shooter). It’s impossible to look back at compacts. Or is it?\nMost DSLR shooters sooner or later realise that their beloved hunk can’t be their only camera. They can’t carry it all the time to family events. They can’t do anything about it if they happened to dine in a fancy restaurant on impulse. Carrying a DSLR has to be planned ahead, owing to its bulk. In the last couple of years, though, the B-level interchangeable lens system manufacturers (anyone other than Canon and Sony) had been pushing the boundaries of how small an interchangeable-lens system could be made. While I’ve followed this category (dubbed EVIL — Electronic Viewfinder, Interchangeable Lens), it’s right about now that I have finally settled on a system. It’s going to be none but the Sony α NEX-5 for me. If you follow me on twitter, you’d already know about this. Here’s a brief overview of stuff that I considered and what sold me on NEX-5.\nLuxury Compacts These are essentially non-interchangeable zoom compacts with puny sensors. Canon G10, G11 and the latest, G12, are leaders in this category, owing to their small size and controls that are suited to DSLR users. Heck, they have an ISO selection dial! Some people also choose the smaller Canon S90/S95 systems. Their undoing, however, is their image quality. While they’re considerably good in terms of IQ, sensor technology has been favouring large and small sensors equally and large sensor systems have leaped further ahead in terms of dynamic range and low-light capabilities. Nikon’s recent offering, the P7000, is a worthy addition to this lineup. It trumps the G12 in terms of specifications though deeper IQ analysis is pending.\nCertainly, though, these systems aren’t going to offer the kind of IQ one gets from today’s DSLRs. Written off.\nm4/3 EVILs While Canon and Nikon were busy fighting it out in the 24mm and 35mm DSLR arena, a bunch of B-segment makers got together to create a new shared sensor format and lens-mount called Four-Thirds. This system boasts a 4:3 sensor (compared to 3:2 for 24 or 35mm ones) that has about 1/4 the area of a 35mm sensor. Much better than tiny compacts (4-9x larger) but still not up to the level of 24mm sensors. Early on, these systems focused more on the almost-similar IQ to larger DSLRs with a wide selection of lenses thanks to the shared lens-mount between manufacturers. Models like the Panasonic\nPanasonic Lumix GF-1 and Olympus PEN EP-1, however, put the same sensor in rangefinder/EV bodies that were significantly smaller. What’s remarkable about them is that they are about the same size as the Luxury Compacts if you put a pancake prime lens on them. Yet, they offer visibly better IQ and flexibility to change lenses. GF-1 and EP-1 were instant hits with shooters of even Nikon and Canon DSLRs as they afforded DSLR-like IQ and operations, without the bulk. Around this time, people started speculating about Nikon and Canon jumping into the Mirrorless Interchangeable Lens (MIL) segment as well.\nI was waiting for a GF-1 update, that fixed some of the auto-focus speed issues and various other shortcomings. That update is yet to come but something else happened along the way…\nEnter Sony α NEX-5 This May, Sony announced two APS-C MIL cameras — α NEX-5 and α NEX-3. For the purpose of this post, I’m going to ignore NEX-3, which is just a cheaper version of NEX-5 designed to win price wars where needed. Sony α NEX-5 with 16mm f/2.8 Lens What’s cool about NEX-5? Well, consider that despite being an all-metal body, it’s considerably lighter than anything in this category. Not only that, it’s also the smallest. It’s smaller and lighter than the Canon G10/11/12, it’s smaller and lighter than Nikon P7000, it’s smaller and lighter than Lumix GF-1, than PEN EP-1 or EP-2… heck, it’s smaller and lighter than anything at all!\nIn terms of IQ, this sensor beats all contemporary m4/3 offerings hands-down, blows the fixed lens, tiny-sensor compacts out of reckoning and stands all the way up to Nikon D5000’s image quality. Other notable and interesting features are:\nTilt-up/-down LCD screen with best-in-class display. Hand-held HDR creation with upto 6-stop difference among bracketed shots. It works. Multiple-shot High ISO blending for reduced noise low-light photography of still subjects. 3-function programmable jog dial and 1-function programmable lower soft-button (firmware v3). High-resolution 2D/3D sweep panoramas. It’s not magical, but it does usually work. A pan-head tripod like the Sony VCT R640 that I have will be of tremendous help. 1080p/24 fps videos with continuous AF (Nikon D3100 is the first to offer continuous video-AF in DSLR form-factor). No wonder then, that the Sony α NEX-5 is the best mirrorless camera available. In the absence of any significant moves from Canon or Nikon and Sony’s continued ground-breaking advancement into compact APS-C bodies, NEX-5 is also likely to hold its position this year.\n","permalink":"https://tahirhashmi.com/posts/2010/09/22/which-compact-camera/","tags":["alpha","camera","compact","e-mount","evil","mil","mirrorless","nex","nex-5","sony"],"title":"Which Compact Interchangeable-Lens Camera?"},{"categories":["Photography","Photography"],"contents":"On my recent vacation, I took a bunch of bracketed exposures to turn into HDRs. Before this trip I only used a tripod for bracketing. This time around, however, I had to deal with hand-held bracketed shots. To make things a bit worse, these shots included foliage, which isn’t always stationary between shots. Picturenaut utterly failed to align these images. I tried Luminance HDR (qtpfsgui) for Windows 7 (64-bit), and it simply kept crashing. Then I tried Hugin, which too failed to do much. Besides, it was extremely confusing since it is a tool for stitching panoramas, with HDR and alignment being a part of the whole.\nFinally, I found HDR Alignment Tool — a free Windows utility for aligning multiple images using manually selected control points. Following is a screenshot of what the tool looks like.\nThere’s a selection of input files at the top, and the output preview in the bottom right corner. The “In”, “Out”, “1:1” and “Fit” buttons are zoom settings for either the source images all together, or for the output preview. “CP1” and “CP2” refer to the control points and allow centering on the chosen control point.\nThe default selection of control point locations and the resulting alignment aren’t great, so just relocate each of them manually and fine-tune the alignment by visually inspecting the sharpness of the output image near the chosen control point. It’s cumbersome, but it works pretty well. The tool also includes some lens correction features but I didn’t use them, instead making Capture NX2 apply lens correction while creating the source JPEGs. Oh, let me now get to the workflow, which is a bit involved since HDR Alignment Tool doesn’t work with TIFF or RAW files.\nLoad each bracketed exposure in Capture NX2. Ensure that all exposure parameters (white-balance, etc.) are the same, apply any other pre-processing steps (lens distortion correction, in my case) and create JPEGs. Open the JPEGs in HDR Alignment Tool, carry out the alignments and save the aligned JPEGs. By default, the tool will append “Aligned” as a suffix to the file name for distinction. Open the “*Aligned.JPG” files in Picturenaut. Apply the desired Tonemapping. Save the tonemapped output as a 16-bit TIFF. Load the TIFF back in Capture NX2, apply post-processing (tilt, sharpening, colour adjustments, etc.) and finally save the HDR JPEG. The following photo is one example of the manually aligned HDR that I created. Creating one HDR took me about 20 minutes, after I got all things sorted out. The bottomline, however, is that its best to avoid these acrobatics and directly shoot with a tripod if you’re intending to create HDRs. You never know when you might want to create an HDR, so the second thing to remember is to always carry a tripod where possible… or get a Sony NEX-5 😉\n","permalink":"https://tahirhashmi.com/posts/2010/09/21/handheld-hdrs/","tags":["alignment","handheld","hdr","picturenaut"],"title":"Creating Hand-held HDRs"},{"categories":["Technology"],"contents":"The Problem Recently, I got a reference to this article from my Product Manager, regarding TripAdvisor badges and how they boost SEO. Their secret is a simple link that links deep into TripAdvisor website and makes your site a donor of link love to TripAdvisor. This is somewhat bad for hotel owners who might find that TripAdvisor trumps the search ranking for their brand name.\nThe easy solution to prevent that from happening would be to slightly edit TA widget and use a rel=\u0026quot;nofollow\u0026quot; attribute in the anchor that links to TripAdvisor. The clever guys at TA, though, have a JavaScript check that disables the widget if you try to do this. Well, how about we turn their own trick on to them?\nFooling the TripAdvisor ‘rel’ check Let’s look at the anatomy of a TripAdvisor badge, as it would have to be embedded on our site. Here’s a sample badge (some element IDs have been munged):\n\u0026lt;div id=\u0026#34;TA_featuredgeo490\u0026#34; class=\u0026#34;TA_featuredgeo\u0026#34;\u0026gt; \u0026lt;ul id=\u0026#34;Cgaw\u0026#34; class=\u0026#34;TA_links sx5tpn4arx\u0026#34;\u0026gt; \u0026lt;li id=\u0026#34;CrCRPg\u0026#34; class=\u0026#34;E4yIRnh9y\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;http://www.tripadvisor.in/Hotel_Review-g293860-d297631.html\u0026#34;\u0026gt;Kerala\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;http://www.jscache.com/wejs?wtype=featuredgeo...\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Note the link in line #04. We want to adorn it with a rel=\u0026quot;nofollow\u0026quot; attribute but TripAdvisor’s JS doesn’t allow us to do so. Rather, it disables the badge if we do so. The solution for getting the nofollow attribute in there is:\nPaste in the widget and set rel=\u0026quot;nofollow\u0026quot; in the widget’s anchor tag Write a JS function to locate the anchor and call .removeAttribute('rel') on it Before the widget code calls any of its own JS, make sure that you call your function to scrub the rel attribute The function to scrub the rel attribute looks like this:\nscrub_rel = function() { var container = document.getElementById(\u0026#34;CrCRPg\u0026#34;); var anchor = container.childNodes[0]; anchor.removeAttribute(\u0026#39;rel\u0026#39;); } The widget HTML then looks like this:\n\u0026lt;div id=\u0026#34;TA_featuredgeo490\u0026#34; class=\u0026#34;TA_featuredgeo\u0026#34;\u0026gt; \u0026lt;ul id=\u0026#34;Cgaw\u0026#34; class=\u0026#34;TA_links sx5tpn4arx\u0026#34;\u0026gt; \u0026lt;li id=\u0026#34;CrCRPg\u0026#34; class=\u0026#34;E4yIRnh9y\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;http://www.tripadvisor.in/Hotel_Review-g293860-d297631.html\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;Kerala\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt;scrub_rel()\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://www.jscache.com/wejs?wtype=featuredgeo...\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Note the rel=\u0026quot;nofollow\u0026quot; attribute in line #05 and the extra script tag calling our scrub_rel() function in line #08. Voila! You’ve stopped yourself from leaking SEO Karma to TripAdvisor!\nFollowing is the complete HTML, so that you get the picture. I’ve also changed the logic to search for tripadvisor anchor tags to a more portable but less efficient implementation. You can choose this or the one above, based on whether you know how to locate the anchor tag for your widget manually.\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;TripAdvisor Badge Demo\u0026lt;/title\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; scrub_rel = function() { var links = document.getElementsByTagName(\u0026#34;a\u0026#34;); for (var i = 0; i \u0026lt; links.length; i++) { var anchor = links[i]; if (anchor.href.search(/(tripadvisor)/) != -1) { anchor.removeAttribute(\u0026#39;rel\u0026#39;); } } } \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;TA_featuredgeo490\u0026#34; class=\u0026#34;TA_featuredgeo\u0026#34;\u0026gt; \u0026lt;ul id=\u0026#34;Cgaw\u0026#34; class=\u0026#34;TA_links sx5tpn4arx\u0026#34;\u0026gt; \u0026lt;li id=\u0026#34;CrCRPg\u0026#34; class=\u0026#34;E4yIRnh9y\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;http://www.tripadvisor.in/Hotel_Review- g293860-d297631.html\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;Kerala\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt;scrub_rel()\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://www.jscache.com/wejs? wtype=featuredgeo\u0026amp;uniq=490\u0026amp;locationId=297631\u0026amp; lang=en_IN\u0026amp;hotel=y\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ","permalink":"https://tahirhashmi.com/posts/2010/08/24/beating-tripadvisor-seo/","tags":["hack","javascript","SEO","tripadvisor"],"title":"Beating the TripAdvisor Badge’s SEO Tactic"},{"categories":["Photography"],"contents":"With the monsoons doing their usual thing and I having the luxury of a not-too-bad view of the rains, I had been itching to capture a shot of the rain in all its glory. Earlier this year I made a capture that brought out the rainy-ness in the scene but didn’t have any rain as such.\nOnline research about shooting the rains didn’t bring much enlightenment because there’s no “formula” for making rain shots. It all depends on what you want to portray. For me, that would be about the prominence of the falling streaks of rain. All I knew was that I had to shoot a somewhat low shutter speed for that.\nThis Sunday, there was a brief spell followed by a longer shower, and I jumped at the opportunity (you don’t keep your camera nicely packed and hidden in your closet, do you?). The only thing I knew was that I had to shoot at 1/60s in Shutter Priority (S) mode. All of the first spell went in experimentation with other settings and by the time I got the hang of it, the rainfall had thinned out. Thankfully, it returned even stronger, so I got a nice shot, but in case you only get a brief opportunity, here are my observations that might help, right after the photograph I got.\nMind Your Background: The streaks of rain are most visible when they capture the light of the sky and there’s a dark background to contrast them against. Foliage works as the best background, both due to its dark colour and because the trees look so different when they’re wet, thereby adding to the “feel” of the photograph. In most cases, if you can see the rain, so can the camera. Don’t Use Cloudy WB: If you’re shooting RAW and you play with WB settings while developing, you’d notice that using Cloudy/Shade WB makes the rain streaks almost disappear. This is because the rain-drops are visible only because they’re reflecting the sky and Cloudy WB tends to flatten the sky colours. If you shoot JPEG, this is the make-or-break setting. I used Daylight/Sunny setting for the above shot. Over-expose and Pull: There is not much dynamic range in a rainfall scene. To get a nicely contrasty and colourful photograph, expose to the right and then pull back the scene while developing (e.g. adjust your black point to the extent that you begin seeing some dark spots in the photo). The above shot was made at +2/3 EV over-exposure. This only works if you’re shooting RAW. Mind the ISO: Some times, the scene may become so dark when it’s raining that the camera may not be able to give a good exposure at 1/60s, even with maximum aperture. Watch out for signs of this (e.g. if the aperture reading in your Nikon’s viewfinder says, “Lo”) and bump up the ISO. The above photo didn’t have this problem but some other shots required ISO-400 at max aperture (~ f/5.6). What Aperture? The above shot is made at f/5.3 but I got decent shots up to f/11. Seems like aperture doesn’t make a huge difference. So, go ahead and try creating your own rain streaks. If you find any of the above tips helpful or not helpful, do drop a me a comment.\nHappy clicking!\n","permalink":"https://tahirhashmi.com/posts/2010/08/18/capturing-rain/","tags":["rain","rainfall"],"title":"Capturing Rain"},{"categories":["Technology","Programming"],"contents":"I was just trying to profile an HTML page for performance bottle-necks. I’m trying to follow a top-down approach, wherein I start from the entry script, and find the block of code that takes the biggest chunk of time before digging deeper into that chunk.\nAt this stage, it’s not feasible to drop in a full-blown profiling tool like xdebug because of the set-up overhead and amount of data it generates. So, I wrote a simple timing function that you can call at various points in your program to provide incremental and cumulative timing info in milliseconds. Just call this function anywhere in your code with a message and it’ll print that message to your PHP output, followed by millisecond timing information at that point. Here’s the function implementation:\n\u0026lt;?php function time_this($msg) { $prefix = \u0026#39;[[\u0026#39;; static $start = 0; static $prev = 0; $now = microtime(true); if ($start == 0) { $start = $now; $prev = $now; return; } printf(\u0026#34;%s%s: %f (cum), %f (incr)\\n\u0026#34;, $prefix, $msg, ($now - $start)*1000.0, ($now - $prev)*1000.0); $prev = $now; } time_this(\u0026#34;\u0026#34;); // initialization ?\u0026gt; The nice thing about this script is that it automatically keeps track of timing information through static variables. You don’t need to keep track of that manually. Since it’s also taking in a message, you could use it to debug your program if you’re among those who find printf to be the best debugger 😀\nSample usage:\n\u0026lt;?php require_once(\u0026#39;time_this.php\u0026#39;); time_this(\u0026#34;starting script\u0026#34;); require_once(\u0026#39;foo.php\u0026#39;); require_once(\u0026#39;bar.php\u0026#39;); time_this(\u0026#34;includes done\u0026#34;); // ... time_this(\u0026#34;somewhere in between\u0026#34;); // ... time_this(\u0026#34;script ends\u0026#34;); ?\u0026gt; Output:\n[[starting script: 0.000015 (cum), 0.000015 (incr) [[includes done: 94.221115 (cum), 94.221100 (incr) [[somewhere in between: 110.846996 (cum), 16.625881 (incr) [[script ends: 145.432760 (cum), 34.585764 (incr) Let me know if you like it, or if you know of something similar that does the job better.\n","permalink":"https://tahirhashmi.com/posts/2010/08/09/simple-php-timer/","tags":["debugging","function","php","printf","script","timer"],"title":"Simple PHP Timer"},{"categories":["Photography","Photography"],"contents":"Digital sensors have made a lot of progress on the light efficiency front. The Nikon D3s sensor, currently the most efficient sensor available, offers amazingly clean images at crazy high ISOs. Something that film shooters could only dream of. There is still some time, however, before a D3s calibre sensor makes it to consumer bodies. Meanwhile, it will help knowing the tips and tricks of shooting in the dark for the win.\nGear Before proceeding further on the topic, I’d like to say that the best thing to do is simply not shoot in low light. Most low light situations can benefit hugely from an off-camera speedlight bounced from the ceiling or walls. This essentially involves placing a large external flash like the SB-600 somewhere out of your field of view (your camera’s hot shoe is also a good candidate) and pointing it toward the ceiling. You can now happily shoot at ISO 200 and enjoy good, clean images with soft pleasant lighting. The following shot of pizza toppings was made at f/5.6, ISO 400, 1/60s in ambient light provided by 2 11-watt CFLs. Without the flash, this shot would have had less tonality, more noise and perhaps motion blur (corresponding flash-less settings would be something like ISO 3200, 1/8s).\nThe other very important piece would be a lens that has a large aperture, i.e. at least f/2.8 or larger. The AF 50mm f/1.8 D Nikkor aka Nifty-Fifty is a good, inexpensive purchase. Beware, though, shooting at very wide apertures places a big ask on focus accuracy because the DoF at large apertures might become too thin to be workable. Auto-focus systems also tend to perform poorly in low light. One useful tip for shooting people with wide apertures is to try and focus on the near eye — eyes are easier for AF systems to lock on, and it gives a natural guide for the viewer of the photograph.\nDespite having the best gear, you still might end up in situations where you can’t do anything but bump up the ISO and face the lack of light. That’s where some of the following techniques will help.\nTaking the Photographs 1. Exposure vs. ISO The first time I was faced with shooting in low light, the camera metered some terribly low shutter speed like 1/20s at ISO 640, which is tough to shoot with even in low light. I decided to under-expose by a stop at the same ISO to get up to 1/8s because I didn’t want to hit the limit of the ISO range at 1600. Higher ISO = More Noise, right? Yes, provided the exposure is comparable in both. Under-exposure results in even more noisy photos. My shots from the above-mentioned sequence were all wasted. Here’s a noisy, under-exposed shot. Do you want to guess what ISO the following shot was made at?\nISO 100! You can see the effect of -5 EV under-exposure on the noise even in this small version. It would print really horrible on 6″x4″ paper. When you have to choose between underexposing and bumping up the ISO to get to a desired shutter speed, always go for higher ISO. An underexposed, low ISO shot is almost always more noisy than a properly exposed high ISO shot. Here’s another shot where I chose to over-expose +2/3 EV even at ISO 1600, to allow good tonality and saturation. The result speaks for itself.\n2. Auto-Focus Another area that suffers badly in low-light is auto-focus. Most AF systems try to lock in on edges that they find under the focus points where the boundaries appear the sharpest. This turns out to be difficult in low light because the difference in light intensity between light and dark parts of an edge are reduced. Having a speedlight like the SB-600 helps here, because it projects a focusing grid on to the field of view (much like the red grid you might see in supermarket barcode readers). If you find your hardware failing, you could try some of these tricks:\nZoom-in; Focus; Zoom-out: This works if you have a zoom lens that’s not “varifocal“, i.e. its focus doesn’t change while zooming in and out. The trick is to zoom all the way in, acquire focus — either manually or automatically — and zoom out to the frame you want. Most of the modern SLR zoom lenses are parfocal. I know that the 18-200mm f/3.5-5.6 VR Nikkor works. Focus and Recompose: Sometimes, we would like to focus on a subject that’s not contrasty enough for AF or manual focus to work. In this case, it might be helpful to focus on a more edgy object at roughly the same distance and recompose. The results are iffy, though. Also, if you are not able to verify exact focus through the camera view-finder, chances are that your photograph would pass on as fairly in focus, anyway. Live View: Most new DSLRs now support Live View feature. Live View can help you a lot when trying to focus manually. Turn on Live View and zoom all the way in on the view. Since you’re zooming in on the image preview and not on the lens itself, this trick works even with non-parfocal lenses. I made the following shot using the same technique in fading daylight. The AF was being tricked too often by the bushes and rubble, and generally being unable to cope with the fast moving bird. 3. People Shots with Slow Sync Flash One of the most frustrating experiences in low-light shooting is blurry people shots. When your shutter speeds drop to levels below 1/20s, asking people to stand still also doesn’t help. You end up needing to try something else. You also can’t pop the on-board flash because\nit gets into people’s eyes (not a problem with ceiling bounce), and it doesn’t bring out the ambient light properly, leading to an unpleasant photograph. Issue #2 can be solved by using the flash in “Slow Sync” mode. In this mode, the flash fires for a short time (1/60s) but the shutter stays open for as long as it would have opened without the flash. You can choose whether the flash fires in the beginning of the exposure (Front Curtain Sync) or toward the end (Rear Curtain Sync). Why would you use slow sync instead of just shooting without flash? Because it de-emphasises motion blur that occurs while the flash is not on and captures a sharp outline when the flash pops.\nWhile shooting people in slow sync mode, Rear curtain sync works better than front curtain for two reasons. One, people don’t notice a snap until the flash goes off. In front curtain, they tend to think that the snap is done when the flash goes off and let go of their pose while the shutter is still open. This leading to blurry shots.\nSecondly, rear curtain captures motion in the forward direction. You get to see the final position of a moving object most prominently, which is more natural than seeing the starting position of an object and seeing a blur in its later direction of movement.\nWhile using slow-sync, just always remember to disable pre-flashes by using FV lock to determine exposure. You want your flash to fire exactly once, and toward the end of the exposure, to get people to instinctively comply with the slow shutter. You might also want to use an orange colour gel in front of your flash if you don’t want the people to appear blue while shooting in incandescent light. Here’s a 1/8s full-on action shot of my friend strumming on the guitar, taken using this technique. If you look carefully, you could reveal a lot more motion blur than what is apparent in first glance. That’s the rear sync advantage!\n4. Camera Stability One of the easy ways of success in low light is to be able to use slow shutter speed, at least for relatively still subjects. The problem here is that slow speed results in camera shake. It is, therefore, important to learn how to have a stable grip on the camera. I myself, took a very long time getting this right. Part of the reason is that people talk about this aspect very infrequently on forums. Part of it is that you only get the information in bits and pieces. I’d invite you to go through this excellent blog post on how to make a “human tripod”. Supporting the camera: holding with your hands.\nOne of the techniques I often apply when faced with camera shake is to burst a series of shots. Chances are that one of those shots will be sharp. It frequently happens to be the first in the series and I find bursting for more than 2 seconds to be a waste. The following is a 1/4s shot made using a Canon PowerShot A630 P\u0026amp;S in burst mode.\nThat’s it for this article. It still talks about shooting at high ISO and it is inevitable that you’ll get noise in such shots. In the next post, I’ll talk about development techniques that would help you reduce the apparent noise, beyond just pushing the NR slider all the way up. Oh yeah, you’re shooting RAW, aren’t you? 😉\n","permalink":"https://tahirhashmi.com/posts/2010/08/03/shooting-in-the-dark/","tags":["ambient light","available light","flash","high iso","lowlight","night","noise"],"title":"Shooting in the Dark"},{"categories":["Opinion"],"contents":" This isn’t my first time writing a photography blog. I started one, almost 3 years ago and wrote a few posts regarding technique and gear. It was probably too early to start writing, though, and the blog fell off the edge when my hosting plan expired and I didn’t renew it. This time, it will hopefully stay.\nPhotography is, for me, a new way to experience things. It’s about delving deep into the visual characteristics of the environment around me — shapes, contours, colours, location, the light that I’m seeing by. This ability to observe also allows me to interact with inanimate objects in a new way. I can play with things by giving them different looks through the medium of photography. It may seem trivial at first, but for those who have to live by themselves for some length of time, it’s a relaxing and challenging pursuit that keeps the mind occupied from devilish or plain depressive thoughts.\nPhotography is also quite rewarding. After spending huge amounts of money on acquiring the right gear, learning the right skills, taking time out to visit the right places and finally making the right image, when you hold a 12″x18″ print in your hands, the sense of satisfaction is as good as from any other accomplishment. Those photographs are not mere memories, they are testimony to the fact that you had a uniquely personal interaction with an environment or an object.\nThrough this blog, I hope to share my observations and notes with anyone who might be interested as I work on developing myself as a better photographer, and share the small joys I may get out of the photographs that I make along the way.\nClick away!\n","permalink":"https://tahirhashmi.com/posts/2010/07/31/hello-world-2/","tags":null,"title":"Take Two"},{"categories":["Technology"],"contents":"These days, more and more people ask the Python vs. PHP question when they start out with a web application from scratch. I’ve developed PHP applications for 5 years but for the last couple of years I’ve been doing Python. This post is meant to note some of my observations. If you don’t want to read the whole of it, my opinion — opinion — is to stick with PHP for dishing out your *ML. Use Python in the back-end, if you must.\nProduction Worthiness In terms of production stability and ubiquity, Python is still playing catch-up to PHP. PHP derives a huge benefit from its custom fit into the web development model, whereas Python is a general purpose language, with an adaptor for web development, called WSGI. PHP doesn’t do multi-threading, which is a great way to guide developers in a “shared nothing” line of thought. PHP and apache also work together to protect you from a lot of issues — you can enforce a limit on the memory your script uses for each request. Memory leaks are not a bother, thanks to periodic reaping of apache processes if you use the default prefork MPM and set MaxRequestsPerChild to a sane value (default is 4000).\nMost Python web application frameworks would like you to build multi-threaded applications and recommend that you run your Python application as a daemon. From what I’ve seen so far, you really don’t want to be doing this. Multi-threading is not easy to begin with. Then there’s the GIL, which basically prevents a single Python daemon from utilising multiple CPUs and/or cores at the same time. That’s enough to be a show-stopper for an application being written in 2010. Third, I’ve seen random lock-ups, and slow memory leaks happen with Python daemons under the kind of production loads a small website may encounter (~ 1 Mbps network I/O; 20 reqs/sec).\nThe model that I would recommend is to run your Python application embedded in apache process space through mod_wsgi. This primarily ensures that you don’t need to renice up the priority of your Python daemon process to give it sufficient CPU time-share while under load and, more importantly, it enables the MaxRequestsPerChild clean-up mechanism.\nRuntime Model In terms of runtime model, one of the things to keep under consideration is that PHP would load all of its extensions, whether or not they’re used, while Python modules are loaded on demand. The PHP scenario is not too bad, though, since the memory usage is shared across all child processes, not multiplied across them. One of the differences that don’t stand out but are significant is that PHP’s extension library is almost all written in C, whereas Python libraries are not, to a very large extent. Many Python frameworks choose to implement a lot of important stuff like data structures in native Python. Given that a typical web application spends most of its time iterating through records from a database and doing various manipulations on it, having core functionality implemented in the script is not optimal. Python templates make the whole situation even worse.\nThat said, if you want to bet on the future, the JIT compilers being developed for Python might help you out. Then there’s HipHop for PHP.\nDevelopment Models One of the big reasons for Python’s attractiveness as a development platform, apart from syntax and language features, is the presence of Ruby On Rails inspired frameworks. The most popular among them is Django, but I’ve also developed on Pylons and Werkzeug. I won’t list down the reasons why these frameworks are attractive, but they’re very good from a development model perspective.\nPHP too has frameworks but they seem extremely loaded and superfluous to me. PHP itself is the framework for web development. It offers templating, it offers routing and it has a rich library of web development helpers. What it doesn’t do is a couple of things:\nenforce MVC model out-of-the-box. You can write your entire application in one file and do it easily to start with\nplay well with the (IMO highly retarded) Java and GoF Patterns style of object oriented programming\nOne large application that I’ve worked on had thousands of files, each corresponding to a PHP class, with layers upon layers of wrapper classes without much individual responsibility. Some poor code organisation decisions also led to a scenario where each user facing page ended up including almost all of the 100k lines of PHP in the application.\nPython would, to some extent, let you get away with this atrocity. More importantly, though, if you’re using the web frameworks, it could effectively prevent you from landing up in such a situation to begin with. If you know enough of MVC and shared-nothing fu, I think it’s worthwhile going with PHP as of now, if only for performance concerns.\nHow About PHP and Python? Like I mentioned, one of the big reasons for using Python for web applications is the development model and the programming language capabilities itself. PHP takes care of the ‘V’ and ‘C’ parts of MVC but trying to write complicated models in PHP can lead you to pushing at the limits of the language’s development model.\nOne of the development models I am exploring (and have working in production already) is a two-tier application layer. The “model” lives as a separate tier of the application, accessed through JSON-HTTP services. The “front-end” is done entirely in PHP and is only responsible for generating web pages and handling browser-based interaction. This model has two benefits:\nThe ‘front-end’ runtime is completely decoupled from the ‘back-end’, whether it be complexity, performance or scaling concerns\nYou get usable webservice APIs right out of the box.\nSomeone’s quipped that not having APIs in 2009 is like not having a website in 1999. I wouldn’t disagree. Every web application that I’ve built since 2004 has eventually had to support some kind of APIs. That’s essentially the reason for wsloader‘s existence. It allows you to write your super heavy-duty back-end in Python, while requiring you to provide a portable interface, which it can automatically expose as a JSON-HTTP web-service. Writing 1000 words to make a plug seems preposterous, and probably is, so please don’t consider this as a plug!\n","permalink":"https://tahirhashmi.com/posts/2010/07/28/php-vs-python-for-web-apps/","tags":["architecture","mvc","php","python","webapps","webservices"],"title":"PHP vs. Python for Web Apps"},{"categories":["Programming","Technology"],"contents":"Whenever people search for a Python library for MySQL, they get directed to MySQL for Python. However, there are some nasties hidden in it. Searching for “mysql python memory leaks” results in a few links which suggest that using Unicode causes memory leaks with the library.\nToday, however, I found another cause for MySQLdb memory leaks, while debugging a leaky Python daemon at work — database errors.\nUse this script:\nimport MySQLdb options = { \u0026#34;user\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;passwd\u0026#34;: \u0026#34;p455\u0026#34;, \u0026#34;db\u0026#34;: \u0026#34;somedb\u0026#34;, \u0026#34;connect_timeout\u0026#34;: 1 } while True: try: MySQLdb.connect(**options) except MySQLdb.Error: pass Make sure MySQL is up and it’s possible to connect to it without failures using the above credentials. Then run this script and watch its memory usage. It’ll be rock steady for as long as you care to run it.\nNow stop mysqld or do anything that would cause errors during connection and rerun the script. Watch it gobble up meg after meg as the errors continue. Any MySQL error, like unreachable server, missing database or tables, malformed query, etc. would trigger the leak.\nI need to check if the recently released (13 days ago) MySQLdb 1.2.3 does better in this regard. MySQLdb 1.2.3 does fix the memory leak in this case.\n","permalink":"https://tahirhashmi.com/posts/2010/06/30/mysqldb-leaks-memory/","tags":["code","memory leak","mysql","mysqldb","programming","python"],"title":"MySQLdb Leaks Memory"},{"categories":["Technology"],"contents":"That snippet of Oz somehow felt like an appropriate title for the first post on my tech blog. Expect something new here once in a while. Happy hacking! 🙂\n","permalink":"https://tahirhashmi.com/posts/2010/06/18/hello-world/","tags":null,"title":"{Browse ‘Hello World!’}"},{"categories":null,"contents":"This website is made with Hugo, sprinkled with some AI pixie dust and hosted on Github Pages\nFont Sources The default body font is Oxanium. Headlines are formatted with Probert. Sidebar links, preformatted text and code snippets are rendered with Berkeley Mono. I also use Berkeley Mono as my preferred font for terminals and code editors.\nOxanium and Probert fonts evoke a futuristic space-explorer feeling while looking neat and tidy. All of these fonts, including Berkeley Mono, are stylistically inspired by the Eurostile font designed in Italy in 1962. Unfortunately, the latest versions of Eurostile sell for ~ $500 US.\nHonorable mention: Matrix. I haven\u0026rsquo;t used this font on the website but it\u0026rsquo;s one handy font for fans of The Matrix. I very nearly used the Matrix code rain as the background for the dark theme and the website logo.\nDark Theme and Interlace Background The website defaults to a dark or light theme based on your system preferences. The light theme uses Probert headings everywhere but the dark theme has a couple of surprises.\nThe dark theme appearance is a homage to the VT220 terminals on which I fell head-over-heels into C programming. A few years ago I bumped into a TrueType version of the VT220 font, called Glass TTY. I knew I just had to use this font for the dark theme.\nAnother surprise with the dark theme is a code snippet in the background. It\u0026rsquo;s just a shell script I wrote around the same time I was building this website. The script copies image files from memory cards while tagging them and organising them in the right directory structure — signifying a blend of my interests in coding and photography.\nLastly, the dark theme would\u0026rsquo;ve looked much less convincing without the interlace effect in the background, which I adapted from here\nAI I\u0026rsquo;ve incorporated little bits of input from Github Copilot and ChatGPT.\nStarter Theme Before I started hacking on my own theme, I had to start off of a working base. I used this theme as the starting point, although now I\u0026rsquo;ve deviated a lot from it.\n","permalink":"https://tahirhashmi.com/about/credits/","tags":null,"title":"Making-of \u0026 Credits"}]